Mon May 12 15:36:46 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100                    On  | 00000000:C6:00.0 Off |                    0 |
| N/A   37C    P0             120W / 600W |      0MiB / 95830MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
[INFO] Extracting squashfs filesystem...
Parallel unsquashfs: Using 20 processors
217327 inodes (393318 blocks) to write


created 215418 files
created 30529 directories
created 1903 symlinks
created 0 devices
created 0 fifos
created 0 sockets

=============
== PyTorch ==
=============

NVIDIA Release 25.01 (build 134983853)
PyTorch Version 2.6.0a0+ecf3bae
Container image Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

NOTE: CUDA Forward Compatibility mode ENABLED.
  Using CUDA 12.8 driver version 570.86.10 with kernel driver version 535.230.02.
  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

INFO 05-12 15:37:36 [__init__.py:239] Automatically detected platform cuda.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: ryanzhangofficial (ryzhangofficial). Use `wandb login --relogin` to force relogin
  0%|          | 0/1 [00:00<?, ?it/s]Loading bert:   0%|          | 0/1 [00:00<?, ?it/s]Loading bert:   0%|          | 0/1 [00:01<?, ?it/s]
/dss/dssfs04/lwp-dss-0002/pn72yi/pn72yi-dss-0000/ge56heh2/mess-plus/venv/lib/python3.12/site-packages/datasets/load.py:1298: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
wandb: Currently logged in as: ryanzhangofficial (tum-i13). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /workspace/wandb/run-20250512_153741-bm71e1k6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bert-winogrande-thr-0.20
wandb: ⭐️ View project at https://wandb.ai/tum-i13/routellm-sweep
wandb: 🚀 View run at https://wandb.ai/tum-i13/routellm-sweep/runs/bm71e1k6
Loaded winogrande: 1267 samples
[2025-05-12 15:37:42,355] [zeus.device.gpu.nvidia](nvidia.py:47) pynvml is available and initialized.
[2025-05-12 15:37:42,364] [zeus.device.cpu.rapl](rapl.py:136) RAPL is available.
[2025-05-12 15:37:42,364] [zeus.monitor.energy](energy.py:208) Monitoring GPU indices [0].
[2025-05-12 15:37:42,365] [zeus.monitor.energy](energy.py:209) Monitoring CPU indices []
[2025-05-12 15:37:42,365] [zeus.utils.framework](framework.py:25) PyTorch with CUDA support is available.
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:37:56 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:37:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:37:59 [cuda.py:292] Using Flash Attention backend.
INFO 05-12 15:37:59 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-12 15:37:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:38:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 15:38:10 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 15:38:10 [model_runner.py:1146] Model loading took 14.9889 GiB and 10.485998 seconds
INFO 05-12 15:38:11 [worker.py:267] Memory profiling takes 0.65 seconds
INFO 05-12 15:38:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:38:11 [worker.py:267] model weights take 14.99GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 67.36GiB.
INFO 05-12 15:38:11 [executor_base.py:112] # cuda blocks: 34487, # CPU blocks: 2048
INFO 05-12 15:38:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 269.43x
INFO 05-12 15:38:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 15:38:36 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.32 GiB
INFO 05-12 15:38:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 26.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 28.81 toks/s, output: 99.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 28.81 toks/s, output: 99.33 toks/s]
[winogrande thr=0.20 | sample 0] energy=6917.22 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:38:39 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:38:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:38:40 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:38:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 15:38:50 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 15:38:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.052181 seconds
INFO 05-12 15:38:51 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:38:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:38:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:38:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:38:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:38:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.40it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:20,  1.37it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.36it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:20,  1.29it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:07<00:18,  1.33it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:18,  1.28it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:18,  1.24it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:09<00:16,  1.33it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:10<00:15,  1.40it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.44it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:11<00:12,  1.48it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:12<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:13<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:15<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:17<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:18<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:20<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:22<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]
INFO 05-12 15:39:15 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 15:39:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.42 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.58 toks/s, output: 101.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.58 toks/s, output: 101.99 toks/s]
[winogrande thr=0.20 | sample 1] energy=4990.62 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:39:17 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:39:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:39:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:39:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.90s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.11s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 15:39:29 [loader.py:458] Loading weights took 10.17 seconds
INFO 05-12 15:39:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.501472 seconds
INFO 05-12 15:39:30 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:39:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:39:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:39:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:39:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:39:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.48it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 15:39:52 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:39:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.37 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.49 toks/s, output: 101.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.49 toks/s, output: 101.47 toks/s]
[winogrande thr=0.20 | sample 2] energy=4947.55 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:39:54 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:39:54 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:39:55 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:39:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 15:40:06 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 15:40:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.050024 seconds
INFO 05-12 15:40:07 [worker.py:267] Memory profiling takes 0.49 seconds
INFO 05-12 15:40:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:40:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:40:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:40:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:40:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.43it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:24,  1.33it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:25,  1.24it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:03<00:23,  1.34it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:21,  1.39it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.44it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.48it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 15:40:30 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 15:40:30 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 3] energy=4944.30 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:40:32 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:40:32 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:40:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:40:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 15:40:44 [loader.py:458] Loading weights took 10.16 seconds
INFO 05-12 15:40:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.601814 seconds
INFO 05-12 15:40:45 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 15:40:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:40:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:40:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:40:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:40:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 15:41:08 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:41:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.40 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.77 toks/s]
[winogrande thr=0.20 | sample 4] energy=4954.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:41:10 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:41:10 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:41:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:41:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.08s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 15:41:22 [loader.py:458] Loading weights took 9.96 seconds
INFO 05-12 15:41:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.390771 seconds
INFO 05-12 15:41:23 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:41:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:41:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:41:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:41:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:41:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 15:41:46 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:41:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.45 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.48 toks/s, output: 101.29 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.48 toks/s, output: 101.29 toks/s]
[winogrande thr=0.20 | sample 5] energy=4946.88 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:41:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:41:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:41:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:41:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.93s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.12s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]

INFO 05-12 15:42:00 [loader.py:458] Loading weights took 10.05 seconds
INFO 05-12 15:42:00 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.375805 seconds
INFO 05-12 15:42:01 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:42:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:42:01 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:42:01 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:42:01 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:42:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 15:42:23 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:42:23 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.42 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.67 toks/s, output: 101.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.67 toks/s, output: 101.77 toks/s]
[winogrande thr=0.20 | sample 6] energy=4958.87 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:42:25 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:42:25 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:42:27 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:42:27 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.94s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 15:42:37 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 15:42:37 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.315732 seconds
INFO 05-12 15:42:38 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:42:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:42:38 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:42:38 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:42:38 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:42:38 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 15:43:01 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:43:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.44 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 38.72 toks/s, output: 99.27 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 38.72 toks/s, output: 99.27 toks/s]
[winogrande thr=0.20 | sample 7] energy=4918.91 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:43:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:43:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:43:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:43:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 15:43:14 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 15:43:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.114901 seconds
INFO 05-12 15:43:15 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 15:43:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:43:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:43:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:43:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:43:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 15:43:38 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:43:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.51 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.48 toks/s, output: 101.45 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.48 toks/s, output: 101.45 toks/s]
[winogrande thr=0.20 | sample 8] energy=4893.53 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:43:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:43:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:43:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:43:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 15:43:52 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 15:43:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.062684 seconds
INFO 05-12 15:43:53 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:43:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:43:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:43:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:43:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:43:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.37it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:05,  1.40it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.45it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 15:44:16 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 15:44:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.57 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.57 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 9] energy=4977.76 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:44:18 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:44:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:44:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:44:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 15:44:30 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 15:44:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.272074 seconds
INFO 05-12 15:44:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:44:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:44:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:44:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:44:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:44:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.47it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:09,  1.36it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.40it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.43it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.47it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 15:44:54 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 15:44:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.54 toks/s, output: 101.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.54 toks/s, output: 101.65 toks/s]
[winogrande thr=0.20 | sample 10] energy=4923.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:44:56 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:44:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:44:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:44:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.07s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]

INFO 05-12 15:45:07 [loader.py:458] Loading weights took 10.06 seconds
INFO 05-12 15:45:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.389836 seconds
INFO 05-12 15:45:08 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:45:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:45:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:45:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:45:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:45:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:12,  1.42it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:12,  1.38it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:11,  1.44it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.46it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.48it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 15:45:31 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 15:45:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.57 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.49 toks/s, output: 101.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.49 toks/s, output: 101.57 toks/s]
[winogrande thr=0.20 | sample 11] energy=4938.63 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:45:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:45:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:45:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:45:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 15:45:45 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 15:45:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.069465 seconds
INFO 05-12 15:45:46 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:45:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:45:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:45:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:45:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:45:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:15,  1.45it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:15,  1.37it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.43it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.47it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 15:46:09 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 15:46:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.62 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 12] energy=4878.18 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:46:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:46:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:46:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:46:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 15:46:22 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 15:46:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.163653 seconds
INFO 05-12 15:46:23 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:46:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:46:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:46:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:46:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:46:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:18,  1.41it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:18,  1.37it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.43it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.44it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 15:46:46 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 15:46:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.61 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.68 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.68 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 13] energy=4914.67 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:46:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:46:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:46:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:46:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 15:46:59 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 15:47:00 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.104724 seconds
INFO 05-12 15:47:00 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 15:47:00 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:47:00 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:47:01 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:47:01 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:47:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:22,  1.36it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:21,  1.38it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:19,  1.44it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.48it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 15:47:23 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 15:47:23 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.63 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.72 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.72 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 14] energy=4917.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:47:26 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:47:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:47:27 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:47:27 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 15:47:37 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 15:47:37 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.097162 seconds
INFO 05-12 15:47:38 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:47:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:47:38 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:47:38 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:47:38 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:47:38 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:26,  1.30it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.46it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 15:48:01 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:48:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.43 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.71 toks/s, output: 101.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.71 toks/s, output: 101.82 toks/s]
[winogrande thr=0.20 | sample 15] energy=4869.57 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:48:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:48:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:48:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:48:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.15s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.16s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.65s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.65s/it]

INFO 05-12 15:48:15 [loader.py:458] Loading weights took 10.80 seconds
INFO 05-12 15:48:16 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.249176 seconds
INFO 05-12 15:48:16 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:48:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:48:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:48:17 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:48:17 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:48:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:48:39 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:48:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.20 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 16] energy=5022.78 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:48:42 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:48:42 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:48:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:48:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.57s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.61s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.83s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.91s/it]

INFO 05-12 15:48:55 [loader.py:458] Loading weights took 11.84 seconds
INFO 05-12 15:48:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.260348 seconds
INFO 05-12 15:48:56 [worker.py:267] Memory profiling takes 0.37 seconds
INFO 05-12 15:48:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:48:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:48:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:48:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:48:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.62it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.62it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.62it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.62it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.62it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.62it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.62it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.62it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.62it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.62it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.62it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:19<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 15:49:19 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:49:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.83 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 17] energy=5200.53 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:49:21 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:49:21 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:49:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:49:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.87s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.89s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.04s/it]

INFO 05-12 15:49:35 [loader.py:458] Loading weights took 12.34 seconds
INFO 05-12 15:49:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.677339 seconds
INFO 05-12 15:49:36 [worker.py:267] Memory profiling takes 0.37 seconds
INFO 05-12 15:49:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:49:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:49:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:49:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:49:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.62it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.62it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.62it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.62it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.62it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.62it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.62it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.62it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.42it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.41it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.47it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:49:59 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:49:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.21 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.63 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.63 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 18] energy=5263.53 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:50:01 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:50:01 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:50:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:50:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.97s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 15:50:13 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 15:50:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.294237 seconds
INFO 05-12 15:50:14 [worker.py:267] Memory profiling takes 0.37 seconds
INFO 05-12 15:50:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:50:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:50:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:50:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:50:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.61it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.62it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.62it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.61it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.46it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:05,  1.40it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.46it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:50:36 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:50:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.16 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.55 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.55 toks/s, output: 101.88 toks/s]
[winogrande thr=0.20 | sample 19] energy=4860.94 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:50:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:50:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:50:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:50:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 15:50:49 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 15:50:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.099289 seconds
INFO 05-12 15:50:50 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:50:50 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:50:50 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:50:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:50:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:50:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.61it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.41it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.47it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:51:13 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:51:13 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.26 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 47.86 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 47.86 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 20] energy=4866.78 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:51:16 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:51:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:51:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:51:17 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.02s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.22s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.57s/it]

INFO 05-12 15:51:28 [loader.py:458] Loading weights took 10.49 seconds
INFO 05-12 15:51:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.813513 seconds
INFO 05-12 15:51:29 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 15:51:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:51:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:51:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:51:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:51:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.61it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.62it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:12,  1.62it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.62it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.62it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.44it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:11,  1.41it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:10,  1.46it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:51:51 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:51:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.22 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.63 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.63 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 21] energy=5039.59 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:51:53 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:51:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:51:55 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:51:55 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 15:52:05 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 15:52:05 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.050042 seconds
INFO 05-12 15:52:06 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:52:06 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:52:06 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:52:06 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:52:06 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:52:06 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.61it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:14,  1.42it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:13,  1.42it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.47it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.62it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.62it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.62it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.62it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.62it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.62it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.62it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.62it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:52:28 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:52:28 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.17 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.59 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.59 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 22] energy=4835.62 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:52:30 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:52:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:52:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:52:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 15:52:41 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 15:52:42 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.048723 seconds
INFO 05-12 15:52:42 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 15:52:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:52:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:52:43 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:52:43 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:52:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.61it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:17,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.61it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.62it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:16,  1.41it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:15,  1.42it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.48it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:53:05 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:53:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.71 toks/s, output: 101.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.71 toks/s, output: 101.87 toks/s]
[winogrande thr=0.20 | sample 23] energy=4840.97 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:53:07 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:53:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:53:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:53:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 15:53:18 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 15:53:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.086287 seconds
INFO 05-12 15:53:19 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:53:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:53:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:53:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:53:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:53:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.61it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.62it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.61it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.42it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:19,  1.36it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:17,  1.43it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.46it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 15:53:42 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:53:42 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.25 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 24] energy=4856.06 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:53:44 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:53:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:53:46 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:53:46 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 15:53:56 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 15:53:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.140079 seconds
INFO 05-12 15:53:57 [worker.py:267] Memory profiling takes 0.37 seconds
INFO 05-12 15:53:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:53:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:53:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:53:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:53:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:22,  1.38it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:22,  1.36it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.44it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.49it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.61it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.61it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:54:19 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:54:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.23 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.70 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.70 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 25] energy=4905.02 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:54:21 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:54:21 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:54:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:54:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 15:54:33 [loader.py:458] Loading weights took 9.85 seconds
INFO 05-12 15:54:33 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.172456 seconds
INFO 05-12 15:54:34 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 15:54:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:54:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:54:34 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:54:34 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:54:34 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:28,  1.18it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:25,  1.30it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:22,  1.43it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.61it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.61it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.62it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.61it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:09,  1.62it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.62it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:54:56 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:54:56 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 26] energy=4845.65 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:54:58 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:54:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:55:00 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:55:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.47s/it]

INFO 05-12 15:55:10 [loader.py:458] Loading weights took 10.14 seconds
INFO 05-12 15:55:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.650732 seconds
INFO 05-12 15:55:11 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:55:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:55:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:55:11 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:55:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:55:12 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.61it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.62it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.62it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.62it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.62it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:19<00:01,  1.62it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]
INFO 05-12 15:55:33 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:55:33 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.81 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.56 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.56 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 27] energy=4855.34 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:55:36 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:55:36 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:55:37 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:55:37 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.16s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.48s/it]

INFO 05-12 15:55:48 [loader.py:458] Loading weights took 10.15 seconds
INFO 05-12 15:55:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.522236 seconds
INFO 05-12 15:55:49 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:55:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:55:49 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:55:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:55:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:55:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.59it/s]
INFO 05-12 15:56:11 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:56:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.00 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.51 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.51 toks/s, output: 101.71 toks/s]
[winogrande thr=0.20 | sample 28] energy=4960.50 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:56:14 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:56:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:56:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:56:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.35s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.54s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.64s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.76s/it]

INFO 05-12 15:56:27 [loader.py:458] Loading weights took 11.16 seconds
INFO 05-12 15:56:27 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.493583 seconds
INFO 05-12 15:56:28 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:56:28 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:56:28 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:56:28 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:56:28 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:56:28 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 15:56:50 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:56:50 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.12 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.63 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.63 toks/s, output: 101.80 toks/s]
[winogrande thr=0.20 | sample 29] energy=5106.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:56:52 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:56:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:56:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:56:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.97s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 15:57:04 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 15:57:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.311717 seconds
INFO 05-12 15:57:05 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:57:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:57:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:57:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:57:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:57:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 15:57:27 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:57:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.05 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 32.64 toks/s, output: 98.91 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 32.64 toks/s, output: 98.91 toks/s]
[winogrande thr=0.20 | sample 30] energy=4868.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:57:30 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:57:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:57:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:57:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 15:57:41 [loader.py:458] Loading weights took 9.81 seconds
INFO 05-12 15:57:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.144237 seconds
INFO 05-12 15:57:42 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:57:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:57:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:57:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:57:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:57:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.39it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.42it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 15:58:05 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 15:58:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.39 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.72 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.72 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 31] energy=4903.89 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:58:07 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:58:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:58:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:58:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.47s/it]

INFO 05-12 15:58:18 [loader.py:458] Loading weights took 10.11 seconds
INFO 05-12 15:58:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.440685 seconds
INFO 05-12 15:58:19 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:58:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:58:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:58:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:58:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:58:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.47it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.37it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.43it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.48it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 15:58:42 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 15:58:42 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.44 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.73 toks/s, output: 101.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.73 toks/s, output: 101.82 toks/s]
[winogrande thr=0.20 | sample 32] energy=4916.60 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:58:44 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:58:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:58:46 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:58:46 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.67s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.63s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.19s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.24s/it]

INFO 05-12 15:58:59 [loader.py:458] Loading weights took 13.08 seconds
INFO 05-12 15:58:59 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.495947 seconds
INFO 05-12 15:59:00 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:59:00 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:59:00 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:59:00 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:59:00 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:59:00 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:10,  1.44it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:10,  1.39it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.45it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:08,  1.49it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 15:59:23 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 15:59:23 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.45 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.70 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.70 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 33] energy=5291.10 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 15:59:25 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 15:59:25 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 15:59:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 15:59:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.50s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.48s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.68s/it]

INFO 05-12 15:59:37 [loader.py:458] Loading weights took 10.92 seconds
INFO 05-12 15:59:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.245396 seconds
INFO 05-12 15:59:38 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 15:59:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 15:59:38 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 15:59:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 15:59:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 15:59:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:14,  1.47it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:00:01 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:00:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.53 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.53 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 34] energy=4993.30 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:00:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:00:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:00:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:00:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 16:00:15 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 16:00:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.079750 seconds
INFO 05-12 16:00:16 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:00:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:00:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:00:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:00:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:00:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:19,  1.44it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.36it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:18,  1.43it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.48it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:00:39 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:00:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.44 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.88 toks/s]
[winogrande thr=0.20 | sample 35] energy=4947.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:00:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:00:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:00:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:00:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 16:00:52 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 16:00:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.363509 seconds
INFO 05-12 16:00:53 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:00:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:00:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:00:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:00:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:00:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:23,  1.34it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:23,  1.32it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:21,  1.40it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.45it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:01:16 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:01:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.42 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.62 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.62 toks/s, output: 101.88 toks/s]
[winogrande thr=0.20 | sample 36] energy=4900.46 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:01:18 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:01:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:01:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:01:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 16:01:29 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 16:01:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.246702 seconds
INFO 05-12 16:01:31 [worker.py:267] Memory profiling takes 0.50 seconds
INFO 05-12 16:01:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:01:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:01:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:01:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:01:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.59it/s]
INFO 05-12 16:01:53 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:01:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.34 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]
[winogrande thr=0.20 | sample 37] energy=4915.28 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:01:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:01:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:01:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:01:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.05s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.22s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.67s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.68s/it]

INFO 05-12 16:02:08 [loader.py:458] Loading weights took 10.95 seconds
INFO 05-12 16:02:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.275851 seconds
INFO 05-12 16:02:09 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:02:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:02:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:02:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:02:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:02:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 16:02:31 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:02:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.08 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 38] energy=5003.89 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:02:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:02:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:02:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:02:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.25s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.16s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]

INFO 05-12 16:02:45 [loader.py:458] Loading weights took 10.27 seconds
INFO 05-12 16:02:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.593592 seconds
INFO 05-12 16:02:46 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:02:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:02:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:02:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:02:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:02:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:03:09 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:03:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.18 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.55 toks/s, output: 101.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.55 toks/s, output: 101.63 toks/s]
[winogrande thr=0.20 | sample 39] energy=4895.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:03:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:03:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:03:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:03:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.21s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.50s/it]

INFO 05-12 16:03:23 [loader.py:458] Loading weights took 10.22 seconds
INFO 05-12 16:03:23 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.554206 seconds
INFO 05-12 16:03:24 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:03:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:03:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:03:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:03:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:03:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:03:46 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:03:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.19 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 40] energy=4928.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:03:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:03:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:03:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:03:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 16:04:00 [loader.py:458] Loading weights took 9.98 seconds
INFO 05-12 16:04:00 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.317854 seconds
INFO 05-12 16:04:01 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:04:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:04:01 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:04:01 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:04:01 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:04:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:04:24 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:04:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.28 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 30.03 toks/s, output: 100.10 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 30.03 toks/s, output: 100.10 toks/s]
[winogrande thr=0.20 | sample 41] energy=4933.52 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:04:26 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:04:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:04:27 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:04:27 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 16:04:38 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 16:04:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.683805 seconds
INFO 05-12 16:04:39 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:04:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:04:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:04:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:04:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:04:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.48it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.48it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:05:01 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:05:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.40 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.76 toks/s, output: 101.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.76 toks/s, output: 101.77 toks/s]
[winogrande thr=0.20 | sample 42] energy=4931.48 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:05:04 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:05:04 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:05:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:05:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 16:05:15 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 16:05:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.123225 seconds
INFO 05-12 16:05:16 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:05:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:05:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:05:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:05:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:05:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.41it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.40it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.45it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.49it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:05:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:05:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.54 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.74 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.74 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 43] energy=4928.04 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:05:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:05:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:05:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:05:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:05:53 [loader.py:458] Loading weights took 9.88 seconds
INFO 05-12 16:05:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.227444 seconds
INFO 05-12 16:05:54 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:05:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:05:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:05:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:05:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:05:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:08,  1.46it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.45it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:06:16 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:06:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.62 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.62 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 44] energy=4923.59 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:06:18 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:06:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:06:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:06:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 16:06:30 [loader.py:458] Loading weights took 9.85 seconds
INFO 05-12 16:06:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.189692 seconds
INFO 05-12 16:06:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:06:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:06:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:06:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:06:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:06:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.47it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.37it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.42it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.47it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:06:54 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:06:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.54 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.66 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.66 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 45] energy=4934.87 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:06:56 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:06:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:06:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:06:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:07:07 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 16:07:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.268700 seconds
INFO 05-12 16:07:08 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:07:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:07:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:07:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:07:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:07:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:14,  1.42it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:13,  1.42it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.47it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:07:31 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:07:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.44 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.64 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.64 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 46] energy=4938.17 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:07:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:07:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:07:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:07:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.08s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.23s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.58s/it]

INFO 05-12 16:07:47 [loader.py:458] Loading weights took 10.46 seconds
INFO 05-12 16:07:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.827091 seconds
INFO 05-12 16:07:48 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:07:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:07:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:07:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:07:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:07:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.46it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:19,  1.36it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:17,  1.42it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.47it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.48it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:08:11 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:08:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.56 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.60 toks/s, output: 101.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 38.60 toks/s, output: 101.58 toks/s]
[winogrande thr=0.20 | sample 47] energy=5125.82 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:08:13 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:08:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:08:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:08:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:08:24 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 16:08:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.234137 seconds
INFO 05-12 16:08:25 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:08:25 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:08:25 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:08:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:08:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:08:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:23,  1.34it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:21,  1.36it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.43it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.48it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:08:48 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:08:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.62 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.63 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.63 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 48] energy=4975.24 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:08:50 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:08:50 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:08:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:08:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:09:02 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 16:09:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.280194 seconds
INFO 05-12 16:09:03 [worker.py:267] Memory profiling takes 0.55 seconds
INFO 05-12 16:09:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:09:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:09:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:09:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:09:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:09:26 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:09:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.56 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.64 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.64 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 49] energy=4922.28 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:09:28 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:09:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:09:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:09:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.90s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.27s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.18s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.85s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.80s/it]

INFO 05-12 16:09:41 [loader.py:458] Loading weights took 11.40 seconds
INFO 05-12 16:09:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.777060 seconds
INFO 05-12 16:09:42 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:09:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:09:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:09:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:09:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:09:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:10:05 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:10:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.20 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.62 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.62 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 50] energy=5093.23 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:10:07 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:10:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:10:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:10:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.16s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.74s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.26s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.32s/it]

INFO 05-12 16:10:22 [loader.py:458] Loading weights took 13.44 seconds
INFO 05-12 16:10:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.771325 seconds
INFO 05-12 16:10:23 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:10:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:10:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:10:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:10:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:10:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:10:45 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:10:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.29 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.43 toks/s, output: 101.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.43 toks/s, output: 101.20 toks/s]
[winogrande thr=0.20 | sample 51] energy=5305.06 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:10:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:10:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:10:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:10:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.43s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.45s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.26s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.53s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.67s/it]

INFO 05-12 16:11:00 [loader.py:458] Loading weights took 10.90 seconds
INFO 05-12 16:11:00 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.227171 seconds
INFO 05-12 16:11:01 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:11:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:11:01 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:11:01 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:11:01 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:11:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.42it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.41it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.46it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:11:24 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:11:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.48 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.84 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.84 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 52] energy=5055.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:11:26 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:11:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:11:27 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:11:27 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 16:11:38 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 16:11:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.261568 seconds
INFO 05-12 16:11:39 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:11:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:11:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:11:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:11:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:11:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.49it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.38it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.43it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.47it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:12:01 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:12:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.46 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.56 toks/s, output: 101.52 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.56 toks/s, output: 101.52 toks/s]
[winogrande thr=0.20 | sample 53] energy=4949.86 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:12:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:12:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:12:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:12:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 16:12:15 [loader.py:458] Loading weights took 9.82 seconds
INFO 05-12 16:12:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.147142 seconds
INFO 05-12 16:12:16 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:12:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:12:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:12:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:12:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:12:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:12:38 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:12:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.21 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.49 toks/s, output: 101.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.49 toks/s, output: 101.33 toks/s]
[winogrande thr=0.20 | sample 54] energy=4877.64 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:12:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:12:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:12:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:12:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 16:12:52 [loader.py:458] Loading weights took 9.84 seconds
INFO 05-12 16:12:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.169874 seconds
INFO 05-12 16:12:53 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:12:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:12:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:12:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:12:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:12:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:13:15 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:13:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]
[winogrande thr=0.20 | sample 55] energy=4844.35 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:13:17 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:13:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:13:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:13:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 16:13:29 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 16:13:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.059560 seconds
INFO 05-12 16:13:30 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:13:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:13:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:13:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:13:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:13:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.46it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:16,  1.37it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:15,  1.43it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.47it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:13:53 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:13:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.46 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.59 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.59 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 56] energy=4885.87 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:13:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:13:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:13:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:13:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.11s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.31s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.58s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.66s/it]

INFO 05-12 16:14:08 [loader.py:458] Loading weights took 10.83 seconds
INFO 05-12 16:14:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.165067 seconds
INFO 05-12 16:14:09 [worker.py:267] Memory profiling takes 0.37 seconds
INFO 05-12 16:14:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:14:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:14:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:14:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:14:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:22,  1.35it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.40it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:19,  1.46it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:14:32 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:14:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.30 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]
[winogrande thr=0.20 | sample 57] energy=5101.11 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:14:34 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:14:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:14:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:14:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 16:14:46 [loader.py:458] Loading weights took 9.82 seconds
INFO 05-12 16:14:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.466514 seconds
INFO 05-12 16:14:47 [worker.py:267] Memory profiling takes 0.47 seconds
INFO 05-12 16:14:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:14:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:14:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:14:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:14:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:25,  1.32it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.47it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 16:15:09 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:15:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.32 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.61 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.61 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 58] energy=4959.29 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:15:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:15:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:15:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:15:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.45s/it]

INFO 05-12 16:15:23 [loader.py:458] Loading weights took 10.02 seconds
INFO 05-12 16:15:23 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.362077 seconds
INFO 05-12 16:15:24 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:15:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:15:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:15:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:15:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:15:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 16:15:46 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:15:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.92 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.62 toks/s, output: 101.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.62 toks/s, output: 101.68 toks/s]
[winogrande thr=0.20 | sample 59] energy=4857.46 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:15:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:15:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:15:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:15:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]

INFO 05-12 16:16:00 [loader.py:458] Loading weights took 10.28 seconds
INFO 05-12 16:16:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.618184 seconds
INFO 05-12 16:16:01 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:16:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:16:01 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:16:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:16:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:16:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.61it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.61it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 16:16:24 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:16:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.96 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.55 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.55 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 60] energy=4896.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:16:26 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:16:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:16:27 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:16:27 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.20s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.48s/it]

INFO 05-12 16:16:37 [loader.py:458] Loading weights took 10.12 seconds
INFO 05-12 16:16:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.452189 seconds
INFO 05-12 16:16:38 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:16:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:16:38 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:16:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:16:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:16:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:17:01 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:17:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.11 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.52 toks/s, output: 101.41 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.52 toks/s, output: 101.41 toks/s]
[winogrande thr=0.20 | sample 61] energy=4907.67 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:17:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:17:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:17:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:17:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.24s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.16s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]

INFO 05-12 16:17:15 [loader.py:458] Loading weights took 10.24 seconds
INFO 05-12 16:17:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.851384 seconds
INFO 05-12 16:17:16 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:17:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:17:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:17:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:17:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:17:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 16:17:39 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:17:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]
[winogrande thr=0.20 | sample 62] energy=4951.21 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:17:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:17:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:17:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:17:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 16:17:43 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.539504 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 16:17:53 [loader.py:458] Loading weights took 9.82 seconds
INFO 05-12 16:17:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.556263 seconds
INFO 05-12 16:17:54 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:17:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:17:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:17:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:17:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:17:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.41it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:18:16 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:18:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.33 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 63] energy=4955.85 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:18:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:18:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:18:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:18:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 16:18:30 [loader.py:458] Loading weights took 9.68 seconds
INFO 05-12 16:18:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.021611 seconds
INFO 05-12 16:18:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:18:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:18:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:18:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:18:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:18:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:17,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.61it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.61it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.61it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:12,  1.62it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.61it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.61it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.46it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.37it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.42it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.47it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:18:54 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:18:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.29 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.79 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.79 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 64] energy=4911.03 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:18:56 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:18:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:18:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:18:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.95s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]

INFO 05-12 16:19:08 [loader.py:458] Loading weights took 10.26 seconds
INFO 05-12 16:19:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.643461 seconds
INFO 05-12 16:19:09 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:19:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:19:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:19:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:19:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:19:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.61it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.61it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.61it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.61it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.61it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.61it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.47it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:06,  1.38it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.44it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.48it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:19:32 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:19:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.33 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.73 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.73 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 65] energy=4991.70 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:19:34 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:19:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:19:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:19:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 16:19:45 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 16:19:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.080363 seconds
INFO 05-12 16:19:46 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:19:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:19:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:19:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:19:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:19:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.48it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:09,  1.38it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:08,  1.43it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.48it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:20:09 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:20:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.59 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.59 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 66] energy=4885.66 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:20:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:20:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:20:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:20:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.98s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.34s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.21s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.73s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.74s/it]

INFO 05-12 16:20:24 [loader.py:458] Loading weights took 11.17 seconds
INFO 05-12 16:20:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.748046 seconds
INFO 05-12 16:20:25 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:20:25 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:20:25 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:20:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:20:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:20:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:14,  1.40it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:13,  1.40it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.45it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.48it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:20:48 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:20:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.34 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.56 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.56 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 67] energy=5090.56 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:20:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:20:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:20:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:20:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.70s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.02s/it]

INFO 05-12 16:21:05 [loader.py:458] Loading weights took 12.20 seconds
INFO 05-12 16:21:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.525483 seconds
INFO 05-12 16:21:06 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:21:06 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:21:06 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:21:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:21:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:21:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.44it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:21,  1.36it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:19,  1.43it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.48it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:21:29 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:21:29 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.68 toks/s, output: 101.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.68 toks/s, output: 101.77 toks/s]
[winogrande thr=0.20 | sample 68] energy=5275.32 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:21:32 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:21:32 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:21:33 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:21:33 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.52s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.90s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.62s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.29s/it]

INFO 05-12 16:21:47 [loader.py:458] Loading weights took 13.30 seconds
INFO 05-12 16:21:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.630444 seconds
INFO 05-12 16:21:48 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:21:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:21:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:21:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:21:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:21:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 16:22:10 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:22:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.94 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.56 toks/s, output: 101.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.56 toks/s, output: 101.92 toks/s]
[winogrande thr=0.20 | sample 69] energy=5363.18 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:22:12 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:22:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:22:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:22:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.39s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.73s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.41s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.78s/it]

INFO 05-12 16:22:25 [loader.py:458] Loading weights took 11.32 seconds
INFO 05-12 16:22:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.657749 seconds
INFO 05-12 16:22:26 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 16:22:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:22:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:22:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:22:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:22:27 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.61it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 16:22:48 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:22:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.98 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.68 toks/s, output: 101.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.68 toks/s, output: 101.90 toks/s]
[winogrande thr=0.20 | sample 70] energy=5007.17 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:22:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:22:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:22:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:22:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.18s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]

INFO 05-12 16:23:02 [loader.py:458] Loading weights took 10.27 seconds
INFO 05-12 16:23:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.601323 seconds
INFO 05-12 16:23:03 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:23:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:23:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:23:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:23:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:23:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.61it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.61it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.61it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.61it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 16:23:26 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:23:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.98 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.69 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.69 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 71] energy=4864.88 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:23:28 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:23:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:23:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:23:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 16:23:39 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 16:23:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.072025 seconds
INFO 05-12 16:23:40 [worker.py:267] Memory profiling takes 0.37 seconds
INFO 05-12 16:23:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:23:40 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:23:40 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:23:40 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:23:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 16:24:02 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:24:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.92 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 40.92 toks/s, output: 99.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 40.92 toks/s, output: 99.79 toks/s]
[winogrande thr=0.20 | sample 72] energy=4841.13 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:24:05 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:24:05 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:24:06 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:24:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:24:16 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 16:24:16 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.236105 seconds
INFO 05-12 16:24:17 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:24:17 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:24:17 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:24:17 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:24:17 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:24:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.61it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.61it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.47it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.48it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:24:40 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:24:40 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.16 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 73] energy=4863.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:24:42 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:24:42 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:24:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:24:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 16:24:53 [loader.py:458] Loading weights took 9.86 seconds
INFO 05-12 16:24:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.243055 seconds
INFO 05-12 16:24:54 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:24:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:24:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:24:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:24:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:24:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.61it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.46it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:25:17 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:25:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.63 toks/s, output: 101.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.63 toks/s, output: 101.85 toks/s]
[winogrande thr=0.20 | sample 74] energy=4880.10 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:25:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:25:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:25:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:25:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.05s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:25:30 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 16:25:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.233366 seconds
INFO 05-12 16:25:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:25:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:25:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:25:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:25:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:25:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.61it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.44it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.41it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.46it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:25:54 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:25:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.32 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.90 toks/s]
[winogrande thr=0.20 | sample 75] energy=4889.23 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:25:57 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:25:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:25:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:25:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.07s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.26s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.17s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.54s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]

INFO 05-12 16:26:09 [loader.py:458] Loading weights took 10.64 seconds
INFO 05-12 16:26:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.972269 seconds
INFO 05-12 16:26:10 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:26:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:26:10 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:26:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:26:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:26:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:10,  1.47it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:10,  1.38it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:09,  1.44it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:08,  1.49it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:26:33 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:26:33 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.33 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 76] energy=5083.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:26:35 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:26:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:26:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:26:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:26:46 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 16:26:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.243080 seconds
INFO 05-12 16:26:47 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:26:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:26:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:26:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:26:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:26:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.47it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:13,  1.37it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.42it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.47it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:27:10 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:27:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.46 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.58 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.58 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 77] energy=4894.73 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:27:12 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:27:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:27:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:27:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 16:27:24 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 16:27:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.208648 seconds
INFO 05-12 16:27:25 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:27:25 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:27:25 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:27:25 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:27:25 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:27:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:16,  1.38it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:15,  1.42it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.47it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.49it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:27:48 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:27:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 78] energy=4937.81 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:27:50 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:27:50 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:27:51 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:27:51 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 16:28:01 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 16:28:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.065055 seconds
INFO 05-12 16:28:02 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:28:02 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:28:02 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:28:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:28:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:28:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.42it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.46it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:28:25 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:28:25 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.23 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.63 toks/s, output: 101.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.63 toks/s, output: 101.87 toks/s]
[winogrande thr=0.20 | sample 79] energy=4873.55 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:28:27 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:28:27 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:28:28 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:28:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 16:28:38 [loader.py:458] Loading weights took 9.85 seconds
INFO 05-12 16:28:39 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.189642 seconds
INFO 05-12 16:28:40 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:28:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:28:40 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:28:40 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:28:40 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:28:40 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.61it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:21,  1.43it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:22,  1.36it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.43it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.48it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:29:02 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:29:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.33 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.62 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.62 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 80] energy=4876.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:29:04 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:29:04 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:29:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:29:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 16:29:16 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 16:29:16 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.113203 seconds
INFO 05-12 16:29:17 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 16:29:17 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:29:17 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:29:17 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:29:17 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:29:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:29,  1.16it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:24,  1.33it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:22,  1.44it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:21,  1.46it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.61it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.61it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:29:39 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:29:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.54 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.54 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 81] energy=4864.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:29:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:29:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:29:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:29:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]

INFO 05-12 16:29:53 [loader.py:458] Loading weights took 10.27 seconds
INFO 05-12 16:29:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.646974 seconds
INFO 05-12 16:29:54 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:29:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:29:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:29:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:29:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:29:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 16:30:16 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:30:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.92 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.61it/s, est. speed input: 326.93 toks/s, output: 86.52 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.53it/s, est. speed input: 326.93 toks/s, output: 86.52 toks/s]
[winogrande thr=0.20 | sample 82] energy=4589.25 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:30:18 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:30:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:30:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:30:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.08s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.68s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.64s/it]

INFO 05-12 16:30:30 [loader.py:458] Loading weights took 10.75 seconds
INFO 05-12 16:30:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.091935 seconds
INFO 05-12 16:30:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:30:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:30:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:30:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:30:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:30:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:30:54 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:30:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.12 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 83] energy=4940.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:30:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:30:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:30:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:30:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.10s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.33s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.74s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.28s/it]

INFO 05-12 16:31:11 [loader.py:458] Loading weights took 13.29 seconds
INFO 05-12 16:31:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.615081 seconds
INFO 05-12 16:31:12 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:31:12 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:31:12 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:31:12 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:31:12 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:31:12 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:31:34 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:31:34 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.36 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.69 toks/s, output: 101.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.69 toks/s, output: 101.87 toks/s]
[winogrande thr=0.20 | sample 84] energy=5310.53 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:31:36 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:31:36 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:31:38 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:31:38 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.13s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]

INFO 05-12 16:31:48 [loader.py:458] Loading weights took 10.06 seconds
INFO 05-12 16:31:49 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.387961 seconds
INFO 05-12 16:31:49 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:31:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:31:49 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:31:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:31:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:31:50 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.43it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.35it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:32:12 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:32:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.60 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.60 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 85] energy=4979.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:32:14 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:32:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:32:16 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:32:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 16:32:26 [loader.py:458] Loading weights took 9.68 seconds
INFO 05-12 16:32:26 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.029803 seconds
INFO 05-12 16:32:27 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:32:27 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:32:27 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:32:27 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:32:27 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:32:27 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.41it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.38it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.44it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.48it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.48it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:32:50 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:32:50 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.77 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.60 toks/s, output: 101.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.60 toks/s, output: 101.62 toks/s]
[winogrande thr=0.20 | sample 86] energy=4927.57 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:32:52 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:32:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:32:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:32:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.06s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 16:33:04 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 16:33:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.272506 seconds
INFO 05-12 16:33:05 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:33:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:33:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:33:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:33:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:33:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:07,  1.41it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.37it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.43it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.47it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:33:28 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:33:28 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.76 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.73 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.73 toks/s, output: 101.88 toks/s]
[winogrande thr=0.20 | sample 87] energy=4983.79 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:33:30 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:33:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:33:32 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:33:32 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 16:33:42 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 16:33:42 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.026011 seconds
INFO 05-12 16:33:43 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:33:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:33:43 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:33:43 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:33:43 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:33:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:10,  1.41it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:10,  1.35it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:09,  1.41it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.46it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.47it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:34:06 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:34:06 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.87 toks/s]
[winogrande thr=0.20 | sample 88] energy=4967.56 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:34:08 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:34:08 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:34:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:34:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 16:34:19 [loader.py:458] Loading weights took 9.97 seconds
INFO 05-12 16:34:20 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.319616 seconds
INFO 05-12 16:34:20 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:34:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:34:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:34:21 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:34:21 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:34:21 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:34:43 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:34:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.45 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.56 toks/s, output: 101.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.56 toks/s, output: 101.65 toks/s]
[winogrande thr=0.20 | sample 89] energy=4899.69 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:34:45 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:34:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:34:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:34:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 16:34:57 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 16:34:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.130498 seconds
INFO 05-12 16:34:58 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:34:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:34:58 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:34:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:34:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:34:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.41it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:19,  1.34it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:17,  1.41it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.43it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.48it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:35:21 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:35:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.72 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.67 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.67 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 90] energy=4924.77 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:35:23 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:35:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:35:24 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:35:24 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:35:34 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 16:35:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.244945 seconds
INFO 05-12 16:35:35 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:35:35 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:35:35 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:35:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:35:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:35:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:22,  1.37it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:22,  1.34it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.41it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:19,  1.46it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:35:58 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:35:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.70 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.71 toks/s]
[winogrande thr=0.20 | sample 91] energy=4947.95 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:36:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:36:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:36:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:36:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 16:36:12 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 16:36:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.062255 seconds
INFO 05-12 16:36:13 [worker.py:267] Memory profiling takes 0.48 seconds
INFO 05-12 16:36:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:36:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:36:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:36:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:36:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:25,  1.32it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.46it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:36:36 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:36:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.68 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.60 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.60 toks/s, output: 101.61 toks/s]
[winogrande thr=0.20 | sample 92] energy=4907.87 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:36:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:36:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:36:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:36:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.95s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.57s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.58s/it]

INFO 05-12 16:36:50 [loader.py:458] Loading weights took 10.50 seconds
INFO 05-12 16:36:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.838827 seconds
INFO 05-12 16:36:51 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:36:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:36:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:36:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:36:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:36:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:37:14 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:37:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.31 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.56 toks/s, output: 101.51 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.56 toks/s, output: 101.51 toks/s]
[winogrande thr=0.20 | sample 93] energy=4973.65 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:37:16 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:37:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:37:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:37:17 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.22s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 16:37:28 [loader.py:458] Loading weights took 10.16 seconds
INFO 05-12 16:37:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.496305 seconds
INFO 05-12 16:37:29 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:37:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:37:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:37:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:37:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:37:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:37:51 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:37:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.37 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 94] energy=4934.85 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:37:53 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:37:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:37:55 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:37:55 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.24s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.16s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]

INFO 05-12 16:38:05 [loader.py:458] Loading weights took 10.27 seconds
INFO 05-12 16:38:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.680521 seconds
INFO 05-12 16:38:06 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:38:06 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:38:06 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:38:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:38:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:38:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:38:29 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:38:29 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.40 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.51 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.51 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 95] energy=4961.51 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:38:31 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:38:31 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:38:33 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:38:33 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:38:43 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 16:38:43 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.224032 seconds
INFO 05-12 16:38:44 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:38:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:38:44 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:38:44 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:38:44 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:38:44 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:39:07 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:39:07 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.44 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 34.06 toks/s, output: 100.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 34.06 toks/s, output: 100.18 toks/s]
[winogrande thr=0.20 | sample 96] energy=4970.33 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:39:09 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:39:09 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:39:10 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:39:10 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 16:39:21 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 16:39:21 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.265354 seconds
INFO 05-12 16:39:22 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 16:39:22 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:39:22 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:39:22 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:39:22 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:39:22 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.46it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.33it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.40it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.45it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.49it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:39:45 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:39:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.77 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.82 toks/s]
[winogrande thr=0.20 | sample 97] energy=4945.27 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:39:47 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:39:47 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:39:48 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:39:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 16:39:58 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 16:39:58 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.092826 seconds
INFO 05-12 16:39:59 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:39:59 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:39:59 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:39:59 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:39:59 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:39:59 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.42it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:05,  1.37it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.42it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.47it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:40:22 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:40:22 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.77 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 98] energy=4909.63 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:40:24 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:40:24 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:40:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:40:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 16:40:36 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 16:40:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.143405 seconds
INFO 05-12 16:40:37 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:40:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:40:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:40:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:40:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:40:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.46it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.34it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.40it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.45it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.48it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 16:41:00 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:41:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.89 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.67 toks/s, output: 101.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.67 toks/s, output: 101.75 toks/s]
[winogrande thr=0.20 | sample 99] energy=4966.59 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:41:02 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:41:02 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:41:03 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:41:03 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 16:41:04 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.772459 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 16:41:14 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 16:41:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.891722 seconds
INFO 05-12 16:41:15 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:41:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:41:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:41:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:41:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:41:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.47it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.39it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.42it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.46it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.49it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:41:38 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:41:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.77 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.50 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.50 toks/s, output: 101.71 toks/s]
[winogrande thr=0.20 | sample 100] energy=5018.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:41:40 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:41:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:41:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:41:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.06s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.43s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.26s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.80s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.81s/it]

INFO 05-12 16:41:53 [loader.py:458] Loading weights took 11.45 seconds
INFO 05-12 16:41:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.791790 seconds
INFO 05-12 16:41:55 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:41:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:41:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:41:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:41:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:41:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:42:17 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:42:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.58 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.54 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.54 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 101] energy=5100.46 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:42:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:42:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:42:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:42:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.63s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.22s/it]

INFO 05-12 16:42:34 [loader.py:458] Loading weights took 13.08 seconds
INFO 05-12 16:42:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.420935 seconds
INFO 05-12 16:42:35 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:42:35 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:42:35 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:42:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:42:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:42:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:42:58 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:42:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.55 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.71 toks/s, output: 101.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.71 toks/s, output: 101.66 toks/s]
[winogrande thr=0.20 | sample 102] energy=5302.73 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:43:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:43:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:43:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:43:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.41s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.66s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.64s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.79s/it]

INFO 05-12 16:43:13 [loader.py:458] Loading weights took 11.36 seconds
INFO 05-12 16:43:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.696907 seconds
INFO 05-12 16:43:14 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:43:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:43:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:43:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:43:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:43:15 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:43:37 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:43:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.50 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.56 toks/s, output: 101.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.56 toks/s, output: 101.68 toks/s]
[winogrande thr=0.20 | sample 103] energy=5107.75 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:43:39 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:43:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:43:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:43:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.13s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.15s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 16:43:51 [loader.py:458] Loading weights took 10.18 seconds
INFO 05-12 16:43:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.665139 seconds
INFO 05-12 16:43:52 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:43:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:43:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:43:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:43:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:43:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:44:15 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:44:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.53 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.67 toks/s, output: 101.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.67 toks/s, output: 101.85 toks/s]
[winogrande thr=0.20 | sample 104] energy=4946.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:44:17 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:44:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:44:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:44:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 16:44:29 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 16:44:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.097989 seconds
INFO 05-12 16:44:30 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:44:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:44:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:44:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:44:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:44:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.49it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:44:53 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:44:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 32.94 toks/s, output: 99.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 32.94 toks/s, output: 99.81 toks/s]
[winogrande thr=0.20 | sample 105] energy=4959.45 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:44:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:44:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:44:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:44:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 16:45:06 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 16:45:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.072553 seconds
INFO 05-12 16:45:07 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:45:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:45:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:45:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:45:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:45:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.44it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.33it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.39it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.42it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.46it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 16:45:31 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:45:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.99 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.62 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.62 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 106] energy=4950.85 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:45:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:45:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:45:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:45:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 16:45:44 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 16:45:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.083494 seconds
INFO 05-12 16:45:45 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:45:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:45:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:45:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:45:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:45:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:46:08 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:46:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.62 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.74 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.74 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 107] energy=4916.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:46:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:46:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:46:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:46:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 16:46:22 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 16:46:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.123737 seconds
INFO 05-12 16:46:23 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:46:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:46:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:46:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:46:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:46:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.49it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:10,  1.38it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.43it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.48it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:46:46 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:46:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.82 toks/s, output: 101.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.82 toks/s, output: 101.90 toks/s]
[winogrande thr=0.20 | sample 108] energy=4940.90 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:46:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:46:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:46:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:46:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 16:46:59 [loader.py:458] Loading weights took 9.69 seconds
INFO 05-12 16:47:00 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.107182 seconds
INFO 05-12 16:47:00 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:47:00 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:47:00 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:47:01 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:47:01 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:47:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:13,  1.40it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.41it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.46it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.48it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:47:23 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:47:23 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.37 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 109] energy=4882.41 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:47:25 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:47:25 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:47:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:47:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 16:47:36 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 16:47:37 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.093901 seconds
INFO 05-12 16:47:37 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:47:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:47:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:47:38 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:47:38 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:47:38 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.47it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 16:48:00 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:48:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.21 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s, est. speed input: 169.43 toks/s, output: 94.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s, est. speed input: 169.43 toks/s, output: 94.67 toks/s]
[winogrande thr=0.20 | sample 110] energy=4602.03 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:48:01 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:48:01 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:48:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:48:03 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.89s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.08s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]

INFO 05-12 16:48:13 [loader.py:458] Loading weights took 10.03 seconds
INFO 05-12 16:48:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.371655 seconds
INFO 05-12 16:48:14 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:48:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:48:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:48:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:48:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:48:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.45it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:18,  1.35it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.42it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.46it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:48:37 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:48:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.40 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.62 toks/s, output: 101.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.62 toks/s, output: 101.87 toks/s]
[winogrande thr=0.20 | sample 111] energy=4911.19 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:48:39 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:48:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:48:40 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:48:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 16:48:50 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 16:48:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.080837 seconds
INFO 05-12 16:48:51 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:48:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:48:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:48:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:48:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:48:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:21,  1.37it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:20,  1.39it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.45it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.49it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:49:14 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:49:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.42 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.58 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.58 toks/s, output: 101.88 toks/s]
[winogrande thr=0.20 | sample 112] energy=4900.43 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:49:16 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:49:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:49:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:49:17 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 16:49:27 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 16:49:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.126308 seconds
INFO 05-12 16:49:28 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:49:28 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:49:28 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:49:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:49:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:49:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:26,  1.30it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:26,  1.23it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:23,  1.35it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:21,  1.44it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.49it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:49:51 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:49:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.39 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]
[winogrande thr=0.20 | sample 113] energy=4923.01 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:49:53 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:49:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:49:54 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:49:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.48s/it]

INFO 05-12 16:50:05 [loader.py:458] Loading weights took 10.20 seconds
INFO 05-12 16:50:05 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.529522 seconds
INFO 05-12 16:50:06 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:50:06 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:50:06 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:50:06 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:50:06 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:50:06 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:50:29 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:50:29 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.57 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.57 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 114] energy=4977.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:50:31 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:50:31 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:50:32 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:50:32 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.18s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.47s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]

INFO 05-12 16:50:43 [loader.py:458] Loading weights took 10.30 seconds
INFO 05-12 16:50:43 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.703602 seconds
INFO 05-12 16:50:44 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:50:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:50:44 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:50:44 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:50:44 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:50:44 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 16:51:07 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:51:07 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.58 toks/s, output: 101.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.58 toks/s, output: 101.87 toks/s]
[winogrande thr=0.20 | sample 115] energy=4973.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:51:09 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:51:09 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:51:10 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:51:10 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.22s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.50s/it]

INFO 05-12 16:51:21 [loader.py:458] Loading weights took 10.20 seconds
INFO 05-12 16:51:21 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.524321 seconds
INFO 05-12 16:51:22 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:51:22 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:51:22 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:51:22 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:51:22 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:51:22 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:51:45 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:51:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.39 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.63 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.63 toks/s, output: 101.71 toks/s]
[winogrande thr=0.20 | sample 116] energy=4952.59 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:51:47 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:51:47 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:51:48 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:51:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.15s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.12s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.50s/it]

INFO 05-12 16:51:59 [loader.py:458] Loading weights took 10.21 seconds
INFO 05-12 16:51:59 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.546256 seconds
INFO 05-12 16:52:00 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:52:00 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:52:00 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:52:00 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:52:00 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:52:00 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:52:22 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:52:22 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 37.73 toks/s, output: 99.29 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 37.73 toks/s, output: 99.29 toks/s]
[winogrande thr=0.20 | sample 117] energy=4955.41 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:52:24 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:52:24 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:52:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:52:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 16:52:36 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 16:52:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.231730 seconds
INFO 05-12 16:52:37 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:52:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:52:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:52:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:52:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:52:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.48it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.36it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.41it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:53:00 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:53:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.66 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.66 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 118] energy=4943.52 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:53:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:53:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:53:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:53:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.12s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.26s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.54s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]

INFO 05-12 16:53:15 [loader.py:458] Loading weights took 10.58 seconds
INFO 05-12 16:53:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.912759 seconds
INFO 05-12 16:53:16 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:53:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:53:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:53:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:53:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:53:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:06,  1.46it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.36it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.42it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.46it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:53:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:53:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.83 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 119] energy=5138.74 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:53:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:53:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:53:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:53:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 16:53:53 [loader.py:458] Loading weights took 9.84 seconds
INFO 05-12 16:53:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.196088 seconds
INFO 05-12 16:53:54 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:53:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:53:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:53:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:53:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:53:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:09,  1.39it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.38it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.43it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.47it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:54:17 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:54:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.75 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 120] energy=4952.12 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:54:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:54:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:54:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:54:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.92s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.33s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.20s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.72s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.73s/it]

INFO 05-12 16:54:32 [loader.py:458] Loading weights took 11.13 seconds
INFO 05-12 16:54:32 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.522426 seconds
INFO 05-12 16:54:33 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:54:33 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:54:33 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:54:33 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:54:33 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:54:33 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.43it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:14,  1.35it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.41it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.43it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.47it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:54:56 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:54:56 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.73 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.73 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 121] energy=5094.63 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:54:58 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:54:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:54:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:54:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.50s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.87s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.58s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.14s/it]

INFO 05-12 16:55:12 [loader.py:458] Loading weights took 12.70 seconds
INFO 05-12 16:55:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.116864 seconds
INFO 05-12 16:55:14 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 16:55:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:55:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:55:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:55:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:55:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.41it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:20,  1.34it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.41it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.46it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.49it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:55:37 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:55:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.69 toks/s, output: 101.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.69 toks/s, output: 101.65 toks/s]
[winogrande thr=0.20 | sample 122] energy=5276.00 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:55:39 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:55:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:55:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:55:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.90s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.10s/it]

INFO 05-12 16:55:54 [loader.py:458] Loading weights took 12.59 seconds
INFO 05-12 16:55:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.937455 seconds
INFO 05-12 16:55:55 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:55:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:55:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:55:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:55:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:55:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:56:17 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:56:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.36 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.71 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.71 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 123] energy=5323.75 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:56:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:56:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:56:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:56:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.19s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:07,  3.51s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.50s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.65s/it]

INFO 05-12 16:56:32 [loader.py:458] Loading weights took 10.83 seconds
INFO 05-12 16:56:32 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.186218 seconds
INFO 05-12 16:56:33 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:56:33 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:56:33 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:56:33 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:56:33 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:56:33 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 16:56:56 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 16:56:56 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.40 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 124] energy=5008.31 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:56:58 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:56:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:56:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:56:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.97s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.16s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 16:57:10 [loader.py:458] Loading weights took 10.17 seconds
INFO 05-12 16:57:10 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.562928 seconds
INFO 05-12 16:57:11 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:57:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:57:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:57:11 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:57:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:57:11 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:57:34 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:57:34 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s, est. speed input: 35.78 toks/s, output: 101.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s, est. speed input: 35.78 toks/s, output: 101.57 toks/s]
[winogrande thr=0.20 | sample 125] energy=4936.95 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:57:36 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:57:36 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:57:37 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:57:37 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.92s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.45s/it]

INFO 05-12 16:57:47 [loader.py:458] Loading weights took 10.03 seconds
INFO 05-12 16:57:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.391683 seconds
INFO 05-12 16:57:48 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:57:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:57:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:57:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:57:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:57:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 16:58:11 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:58:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 30.98 toks/s, output: 99.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 30.98 toks/s, output: 99.93 toks/s]
[winogrande thr=0.20 | sample 126] energy=5017.36 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:58:14 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:58:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:58:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:58:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 16:58:25 [loader.py:458] Loading weights took 9.83 seconds
INFO 05-12 16:58:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.161426 seconds
INFO 05-12 16:58:26 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:58:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:58:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:58:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:58:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:58:27 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.38it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.40it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.45it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.48it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 16:58:49 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:58:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.84 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.81 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.81 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 127] energy=4953.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:58:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:58:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:58:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:58:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 16:59:03 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 16:59:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.619326 seconds
INFO 05-12 16:59:04 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:59:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:59:04 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:59:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:59:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:59:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:06,  1.44it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.36it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.42it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.46it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 16:59:28 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 16:59:28 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.77 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.73 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.73 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 128] energy=5005.01 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 16:59:30 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 16:59:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 16:59:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 16:59:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 16:59:41 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 16:59:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.053933 seconds
INFO 05-12 16:59:42 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 16:59:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 16:59:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 16:59:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 16:59:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 16:59:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:09,  1.41it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.38it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.43it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.47it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:00:05 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:00:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.78 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.67 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.67 toks/s, output: 101.80 toks/s]
[winogrande thr=0.20 | sample 129] energy=4930.37 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:00:07 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:00:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:00:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:00:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.98s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.50s/it]

INFO 05-12 17:00:19 [loader.py:458] Loading weights took 10.19 seconds
INFO 05-12 17:00:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.579675 seconds
INFO 05-12 17:00:20 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:00:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:00:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:00:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:00:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:00:21 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:13,  1.44it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:13,  1.34it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:12,  1.40it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:11,  1.44it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.47it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.48it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:00:44 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:00:44 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.08 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.59 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.59 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 130] energy=5021.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:00:46 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:00:46 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:00:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:00:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 17:00:57 [loader.py:458] Loading weights took 9.83 seconds
INFO 05-12 17:00:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.170908 seconds
INFO 05-12 17:00:58 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 17:00:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:00:58 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:00:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:00:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:00:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.49it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:16,  1.36it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:15,  1.38it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.43it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.47it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:01:21 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:01:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.00 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 131] energy=4971.55 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:01:24 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:01:24 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:01:25 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:01:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 17:01:36 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 17:01:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.243747 seconds
INFO 05-12 17:01:37 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:01:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:01:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:01:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:01:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:01:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:21,  1.38it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:20,  1.33it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.40it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:18,  1.44it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:17,  1.47it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:02:00 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:02:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.93 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.82 toks/s, output: 101.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.82 toks/s, output: 101.90 toks/s]
[winogrande thr=0.20 | sample 132] energy=5029.73 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:02:02 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:02:02 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:02:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:02:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:02:14 [loader.py:458] Loading weights took 9.69 seconds
INFO 05-12 17:02:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.042666 seconds
INFO 05-12 17:02:15 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:02:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:02:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:02:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:02:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:02:15 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:28,  1.21it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:25,  1.29it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:22,  1.42it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:21,  1.48it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:02:37 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:02:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.60 toks/s, output: 101.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.60 toks/s, output: 101.62 toks/s]
[winogrande thr=0.20 | sample 133] energy=4918.51 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:02:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:02:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:02:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:02:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.93s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.48s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.48s/it]

INFO 05-12 17:02:52 [loader.py:458] Loading weights took 10.12 seconds
INFO 05-12 17:02:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.447700 seconds
INFO 05-12 17:02:53 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:02:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:02:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:02:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:02:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:02:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 17:03:16 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:03:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.11 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.66 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.66 toks/s, output: 101.88 toks/s]
[winogrande thr=0.20 | sample 134] energy=5046.02 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:03:18 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:03:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:03:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:03:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.19s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.48s/it]

INFO 05-12 17:03:30 [loader.py:458] Loading weights took 10.12 seconds
INFO 05-12 17:03:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.459752 seconds
INFO 05-12 17:03:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:03:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:03:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:03:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:03:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:03:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 17:03:53 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:03:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.12 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.60 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.60 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 135] energy=4914.03 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:03:56 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:03:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:03:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:03:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.18s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.18s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.53s/it]

INFO 05-12 17:04:08 [loader.py:458] Loading weights took 10.33 seconds
INFO 05-12 17:04:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.658403 seconds
INFO 05-12 17:04:09 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:04:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:04:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:04:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:04:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:04:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 17:04:31 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:04:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.09 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.59 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.59 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 136] energy=4982.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:04:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:04:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:04:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:04:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.05s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 17:04:45 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 17:04:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.543588 seconds
INFO 05-12 17:04:46 [worker.py:267] Memory profiling takes 0.37 seconds
INFO 05-12 17:04:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:04:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:04:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:04:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:04:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 17:05:09 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:05:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.04 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 32.13 toks/s, output: 100.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 32.13 toks/s, output: 100.42 toks/s]
[winogrande thr=0.20 | sample 137] energy=4900.37 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:05:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:05:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:05:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:05:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.91s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.12s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.50s/it]

INFO 05-12 17:05:23 [loader.py:458] Loading weights took 10.21 seconds
INFO 05-12 17:05:23 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.549358 seconds
INFO 05-12 17:05:24 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:05:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:05:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:05:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:05:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:05:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.39it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.42it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.46it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:05:46 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:05:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.42 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.61 toks/s, output: 101.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.61 toks/s, output: 101.90 toks/s]
[winogrande thr=0.20 | sample 138] energy=4958.65 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:05:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:05:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:05:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:05:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.07s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.53s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.53s/it]

INFO 05-12 17:06:00 [loader.py:458] Loading weights took 10.35 seconds
INFO 05-12 17:06:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.732964 seconds
INFO 05-12 17:06:01 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:06:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:06:01 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:06:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:06:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:06:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.46it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:07,  1.36it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:06,  1.42it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.46it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:06:25 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:06:25 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.72 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.88 toks/s]
[winogrande thr=0.20 | sample 139] energy=5011.69 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:06:27 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:06:27 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:06:28 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:06:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.41s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.80s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.50s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.08s/it]

INFO 05-12 17:06:41 [loader.py:458] Loading weights took 12.45 seconds
INFO 05-12 17:06:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.777600 seconds
INFO 05-12 17:06:42 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:06:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:06:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:06:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:06:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:06:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:13,  1.38it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.39it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.45it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.49it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:07:05 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:07:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.50 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 140] energy=5243.98 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:07:07 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:07:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:07:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:07:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.19s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.19s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.50s/it]

INFO 05-12 17:07:19 [loader.py:458] Loading weights took 10.23 seconds
INFO 05-12 17:07:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.587834 seconds
INFO 05-12 17:07:20 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:07:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:07:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:07:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:07:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:07:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.43it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:16,  1.40it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:15,  1.42it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.46it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.49it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:07:43 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:07:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.57 toks/s, output: 101.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.57 toks/s, output: 101.90 toks/s]
[winogrande thr=0.20 | sample 141] energy=4962.71 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:07:45 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:07:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:07:46 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:07:46 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:07:56 [loader.py:458] Loading weights took 9.68 seconds
INFO 05-12 17:07:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.016847 seconds
INFO 05-12 17:07:57 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:07:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:07:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:07:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:07:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:07:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:19,  1.42it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.38it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:18,  1.44it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.48it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:08:20 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:08:20 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.49 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]
[winogrande thr=0.20 | sample 142] energy=4888.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:08:22 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:08:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:08:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:08:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:08:33 [loader.py:458] Loading weights took 9.68 seconds
INFO 05-12 17:08:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.170782 seconds
INFO 05-12 17:08:34 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:08:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:08:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:08:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:08:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:08:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:23,  1.36it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:23,  1.31it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:21,  1.40it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.46it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:08:57 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:08:57 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.52 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.49 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.49 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 143] energy=4913.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:08:59 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:08:59 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:09:01 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:09:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 17:09:11 [loader.py:458] Loading weights took 9.97 seconds
INFO 05-12 17:09:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.493929 seconds
INFO 05-12 17:09:12 [worker.py:267] Memory profiling takes 0.48 seconds
INFO 05-12 17:09:12 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:09:12 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:09:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:09:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:09:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 17:09:35 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:09:35 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.29 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.63 toks/s, output: 101.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.63 toks/s, output: 101.92 toks/s]
[winogrande thr=0.20 | sample 144] energy=4919.05 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:09:37 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:09:37 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:09:38 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:09:38 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.50s/it]

INFO 05-12 17:09:49 [loader.py:458] Loading weights took 10.21 seconds
INFO 05-12 17:09:49 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.539964 seconds
INFO 05-12 17:09:50 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:09:50 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:09:50 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:09:50 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:09:50 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:09:50 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 17:10:12 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:10:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.10 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 145] energy=4955.38 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:10:15 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:10:15 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:10:16 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:10:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.20s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.50s/it]

INFO 05-12 17:10:26 [loader.py:458] Loading weights took 10.23 seconds
INFO 05-12 17:10:27 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.568504 seconds
INFO 05-12 17:10:27 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:10:27 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:10:27 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:10:28 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:10:28 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:10:28 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 17:10:50 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:10:50 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.07 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.62 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.62 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 146] energy=4912.62 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:10:52 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:10:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:10:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:10:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.15s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.13s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.48s/it]

INFO 05-12 17:11:04 [loader.py:458] Loading weights took 10.14 seconds
INFO 05-12 17:11:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.466066 seconds
INFO 05-12 17:11:05 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:11:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:11:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:11:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:11:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:11:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:11:27 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:11:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.40 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.58 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.58 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 147] energy=4941.62 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:11:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:11:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:11:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:11:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.90s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.47s/it]

INFO 05-12 17:11:41 [loader.py:458] Loading weights took 10.07 seconds
INFO 05-12 17:11:42 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.409707 seconds
INFO 05-12 17:11:42 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:11:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:11:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:11:43 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:11:43 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:11:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.45it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.35it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.40it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.43it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:12:06 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:12:06 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.90 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.59 toks/s, output: 101.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.59 toks/s, output: 101.77 toks/s]
[winogrande thr=0.20 | sample 148] energy=5015.46 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:12:08 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:12:08 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:12:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:12:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:12:19 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 17:12:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.149471 seconds
INFO 05-12 17:12:20 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:12:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:12:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:12:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:12:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:12:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:05,  1.39it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:04,  1.38it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.44it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.47it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:12:43 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:12:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.89 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.53 toks/s, output: 101.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.53 toks/s, output: 101.82 toks/s]
[winogrande thr=0.20 | sample 149] energy=4945.44 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:12:45 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:12:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:12:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:12:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 17:12:57 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 17:12:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.146500 seconds
INFO 05-12 17:12:58 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:12:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:12:58 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:12:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:12:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:12:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:08,  1.41it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:08,  1.33it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:07,  1.40it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.45it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.46it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.49it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:13:21 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:13:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.90 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.54 toks/s, output: 101.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.54 toks/s, output: 101.57 toks/s]
[winogrande thr=0.20 | sample 150] energy=4975.20 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:13:23 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:13:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:13:24 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:13:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 17:13:35 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 17:13:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.129008 seconds
INFO 05-12 17:13:36 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:13:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:13:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:13:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:13:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:13:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:11,  1.38it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.39it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.44it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.48it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:13:59 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:13:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.72 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.69 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.69 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 151] energy=4924.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:14:01 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:14:01 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:14:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:14:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 17:14:12 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 17:14:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.355854 seconds
INFO 05-12 17:14:13 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:14:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:14:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:14:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:14:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:14:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.44it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:14,  1.36it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:13,  1.41it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.46it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:14:36 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:14:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.54 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.66 toks/s, output: 101.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.66 toks/s, output: 101.87 toks/s]
[winogrande thr=0.20 | sample 152] energy=4924.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:14:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:14:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:14:40 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:14:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.72s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:14:50 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 17:14:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.107121 seconds
INFO 05-12 17:14:51 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:14:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:14:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:14:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:14:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:14:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:17,  1.40it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:17,  1.39it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.45it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:15:14 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:15:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.56 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.60 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.60 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 153] energy=4917.71 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:15:16 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:15:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:15:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:15:17 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 17:15:27 [loader.py:458] Loading weights took 9.88 seconds
INFO 05-12 17:15:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.249170 seconds
INFO 05-12 17:15:28 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:15:28 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:15:28 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:15:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:15:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:15:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:21,  1.33it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:20,  1.37it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.43it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.47it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:15:51 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:15:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.50 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.70 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.70 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 154] energy=4887.94 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:15:53 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:15:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:15:55 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:15:55 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]

INFO 05-12 17:16:05 [loader.py:458] Loading weights took 10.25 seconds
INFO 05-12 17:16:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.593500 seconds
INFO 05-12 17:16:06 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:16:06 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:16:06 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:16:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:16:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:16:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:27,  1.24it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:26,  1.24it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:23,  1.38it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:21,  1.46it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:16:29 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:16:29 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.49 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.67 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.67 toks/s, output: 101.80 toks/s]
[winogrande thr=0.20 | sample 155] energy=4963.40 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:16:31 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:16:31 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:16:32 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:16:32 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.51s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.88s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.57s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.25s/it]

INFO 05-12 17:16:46 [loader.py:458] Loading weights took 13.16 seconds
INFO 05-12 17:16:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.494329 seconds
INFO 05-12 17:16:47 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:16:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:16:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:16:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:16:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:16:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 17:17:09 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:17:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 46.72 toks/s, output: 101.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 46.72 toks/s, output: 101.57 toks/s]
[winogrande thr=0.20 | sample 156] energy=5244.90 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:17:12 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:17:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:17:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:17:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.98s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.92s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.50s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.80s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.98s/it]

INFO 05-12 17:17:26 [loader.py:458] Loading weights took 12.12 seconds
INFO 05-12 17:17:27 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.453289 seconds
INFO 05-12 17:17:27 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:17:27 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:17:27 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:17:28 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:17:28 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:17:28 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.45it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.37it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:17:50 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:17:50 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.79 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 157] energy=5338.86 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:17:52 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:17:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:17:54 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:17:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 17:18:04 [loader.py:458] Loading weights took 9.82 seconds
INFO 05-12 17:18:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.170992 seconds
INFO 05-12 17:18:05 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 17:18:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:18:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:18:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:18:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:18:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.37it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.37it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.43it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.44it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.48it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:18:28 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:18:28 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.83 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.62 toks/s, output: 101.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.62 toks/s, output: 101.82 toks/s]
[winogrande thr=0.20 | sample 158] energy=4955.37 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:18:30 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:18:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:18:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:18:32 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 17:18:42 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 17:18:42 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.227178 seconds
INFO 05-12 17:18:43 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:18:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:18:43 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:18:43 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:18:43 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:18:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.43it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.36it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.41it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.46it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:04,  1.49it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:19:06 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:19:06 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.83 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.75 toks/s]
[winogrande thr=0.20 | sample 159] energy=4941.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:19:08 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:19:08 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:19:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:19:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 17:19:20 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 17:19:20 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.273705 seconds
INFO 05-12 17:19:21 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:19:21 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:19:21 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:19:21 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:19:21 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:19:21 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.39it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:10,  1.39it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:09,  1.43it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.45it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.48it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:19:44 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:19:44 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.99 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.46 toks/s, output: 101.48 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.46 toks/s, output: 101.48 toks/s]
[winogrande thr=0.20 | sample 160] energy=4994.90 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:19:46 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:19:46 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:19:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:19:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 17:19:58 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 17:19:58 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.094422 seconds
INFO 05-12 17:19:59 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:19:59 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:19:59 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:19:59 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:19:59 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:19:59 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.49it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:20:22 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:20:22 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.76 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.48 toks/s, output: 101.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.48 toks/s, output: 101.55 toks/s]
[winogrande thr=0.20 | sample 161] energy=4963.72 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:20:24 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:20:24 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:20:25 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:20:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 17:20:35 [loader.py:458] Loading weights took 9.88 seconds
INFO 05-12 17:20:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.258636 seconds
INFO 05-12 17:20:36 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:20:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:20:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:20:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:20:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:20:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:19,  1.46it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:20,  1.34it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:18,  1.38it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:17,  1.43it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.47it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 17:21:00 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:21:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.03 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 21.32it/s, est. speed input: 796.49 toks/s, output: 64.56 toks/s]
[winogrande thr=0.20 | sample 162] energy=4653.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:21:01 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:21:01 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:21:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:21:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:21:12 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 17:21:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.066166 seconds
INFO 05-12 17:21:13 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:21:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:21:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:21:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:21:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:21:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:22,  1.38it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:21,  1.37it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.43it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:19,  1.47it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:21:37 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:21:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.94 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.51 toks/s, output: 101.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.51 toks/s, output: 101.55 toks/s]
[winogrande thr=0.20 | sample 163] energy=4939.30 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:21:39 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:21:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:21:40 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:21:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 17:21:41 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.553294 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 17:21:51 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 17:21:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.647595 seconds
INFO 05-12 17:21:52 [worker.py:267] Memory profiling takes 0.44 seconds
INFO 05-12 17:21:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:21:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:21:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:21:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:21:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:22:15 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:22:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 164] energy=5064.12 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:22:18 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:22:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:22:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:22:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.95s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.23s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.28s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.55s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.62s/it]

INFO 05-12 17:22:30 [loader.py:458] Loading weights took 10.70 seconds
INFO 05-12 17:22:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.040121 seconds
INFO 05-12 17:22:31 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 17:22:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:22:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:22:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:22:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:22:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:22:54 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:22:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.60 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.54 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.54 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 165] energy=5104.78 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:22:56 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:22:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:22:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:22:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 17:23:08 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 17:23:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.222802 seconds
INFO 05-12 17:23:09 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:23:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:23:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:23:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:23:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:23:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:23:32 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:23:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.58 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.28 toks/s, output: 100.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.28 toks/s, output: 100.77 toks/s]
[winogrande thr=0.20 | sample 166] energy=4931.98 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:23:34 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:23:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:23:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:23:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:23:45 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 17:23:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.051941 seconds
INFO 05-12 17:23:46 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:23:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:23:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:23:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:23:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:23:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.46it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.42it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.46it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.48it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:24:10 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:24:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.92 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.64 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.64 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 167] energy=4976.90 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:24:13 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:24:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:24:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:24:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:24:24 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 17:24:24 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.091921 seconds
INFO 05-12 17:24:25 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:24:25 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:24:25 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:24:25 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:24:25 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:24:25 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.43it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:07,  1.33it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.37it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.42it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.46it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:04,  1.49it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 17:24:48 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:24:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.00 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 168] energy=5054.53 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:24:50 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:24:50 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:24:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:24:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:25:02 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 17:25:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.037639 seconds
INFO 05-12 17:25:03 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:25:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:25:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:25:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:25:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:25:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.39it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.42it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.46it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.48it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:25:26 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:25:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.91 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.61 toks/s, output: 101.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.61 toks/s, output: 101.90 toks/s]
[winogrande thr=0.20 | sample 169] energy=4950.70 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:25:28 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:25:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:25:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:25:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:25:40 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 17:25:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.047633 seconds
INFO 05-12 17:25:41 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:25:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:25:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:25:41 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:25:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:25:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:14,  1.40it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:14,  1.32it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:13,  1.38it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:12,  1.41it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:11,  1.45it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.48it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 17:26:04 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:26:04 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.07 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.64 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.64 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 170] energy=4990.36 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:26:06 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:26:06 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:26:07 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:26:07 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:26:17 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 17:26:18 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.061380 seconds
INFO 05-12 17:26:18 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:26:18 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:26:18 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:26:19 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:26:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:26:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.48it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:17,  1.35it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:16,  1.38it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:15,  1.43it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.47it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 17:26:42 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:26:42 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.90 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.68 toks/s, output: 101.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.68 toks/s, output: 101.75 toks/s]
[winogrande thr=0.20 | sample 171] energy=4944.75 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:26:44 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:26:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:26:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:26:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 17:26:55 [loader.py:458] Loading weights took 9.83 seconds
INFO 05-12 17:26:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.168472 seconds
INFO 05-12 17:26:56 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:26:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:26:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:26:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:26:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:26:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:20,  1.35it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.39it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.45it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.48it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:27:19 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:27:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.49 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.75 toks/s, output: 101.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.75 toks/s, output: 101.82 toks/s]
[winogrande thr=0.20 | sample 172] energy=4904.03 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:27:21 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:27:21 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:27:22 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:27:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 17:27:33 [loader.py:458] Loading weights took 9.88 seconds
INFO 05-12 17:27:33 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.210586 seconds
INFO 05-12 17:27:34 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:27:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:27:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:27:34 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:27:34 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:27:34 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:25,  1.31it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:24,  1.31it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:22,  1.37it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.44it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:27:56 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:27:56 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.61 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.61 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 173] energy=4896.91 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:27:58 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:27:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizerFast'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
INFO 05-12 17:28:00 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:28:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.50s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.81s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.17s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.16s/it]

INFO 05-12 17:28:13 [loader.py:458] Loading weights took 12.79 seconds
INFO 05-12 17:28:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.132564 seconds
INFO 05-12 17:28:14 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:28:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:28:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:28:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:28:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:28:15 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:28:37 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:28:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.25 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.72 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.72 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 174] energy=5279.52 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:28:40 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:28:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:28:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:28:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.39s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.16s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.63s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.90s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.13s/it]

INFO 05-12 17:28:54 [loader.py:458] Loading weights took 12.69 seconds
INFO 05-12 17:28:55 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.055598 seconds
INFO 05-12 17:28:55 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:28:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:28:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:28:56 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:28:56 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:28:56 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:29:18 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:29:18 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.70 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 33.63 toks/s, output: 98.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 33.63 toks/s, output: 98.92 toks/s]
[winogrande thr=0.20 | sample 175] energy=5382.22 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:29:20 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:29:20 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:29:22 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:29:22 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.91s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 17:29:32 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 17:29:32 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.266486 seconds
INFO 05-12 17:29:33 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:29:33 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:29:33 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:29:33 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:29:33 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:29:33 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:02,  1.47it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.34it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.39it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.43it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 17:29:57 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:29:57 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.15 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.55 toks/s, output: 101.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.55 toks/s, output: 101.77 toks/s]
[winogrande thr=0.20 | sample 176] energy=5009.98 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:29:59 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:29:59 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:30:00 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:30:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 17:30:10 [loader.py:458] Loading weights took 9.82 seconds
INFO 05-12 17:30:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.158716 seconds
INFO 05-12 17:30:11 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:30:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:30:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:30:12 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:30:12 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:30:12 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.41it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:05,  1.32it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:04,  1.38it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.43it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.46it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.49it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 17:30:35 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:30:35 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.61 toks/s]
[winogrande thr=0.20 | sample 177] energy=4996.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:30:37 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:30:37 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:30:38 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:30:38 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:30:48 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 17:30:49 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.125905 seconds
INFO 05-12 17:30:49 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:30:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:30:49 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:30:50 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:30:50 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:30:50 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.48it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.36it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.38it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.43it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.46it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.48it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:04,  1.49it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 17:31:13 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:31:13 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.59 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.59 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 178] energy=4973.00 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:31:15 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:31:15 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:31:16 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:31:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 17:31:26 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 17:31:27 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.363379 seconds
INFO 05-12 17:31:27 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:31:27 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:31:27 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:31:28 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:31:28 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:31:28 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.42it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:12,  1.33it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:11,  1.38it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.43it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.46it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.49it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 17:31:51 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:31:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.07 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.57 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.57 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 179] energy=5009.66 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:31:53 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:31:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:31:54 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:31:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:32:04 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 17:32:05 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.062788 seconds
INFO 05-12 17:32:05 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:32:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:32:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:32:06 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:32:06 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:32:06 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.45it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:16,  1.32it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:15,  1.36it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:14,  1.42it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:13,  1.43it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.47it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 17:32:29 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:32:29 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.08 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.81 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.81 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 180] energy=4967.17 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:32:31 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:32:31 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:32:32 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:32:32 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:32:42 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 17:32:43 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.068357 seconds
INFO 05-12 17:32:43 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:32:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:32:43 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:32:44 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:32:44 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:32:44 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:19,  1.40it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:19,  1.34it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:19,  1.30it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:19,  1.25it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:18,  1.21it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:09<00:18,  1.19it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:10<00:17,  1.19it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:11<00:16,  1.20it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:12<00:15,  1.21it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:12<00:14,  1.22it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:13<00:14,  1.21it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:14<00:13,  1.21it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:15<00:12,  1.24it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:16<00:11,  1.22it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:16<00:10,  1.24it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:17<00:09,  1.23it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:18<00:08,  1.23it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:19<00:07,  1.25it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:20<00:07,  1.27it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:20<00:06,  1.27it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:21<00:05,  1.28it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:22<00:04,  1.30it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:23<00:03,  1.30it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:24<00:03,  1.26it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:24<00:02,  1.23it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:25<00:01,  1.22it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:26<00:00,  1.20it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:27<00:00,  1.21it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:27<00:00,  1.28it/s]
INFO 05-12 17:33:11 [model_runner.py:1598] Graph capturing finished in 27 secs, took 0.06 GiB
INFO 05-12 17:33:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 28.40 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 38.96 toks/s, output: 97.40 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 38.96 toks/s, output: 97.40 toks/s]
[winogrande thr=0.20 | sample 181] energy=5520.86 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:33:13 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:33:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:33:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:33:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.52s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.78s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.53s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.92s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.01s/it]

INFO 05-12 17:33:28 [loader.py:458] Loading weights took 12.30 seconds
INFO 05-12 17:33:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.677816 seconds
INFO 05-12 17:33:29 [worker.py:267] Memory profiling takes 0.43 seconds
INFO 05-12 17:33:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:33:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:33:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:33:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:33:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:25,  1.33it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:25,  1.28it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:25,  1.25it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:03<00:24,  1.26it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:23,  1.27it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:22,  1.29it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:05<00:20,  1.37it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.43it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.48it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:07<00:16,  1.49it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:09<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:11<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:18<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 17:33:52 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:33:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.61 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.61 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 182] energy=5336.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:33:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:33:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:33:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:33:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.95s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.13s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.50s/it]

INFO 05-12 17:34:06 [loader.py:458] Loading weights took 10.15 seconds
INFO 05-12 17:34:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.489077 seconds
INFO 05-12 17:34:07 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:34:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:34:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:34:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:34:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:34:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 17:34:30 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:34:30 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.16 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.88 toks/s]
[winogrande thr=0.20 | sample 183] energy=4906.62 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:34:32 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:34:32 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:34:33 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:34:33 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 17:34:44 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 17:34:44 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.592600 seconds
INFO 05-12 17:34:45 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:34:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:34:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:34:45 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:34:45 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:34:45 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 17:35:08 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:35:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.18 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s, est. speed input: 208.81 toks/s, output: 91.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s, est. speed input: 208.81 toks/s, output: 91.34 toks/s]
[winogrande thr=0.20 | sample 184] energy=4657.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:35:09 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:35:09 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:35:10 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:35:10 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]

INFO 05-12 17:35:20 [loader.py:458] Loading weights took 9.67 seconds
INFO 05-12 17:35:20 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.017149 seconds
INFO 05-12 17:35:21 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:35:21 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:35:21 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:35:21 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:35:21 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:35:21 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 17:35:44 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:35:44 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.18 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.56 toks/s, output: 101.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.56 toks/s, output: 101.85 toks/s]
[winogrande thr=0.20 | sample 185] energy=4852.46 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:35:46 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:35:46 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:35:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:35:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.05s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.45s/it]

INFO 05-12 17:35:58 [loader.py:458] Loading weights took 10.00 seconds
INFO 05-12 17:35:58 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.503166 seconds
INFO 05-12 17:35:59 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:35:59 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:35:59 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:35:59 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:35:59 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:35:59 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:36:21 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:36:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.29 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 35.94 toks/s, output: 101.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 35.94 toks/s, output: 101.66 toks/s]
[winogrande thr=0.20 | sample 186] energy=4953.69 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:36:23 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:36:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:36:27 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:36:27 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:36:37 [loader.py:458] Loading weights took 9.69 seconds
INFO 05-12 17:36:37 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.022556 seconds
INFO 05-12 17:36:38 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:36:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:36:38 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:36:38 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:36:38 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:36:38 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:37:01 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:37:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.58 toks/s, output: 101.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.58 toks/s, output: 101.75 toks/s]
[winogrande thr=0.20 | sample 187] energy=5129.33 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:37:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:37:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:37:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:37:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:37:14 [loader.py:458] Loading weights took 9.69 seconds
INFO 05-12 17:37:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.036968 seconds
INFO 05-12 17:37:15 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:37:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:37:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:37:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:37:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:37:15 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:37:38 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:37:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.28 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 188] energy=4868.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:37:40 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:37:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:37:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:37:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 17:37:51 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 17:37:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.143254 seconds
INFO 05-12 17:37:52 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:37:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:37:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:37:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:37:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:37:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:38:15 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:38:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.26 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.53 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.53 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 189] energy=4893.14 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:38:17 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:38:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:38:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:38:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 17:38:28 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 17:38:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.272423 seconds
INFO 05-12 17:38:29 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:38:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:38:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:38:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:38:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:38:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:38:52 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:38:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.27 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.74 toks/s, output: 101.91 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.74 toks/s, output: 101.91 toks/s]
[winogrande thr=0.20 | sample 190] energy=4883.12 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:38:54 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:38:54 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:38:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:38:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.98s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.16s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.45s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]

INFO 05-12 17:39:07 [loader.py:458] Loading weights took 10.28 seconds
INFO 05-12 17:39:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.632275 seconds
INFO 05-12 17:39:08 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:39:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:39:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:39:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:39:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:39:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 17:39:31 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:39:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.58 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.64 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.64 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 191] energy=5132.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:39:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:39:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:39:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:39:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 17:39:46 [loader.py:458] Loading weights took 9.82 seconds
INFO 05-12 17:39:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.315305 seconds
INFO 05-12 17:39:46 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:39:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:39:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:39:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:39:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:39:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:40:09 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:40:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.33 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.52 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.52 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 192] energy=5003.82 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:40:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:40:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:40:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:40:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 17:40:23 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 17:40:23 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.121961 seconds
INFO 05-12 17:40:24 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:40:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:40:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:40:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:40:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:40:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:40:46 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:40:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.30 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.57 toks/s, output: 101.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.57 toks/s, output: 101.82 toks/s]
[winogrande thr=0.20 | sample 193] energy=4892.16 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:40:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:40:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:40:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:40:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 17:40:51 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.720229 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.05s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.45s/it]

INFO 05-12 17:41:01 [loader.py:458] Loading weights took 10.03 seconds
INFO 05-12 17:41:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.947945 seconds
INFO 05-12 17:41:02 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 17:41:02 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:41:02 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:41:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:41:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:41:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:41:24 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:41:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.32 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.63 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.63 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 194] energy=4957.54 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:41:26 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:41:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:41:28 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:41:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.05s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 17:41:38 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 17:41:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.301741 seconds
INFO 05-12 17:41:39 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:41:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:41:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:41:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:41:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:41:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:42:02 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:42:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.31 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.50 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.50 toks/s, output: 101.71 toks/s]
[winogrande thr=0.20 | sample 195] energy=4914.72 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:42:04 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:42:04 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:42:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:42:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 17:42:15 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 17:42:16 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.279434 seconds
INFO 05-12 17:42:16 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:42:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:42:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:42:17 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:42:17 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:42:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:42:39 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:42:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.44 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.51 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.51 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 196] energy=4908.28 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:42:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:42:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:42:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:42:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.24s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:07,  3.53s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.87s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.89s/it]

INFO 05-12 17:42:55 [loader.py:458] Loading weights took 11.78 seconds
INFO 05-12 17:42:55 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.104771 seconds
INFO 05-12 17:42:56 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:42:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:42:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:42:56 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:42:56 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:42:56 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:43:19 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:43:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.68 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.63 toks/s, output: 101.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.63 toks/s, output: 101.75 toks/s]
[winogrande thr=0.20 | sample 197] energy=5155.59 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:43:21 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:43:21 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:43:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:43:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.65s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.56s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.13s/it]

INFO 05-12 17:43:36 [loader.py:458] Loading weights took 12.69 seconds
INFO 05-12 17:43:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.014428 seconds
INFO 05-12 17:43:37 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:43:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:43:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:43:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:43:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:43:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:44:00 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:44:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.67 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.55 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.55 toks/s, output: 101.81 toks/s]
[winogrande thr=0.20 | sample 198] energy=5325.63 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:44:02 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:44:02 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:44:03 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:44:03 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.34s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.37s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.45s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.60s/it]

INFO 05-12 17:44:14 [loader.py:458] Loading weights took 10.61 seconds
INFO 05-12 17:44:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.949941 seconds
INFO 05-12 17:44:15 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:44:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:44:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:44:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:44:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:44:15 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:44:38 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:44:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.67 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.48 toks/s, output: 101.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.48 toks/s, output: 101.55 toks/s]
[winogrande thr=0.20 | sample 199] energy=5035.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:44:40 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:44:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:44:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:44:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:44:52 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 17:44:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.055465 seconds
INFO 05-12 17:44:53 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:44:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:44:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:44:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:44:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:44:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:45:16 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:45:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.68 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.60 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.60 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 200] energy=4939.26 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:45:18 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:45:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:45:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:45:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.15s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.31s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.19s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.57s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.64s/it]

INFO 05-12 17:45:31 [loader.py:458] Loading weights took 10.71 seconds
INFO 05-12 17:45:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.124398 seconds
INFO 05-12 17:45:32 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:45:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:45:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:45:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:45:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:45:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.42it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:45:55 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:45:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.54 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 201] energy=5114.72 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:45:57 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:45:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:45:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:45:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:46:08 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 17:46:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.057677 seconds
INFO 05-12 17:46:09 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:46:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:46:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:46:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:46:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:46:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:46:32 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:46:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.65 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.51 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.51 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 202] energy=4920.06 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:46:34 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:46:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:46:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:46:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:46:46 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 17:46:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.047253 seconds
INFO 05-12 17:46:47 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:46:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:46:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:46:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:46:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:46:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:47:10 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:47:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.81 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.55 toks/s, output: 101.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.55 toks/s, output: 101.62 toks/s]
[winogrande thr=0.20 | sample 203] energy=4978.57 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:47:12 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:47:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:47:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:47:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:47:24 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 17:47:24 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.048401 seconds
INFO 05-12 17:47:25 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:47:25 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:47:25 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:47:25 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:47:25 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:47:25 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:47:48 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:47:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.72 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.63 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.63 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 204] energy=4929.42 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:47:50 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:47:50 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:47:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:47:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:48:02 [loader.py:458] Loading weights took 9.69 seconds
INFO 05-12 17:48:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.166490 seconds
INFO 05-12 17:48:03 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:48:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:48:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:48:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:48:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:48:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:48:26 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:48:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.43 toks/s, output: 101.45 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.43 toks/s, output: 101.45 toks/s]
[winogrande thr=0.20 | sample 205] energy=4995.21 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:48:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:48:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:48:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:48:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:48:40 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 17:48:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.053592 seconds
INFO 05-12 17:48:41 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 17:48:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:48:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:48:41 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:48:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:48:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:49:04 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:49:04 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.75 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.55 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.55 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 206] energy=4978.18 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:49:06 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:49:06 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:49:07 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:49:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 17:49:18 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 17:49:18 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.135352 seconds
INFO 05-12 17:49:19 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:49:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:49:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:49:19 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:49:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:49:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 17:49:42 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:49:42 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.75 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.69 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.69 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 207] energy=4953.62 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:49:44 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:49:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:49:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:49:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:49:55 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 17:49:55 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.062153 seconds
INFO 05-12 17:49:56 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:49:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:49:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:49:56 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:49:56 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:49:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:50:19 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:50:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.88 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.47 toks/s, output: 101.51 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.47 toks/s, output: 101.51 toks/s]
[winogrande thr=0.20 | sample 208] energy=4925.25 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:50:21 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:50:21 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:50:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:50:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.03s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.24s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.16s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]

INFO 05-12 17:50:34 [loader.py:458] Loading weights took 10.57 seconds
INFO 05-12 17:50:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.919939 seconds
INFO 05-12 17:50:35 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 17:50:35 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:50:35 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:50:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:50:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:50:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:50:58 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:50:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.62 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 209] energy=5079.40 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:51:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:51:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:51:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:51:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:51:12 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 17:51:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.049970 seconds
INFO 05-12 17:51:13 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 17:51:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:51:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:51:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:51:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:51:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:51:36 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:51:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.73 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.71 toks/s]
[winogrande thr=0.20 | sample 210] energy=4956.11 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:51:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:51:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:51:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:51:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 17:51:51 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 17:51:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.762943 seconds
INFO 05-12 17:51:52 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:51:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:51:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:51:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:51:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:51:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 17:52:15 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 17:52:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.66 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.50 toks/s, output: 101.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.50 toks/s, output: 101.66 toks/s]
[winogrande thr=0.20 | sample 211] energy=5104.09 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:52:17 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:52:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:52:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:52:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 17:52:29 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 17:52:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.105761 seconds
INFO 05-12 17:52:30 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:52:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:52:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:52:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:52:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:52:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:52:52 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:52:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 212] energy=4920.05 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:52:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:52:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:52:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:52:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 17:53:06 [loader.py:458] Loading weights took 9.82 seconds
INFO 05-12 17:53:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.151842 seconds
INFO 05-12 17:53:07 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:53:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:53:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:53:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:53:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:53:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:53:30 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:53:30 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.33 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.68 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.68 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 213] energy=4902.31 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:53:32 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:53:32 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:53:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:53:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.05s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 17:53:44 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 17:53:44 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.262894 seconds
INFO 05-12 17:53:45 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:53:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:53:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:53:45 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:53:45 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:53:45 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:54:08 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:54:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.68 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.68 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 214] energy=4982.07 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:54:10 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:54:10 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:54:11 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:54:11 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.45s/it]

INFO 05-12 17:54:22 [loader.py:458] Loading weights took 10.01 seconds
INFO 05-12 17:54:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.656674 seconds
INFO 05-12 17:54:23 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:54:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:54:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:54:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:54:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:54:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:54:45 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:54:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.37 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 215] energy=4967.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:54:47 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:54:47 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:54:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:54:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.39s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.80s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.10s/it]

INFO 05-12 17:55:02 [loader.py:458] Loading weights took 12.53 seconds
INFO 05-12 17:55:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.869086 seconds
INFO 05-12 17:55:03 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:55:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:55:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:55:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:55:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:55:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:55:25 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:55:25 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.39 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.63 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.63 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 216] energy=5214.14 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:55:28 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:55:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:55:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:55:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.37s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.38s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.23s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.45s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.60s/it]

INFO 05-12 17:55:40 [loader.py:458] Loading weights took 10.63 seconds
INFO 05-12 17:55:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.961022 seconds
INFO 05-12 17:55:41 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:55:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:55:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:55:41 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:55:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:55:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:56:04 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:56:04 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.43 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.63 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.63 toks/s, output: 101.89 toks/s]
[winogrande thr=0.20 | sample 217] energy=5026.77 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:56:06 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:56:06 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:56:07 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:56:07 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 17:56:08 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.719175 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 17:56:18 [loader.py:458] Loading weights took 9.86 seconds
INFO 05-12 17:56:18 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.777137 seconds
INFO 05-12 17:56:19 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:56:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:56:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:56:19 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:56:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:56:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:56:42 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:56:42 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.34 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.44 toks/s, output: 101.48 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.44 toks/s, output: 101.48 toks/s]
[winogrande thr=0.20 | sample 218] energy=4973.65 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:56:44 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:56:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:56:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:56:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 17:56:55 [loader.py:458] Loading weights took 9.86 seconds
INFO 05-12 17:56:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.183341 seconds
INFO 05-12 17:56:56 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:56:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:56:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:56:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:56:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:56:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:57:19 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:57:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.48 toks/s, output: 101.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.48 toks/s, output: 101.55 toks/s]
[winogrande thr=0.20 | sample 219] energy=4878.55 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:57:22 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:57:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:57:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:57:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.18s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.37s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.25s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.69s/it]

INFO 05-12 17:57:35 [loader.py:458] Loading weights took 10.97 seconds
INFO 05-12 17:57:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.318048 seconds
INFO 05-12 17:57:36 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 17:57:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:57:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:57:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:57:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:57:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:57:58 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:57:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.38 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.77 toks/s, output: 101.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.77 toks/s, output: 101.87 toks/s]
[winogrande thr=0.20 | sample 220] energy=5139.69 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:58:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:58:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:58:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:58:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 17:58:12 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 17:58:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.139988 seconds
INFO 05-12 17:58:13 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 17:58:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:58:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:58:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:58:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:58:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:58:36 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:58:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.39 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 40.84 toks/s, output: 99.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 40.84 toks/s, output: 99.60 toks/s]
[winogrande thr=0.20 | sample 221] energy=4880.73 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:58:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:58:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:58:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:58:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 17:58:50 [loader.py:458] Loading weights took 9.98 seconds
INFO 05-12 17:58:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.303250 seconds
INFO 05-12 17:58:51 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:58:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:58:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:58:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:58:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:58:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 17:59:13 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:59:13 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.58 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.58 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 222] energy=4978.40 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:59:16 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:59:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:59:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:59:17 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 17:59:27 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 17:59:27 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.128110 seconds
INFO 05-12 17:59:28 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 17:59:28 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 17:59:28 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 17:59:28 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 17:59:28 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 17:59:28 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 17:59:51 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 17:59:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.43 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.60 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.60 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 223] energy=4900.36 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 17:59:53 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 17:59:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 17:59:54 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 17:59:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 18:00:04 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 18:00:05 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.035744 seconds
INFO 05-12 18:00:05 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:00:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:00:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:00:06 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:00:06 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:00:06 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:00:28 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:00:28 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.68 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.67 toks/s, output: 101.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.67 toks/s, output: 101.85 toks/s]
[winogrande thr=0.20 | sample 224] energy=4902.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:00:30 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:00:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:00:32 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:00:32 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 18:00:42 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 18:00:42 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.222268 seconds
INFO 05-12 18:00:43 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:00:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:00:43 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:00:43 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:00:43 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:00:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:01:06 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:01:06 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.64 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.64 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 225] energy=4937.60 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:01:08 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:01:08 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:01:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:01:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:01:19 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 18:01:20 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.081491 seconds
INFO 05-12 18:01:20 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:01:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:01:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:01:21 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:01:21 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:01:21 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:01:43 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:01:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.61 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 226] energy=4922.97 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:01:45 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:01:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:01:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:01:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 18:01:57 [loader.py:458] Loading weights took 9.82 seconds
INFO 05-12 18:01:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.150464 seconds
INFO 05-12 18:01:58 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:01:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:01:58 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:01:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:01:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:01:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:02:21 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:02:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.48 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.72 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.72 toks/s, output: 101.80 toks/s]
[winogrande thr=0.20 | sample 227] energy=4901.38 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:02:23 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:02:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:02:24 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:02:24 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.94s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.11s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 18:02:35 [loader.py:458] Loading weights took 10.17 seconds
INFO 05-12 18:02:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.497939 seconds
INFO 05-12 18:02:36 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:02:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:02:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:02:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:02:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:02:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 18:02:58 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:02:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.34 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 29.40 toks/s, output: 101.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 29.40 toks/s, output: 101.39 toks/s]
[winogrande thr=0.20 | sample 228] energy=4939.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:03:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:03:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:03:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:03:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.13s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.49s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.29s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.83s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.85s/it]

INFO 05-12 18:03:14 [loader.py:458] Loading weights took 11.62 seconds
INFO 05-12 18:03:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.957679 seconds
INFO 05-12 18:03:15 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:03:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:03:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:03:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:03:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:03:15 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 18:03:37 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:03:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.38 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.50 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.50 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 229] energy=5122.94 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:03:39 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:03:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:03:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:03:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.95s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.06s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 18:03:51 [loader.py:458] Loading weights took 9.98 seconds
INFO 05-12 18:03:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.313295 seconds
INFO 05-12 18:03:52 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:03:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:03:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:03:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:03:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:03:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:04:15 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:04:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.49 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.56 toks/s, output: 101.56 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.56 toks/s, output: 101.56 toks/s]
[winogrande thr=0.20 | sample 230] energy=4929.25 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:04:17 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:04:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:04:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:04:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 18:04:28 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 18:04:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.137620 seconds
INFO 05-12 18:04:29 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:04:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:04:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:04:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:04:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:04:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:04:52 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:04:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.58 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.68 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.68 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 231] energy=4927.48 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:04:54 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:04:54 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:04:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:04:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:05:06 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 18:05:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.077331 seconds
INFO 05-12 18:05:07 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:05:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:05:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:05:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:05:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:05:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:05:30 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:05:30 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.61 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.57 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.57 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 232] energy=4927.95 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:05:32 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:05:32 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:05:33 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:05:33 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 18:05:34 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.568353 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:05:44 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 18:05:44 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.517471 seconds
INFO 05-12 18:05:45 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:05:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:05:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:05:45 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:05:45 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:05:45 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:06:08 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:06:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.57 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]
[winogrande thr=0.20 | sample 233] energy=4951.35 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:06:10 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:06:10 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:06:11 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:06:11 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 18:06:21 [loader.py:458] Loading weights took 9.86 seconds
INFO 05-12 18:06:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.190516 seconds
INFO 05-12 18:06:22 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 18:06:22 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:06:22 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:06:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:06:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:06:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:06:45 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:06:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.54 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.53 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.53 toks/s, output: 101.61 toks/s]
[winogrande thr=0.20 | sample 234] energy=4948.07 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:06:47 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:06:47 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:06:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:06:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:06:59 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 18:06:59 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.099739 seconds
INFO 05-12 18:07:00 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:07:00 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:07:00 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:07:00 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:07:00 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:07:00 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:07:23 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:07:23 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.56 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.51 toks/s, output: 101.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.51 toks/s, output: 101.50 toks/s]
[winogrande thr=0.20 | sample 235] energy=4904.64 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:07:25 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:07:25 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:07:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:07:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 18:07:36 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 18:07:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.036474 seconds
INFO 05-12 18:07:37 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:07:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:07:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:07:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:07:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:07:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:08:00 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:08:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.55 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.57 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 236] energy=4908.35 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:08:02 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:08:02 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:08:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:08:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.14s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.28s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.16s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.55s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.63s/it]

INFO 05-12 18:08:15 [loader.py:458] Loading weights took 10.64 seconds
INFO 05-12 18:08:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.007215 seconds
INFO 05-12 18:08:16 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 18:08:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:08:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:08:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:08:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:08:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:08:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:08:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.61 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.59 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.59 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 237] energy=5090.42 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:08:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:08:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:08:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:08:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:08:52 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 18:08:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.106908 seconds
INFO 05-12 18:08:53 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:08:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:08:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:08:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:08:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:08:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:09:16 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:09:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.61 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.57 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.57 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 238] energy=4915.64 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:09:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:09:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:09:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:09:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 18:09:31 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 18:09:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.217805 seconds
INFO 05-12 18:09:32 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:09:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:09:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:09:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:09:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:09:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:09:54 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:09:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.44 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.56 toks/s, output: 101.51 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.56 toks/s, output: 101.51 toks/s]
[winogrande thr=0.20 | sample 239] energy=5003.01 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:09:56 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:09:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:09:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:09:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:10:08 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 18:10:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.118855 seconds
INFO 05-12 18:10:09 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:10:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:10:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:10:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:10:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:10:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:10:32 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:10:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.52 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.64 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.64 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 240] energy=4917.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:10:34 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:10:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:10:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:10:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 18:10:46 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 18:10:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.415100 seconds
INFO 05-12 18:10:47 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:10:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:10:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:10:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:10:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:10:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:11:10 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:11:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.50 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.53 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.53 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 241] energy=4951.36 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:11:12 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:11:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:11:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:11:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 18:11:23 [loader.py:458] Loading weights took 9.84 seconds
INFO 05-12 18:11:24 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.171896 seconds
INFO 05-12 18:11:24 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:11:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:11:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:11:25 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:11:25 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:11:25 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:11:47 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:11:47 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.74 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.74 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 242] energy=4928.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:11:49 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:11:49 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:11:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:11:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:12:00 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 18:12:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.065237 seconds
INFO 05-12 18:12:02 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:12:02 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:12:02 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:12:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:12:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:12:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:12:24 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:12:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.45 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.61 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.61 toks/s, output: 101.86 toks/s]
[winogrande thr=0.20 | sample 243] energy=4895.94 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:12:27 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:12:27 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:12:28 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:12:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:12:38 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 18:12:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.134997 seconds
INFO 05-12 18:12:39 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:12:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:12:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:12:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:12:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:12:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:13:02 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:13:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 244] energy=4941.76 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:13:04 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:13:04 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:13:06 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:13:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.96s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.28s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.18s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.68s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.69s/it]

INFO 05-12 18:13:17 [loader.py:458] Loading weights took 10.99 seconds
INFO 05-12 18:13:17 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.388231 seconds
INFO 05-12 18:13:18 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 18:13:18 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:13:18 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:13:18 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:13:18 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:13:18 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:13:41 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:13:41 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.46 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.67 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.67 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 245] energy=5064.27 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:13:43 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:13:43 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:13:44 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:13:44 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.37s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.75s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.04s/it]

INFO 05-12 18:13:57 [loader.py:458] Loading weights took 12.30 seconds
INFO 05-12 18:13:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.857429 seconds
INFO 05-12 18:13:58 [worker.py:267] Memory profiling takes 0.43 seconds
INFO 05-12 18:13:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:13:58 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:13:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:13:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:13:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:14:21 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:14:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.48 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.50 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.50 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 246] energy=5229.16 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:14:23 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:14:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:14:25 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:14:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.63s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.69s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.68s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.83s/it]

INFO 05-12 18:14:37 [loader.py:458] Loading weights took 11.51 seconds
INFO 05-12 18:14:37 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.937591 seconds
INFO 05-12 18:14:38 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:14:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:14:38 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:14:38 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:14:38 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:14:38 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:15:00 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:15:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.50 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.58 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.58 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 247] energy=5183.28 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:15:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:15:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:15:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:15:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 18:15:14 [loader.py:458] Loading weights took 9.81 seconds
INFO 05-12 18:15:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.145968 seconds
INFO 05-12 18:15:15 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:15:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:15:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:15:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:15:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:15:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:15:38 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:15:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.51 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.58 toks/s, output: 101.87 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.58 toks/s, output: 101.87 toks/s]
[winogrande thr=0.20 | sample 248] energy=4947.39 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:15:40 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:15:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:15:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:15:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 18:15:52 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 18:15:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.445148 seconds
INFO 05-12 18:15:53 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:15:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:15:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:15:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:15:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:15:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:16:16 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:16:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.51 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 44.19 toks/s, output: 101.37 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 44.19 toks/s, output: 101.37 toks/s]
[winogrande thr=0.20 | sample 249] energy=4885.37 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:16:18 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:16:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:16:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:16:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 18:16:30 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 18:16:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.298746 seconds
INFO 05-12 18:16:31 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:16:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:16:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:16:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:16:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:16:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:16:53 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:16:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.45 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 250] energy=4927.85 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:16:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:16:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:16:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:16:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:17:07 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 18:17:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.133162 seconds
INFO 05-12 18:17:08 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:17:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:17:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:17:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:17:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:17:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:17:31 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:17:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.53 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.59 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 251] energy=4909.78 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:17:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:17:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:17:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:17:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 18:17:44 [loader.py:458] Loading weights took 9.87 seconds
INFO 05-12 18:17:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.202319 seconds
INFO 05-12 18:17:45 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:17:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:17:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:17:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:17:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:17:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:18:08 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:18:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.48 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.62 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.62 toks/s, output: 101.88 toks/s]
[winogrande thr=0.20 | sample 252] energy=4907.51 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:18:10 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:18:10 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:18:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:18:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 18:18:22 [loader.py:458] Loading weights took 9.68 seconds
INFO 05-12 18:18:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.006397 seconds
INFO 05-12 18:18:23 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:18:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:18:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:18:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:18:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:18:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:18:46 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:18:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.48 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.51 toks/s, output: 101.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.51 toks/s, output: 101.49 toks/s]
[winogrande thr=0.20 | sample 253] energy=4914.35 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:18:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:18:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:18:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:18:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.92s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.12s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.50s/it]

INFO 05-12 18:19:00 [loader.py:458] Loading weights took 10.21 seconds
INFO 05-12 18:19:00 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.538931 seconds
INFO 05-12 18:19:01 [worker.py:267] Memory profiling takes 0.44 seconds
INFO 05-12 18:19:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:19:01 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:19:01 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:19:01 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:19:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:19:24 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:19:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.61 toks/s, output: 101.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.61 toks/s, output: 101.68 toks/s]
[winogrande thr=0.20 | sample 254] energy=5013.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:19:26 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:19:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:19:28 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:19:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.88s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.06s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]

INFO 05-12 18:19:38 [loader.py:458] Loading weights took 10.05 seconds
INFO 05-12 18:19:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.377970 seconds
INFO 05-12 18:19:39 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:19:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:19:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:19:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:19:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:19:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:20:02 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:20:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.51 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.48 toks/s, output: 101.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.48 toks/s, output: 101.60 toks/s]
[winogrande thr=0.20 | sample 255] energy=5002.30 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:20:04 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:20:04 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:20:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:20:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 18:20:16 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 18:20:16 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.045600 seconds
INFO 05-12 18:20:17 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:20:17 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:20:17 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:20:17 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:20:17 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:20:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:20:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:20:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.54 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.54 toks/s, output: 101.78 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.54 toks/s, output: 101.78 toks/s]
[winogrande thr=0.20 | sample 256] energy=4931.96 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:20:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:20:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:20:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:20:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 18:20:53 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 18:20:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.183758 seconds
INFO 05-12 18:20:54 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:20:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:20:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:20:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:20:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:20:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:21:17 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:21:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.50 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.82 toks/s, output: 101.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.82 toks/s, output: 101.82 toks/s]
[winogrande thr=0.20 | sample 257] energy=4925.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:21:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:21:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:21:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:21:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.21s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.62s/it]

INFO 05-12 18:21:31 [loader.py:458] Loading weights took 10.71 seconds
INFO 05-12 18:21:32 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.054060 seconds
INFO 05-12 18:21:32 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:21:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:21:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:21:33 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:21:33 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:21:33 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 18:21:55 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:21:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.69 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.69 toks/s, output: 101.73 toks/s]
[winogrande thr=0.20 | sample 258] energy=5019.14 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:21:58 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:21:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:21:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:21:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.62s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.92s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.59s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.18s/it]

INFO 05-12 18:22:12 [loader.py:458] Loading weights took 12.84 seconds
INFO 05-12 18:22:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.180062 seconds
INFO 05-12 18:22:13 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:22:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:22:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:22:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:22:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:22:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:22:36 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:22:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.57 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.59 toks/s, output: 101.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.59 toks/s, output: 101.59 toks/s]
[winogrande thr=0.20 | sample 259] energy=5349.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:22:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:22:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:22:40 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:22:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.46s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.49s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.28s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.67s/it]

INFO 05-12 18:22:51 [loader.py:458] Loading weights took 10.88 seconds
INFO 05-12 18:22:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.219635 seconds
INFO 05-12 18:22:52 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:22:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:22:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:22:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:22:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:22:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:23:15 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:23:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.85 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.54 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.54 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 260] energy=5087.72 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:23:17 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:23:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:23:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:23:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 18:23:29 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 18:23:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.045328 seconds
INFO 05-12 18:23:30 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 18:23:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:23:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:23:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:23:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:23:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:23:53 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:23:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.77 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.65 toks/s, output: 101.85 toks/s]
[winogrande thr=0.20 | sample 261] energy=4947.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:23:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:23:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:23:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:23:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 18:24:07 [loader.py:458] Loading weights took 9.84 seconds
INFO 05-12 18:24:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.175453 seconds
INFO 05-12 18:24:08 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:24:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:24:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:24:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:24:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:24:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:24:31 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:24:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.79 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.54 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.54 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 262] energy=4949.64 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:24:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:24:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:24:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:24:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 18:24:44 [loader.py:458] Loading weights took 9.83 seconds
INFO 05-12 18:24:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.175452 seconds
INFO 05-12 18:24:45 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:24:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:24:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:24:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:24:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:24:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:25:08 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:25:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.76 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 263] energy=4945.38 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:25:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:25:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:25:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:25:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 18:25:22 [loader.py:458] Loading weights took 9.81 seconds
INFO 05-12 18:25:23 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.304188 seconds
INFO 05-12 18:25:23 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:25:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:25:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:25:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:25:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:25:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:25:46 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:25:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.78 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.56 toks/s, output: 101.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.56 toks/s, output: 101.68 toks/s]
[winogrande thr=0.20 | sample 264] energy=4972.24 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:25:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:25:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:25:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:25:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.97s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]

INFO 05-12 18:26:00 [loader.py:458] Loading weights took 10.20 seconds
INFO 05-12 18:26:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.525514 seconds
INFO 05-12 18:26:01 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:26:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:26:01 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:26:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:26:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:26:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.48it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:26:24 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:26:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.86 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.46 toks/s, output: 101.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.46 toks/s, output: 101.32 toks/s]
[winogrande thr=0.20 | sample 265] energy=4990.06 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:26:26 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:26:27 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:26:28 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:26:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 18:26:38 [loader.py:458] Loading weights took 9.86 seconds
INFO 05-12 18:26:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.187682 seconds
INFO 05-12 18:26:39 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:26:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:26:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:26:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:26:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:26:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.46it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 18:27:02 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:27:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.86 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.60 toks/s, output: 101.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.60 toks/s, output: 101.66 toks/s]
[winogrande thr=0.20 | sample 266] energy=4957.45 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:27:04 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:27:04 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:27:06 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:27:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 18:27:16 [loader.py:458] Loading weights took 9.68 seconds
INFO 05-12 18:27:16 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.026830 seconds
INFO 05-12 18:27:17 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:27:17 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:27:17 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:27:17 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:27:17 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:27:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:27:40 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:27:40 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.74 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.74 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 267] energy=4923.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:27:42 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:27:42 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:27:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:27:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 18:27:54 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 18:27:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.299594 seconds
INFO 05-12 18:27:55 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:27:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:27:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:27:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:27:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:27:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 18:28:17 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:28:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.38 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.79 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.79 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 268] energy=4925.80 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:28:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:28:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:28:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:28:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 18:28:31 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 18:28:32 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.272678 seconds
INFO 05-12 18:28:32 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:28:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:28:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:28:33 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:28:33 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:28:33 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 18:28:55 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:28:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.34 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.70 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.70 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 269] energy=4956.55 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:28:57 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:28:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:28:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:28:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 18:29:09 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 18:29:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.043352 seconds
INFO 05-12 18:29:10 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:29:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:29:10 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:29:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:29:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:29:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 18:29:32 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:29:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.56 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.56 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 270] energy=4895.76 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:29:34 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:29:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:29:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:29:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 18:29:46 [loader.py:458] Loading weights took 9.69 seconds
INFO 05-12 18:29:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.023843 seconds
INFO 05-12 18:29:47 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:29:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:29:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:29:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:29:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:29:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 18:30:10 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:30:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.56 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 271] energy=4902.50 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:30:12 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:30:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:30:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:30:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 18:30:23 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 18:30:24 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.287763 seconds
INFO 05-12 18:30:24 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:30:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:30:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:30:25 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:30:25 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:30:25 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 18:30:47 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 18:30:47 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.31 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.72 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.72 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 272] energy=4917.35 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:30:50 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:30:50 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:30:51 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:30:51 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.94s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.15s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.45s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]

INFO 05-12 18:31:02 [loader.py:458] Loading weights took 10.30 seconds
INFO 05-12 18:31:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.639299 seconds
INFO 05-12 18:31:03 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 18:31:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:31:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:31:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:31:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:31:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:31:26 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:31:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.78 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.62 toks/s, output: 101.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.62 toks/s, output: 101.77 toks/s]
[winogrande thr=0.20 | sample 273] energy=5086.15 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:31:28 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:31:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:31:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:31:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:31:39 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 18:31:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.114571 seconds
INFO 05-12 18:31:40 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:31:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:31:40 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:31:41 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:31:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:31:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 18:32:04 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:32:04 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.86 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.55 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.55 toks/s, output: 101.76 toks/s]
[winogrande thr=0.20 | sample 274] energy=4941.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:32:06 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:32:06 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:32:07 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:32:07 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:32:17 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 18:32:17 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.060903 seconds
INFO 05-12 18:32:18 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:32:18 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:32:18 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:32:18 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:32:18 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:32:18 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 18:32:41 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:32:41 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.99 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 275] energy=4961.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:32:43 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:32:43 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:32:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:32:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:32:55 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 18:32:55 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.098044 seconds
INFO 05-12 18:32:56 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:32:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:32:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:32:56 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:32:56 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:32:56 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 18:33:19 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:33:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.88 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.52 toks/s, output: 101.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.52 toks/s, output: 101.68 toks/s]
[winogrande thr=0.20 | sample 276] energy=4956.69 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:33:21 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:33:21 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:33:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:33:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.20s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.57s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.60s/it]

INFO 05-12 18:33:34 [loader.py:458] Loading weights took 10.59 seconds
INFO 05-12 18:33:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.926774 seconds
INFO 05-12 18:33:35 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 18:33:35 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:33:35 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:33:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:33:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:33:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:33:58 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:33:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.71 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.58 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.58 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 277] energy=5066.62 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:34:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:34:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:34:01 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:34:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:08,  4.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.70s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.20s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.26s/it]

INFO 05-12 18:34:15 [loader.py:458] Loading weights took 13.20 seconds
INFO 05-12 18:34:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.533547 seconds
INFO 05-12 18:34:16 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:34:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:34:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:34:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:34:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:34:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:34:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:34:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.69 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.74 toks/s]
[winogrande thr=0.20 | sample 278] energy=5352.13 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:34:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:34:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:34:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:34:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.33s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.36s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.21s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]

INFO 05-12 18:34:53 [loader.py:458] Loading weights took 10.58 seconds
INFO 05-12 18:34:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.159525 seconds
INFO 05-12 18:34:55 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:34:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:34:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:34:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:34:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:34:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:35:18 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:35:18 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.70 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.55 toks/s, output: 101.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.55 toks/s, output: 101.68 toks/s]
[winogrande thr=0.20 | sample 279] energy=5042.91 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:35:20 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:35:20 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:35:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:35:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:35:31 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 18:35:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.105173 seconds
INFO 05-12 18:35:32 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:35:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:35:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:35:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:35:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:35:33 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:35:55 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:35:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.65 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.54 toks/s, output: 101.75 toks/s]
[winogrande thr=0.20 | sample 280] energy=4939.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:35:57 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:35:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:35:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:35:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 18:36:09 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 18:36:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.042949 seconds
INFO 05-12 18:36:10 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:36:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:36:10 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:36:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:36:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:36:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:36:33 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:36:33 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.62 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.49 toks/s, output: 101.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.49 toks/s, output: 101.58 toks/s]
[winogrande thr=0.20 | sample 281] energy=4931.87 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:36:35 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:36:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:36:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:36:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:36:47 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 18:36:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.009595 seconds
INFO 05-12 18:36:48 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:36:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:36:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:36:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:36:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:36:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:37:11 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:37:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.68 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.63 toks/s, output: 101.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 40.63 toks/s, output: 101.57 toks/s]
[winogrande thr=0.20 | sample 282] energy=5034.81 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:37:13 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:37:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:37:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:37:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 18:37:25 [loader.py:458] Loading weights took 9.87 seconds
INFO 05-12 18:37:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.229005 seconds
INFO 05-12 18:37:26 [worker.py:267] Memory profiling takes 0.43 seconds
INFO 05-12 18:37:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:37:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:37:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:37:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:37:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:37:49 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:37:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.68 toks/s, output: 101.84 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.68 toks/s, output: 101.84 toks/s]
[winogrande thr=0.20 | sample 283] energy=4979.87 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:37:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:37:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:37:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:37:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:38:03 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 18:38:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.061283 seconds
INFO 05-12 18:38:04 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:38:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:38:04 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:38:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:38:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:38:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 18:38:27 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:38:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.85 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.69 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.69 toks/s, output: 101.71 toks/s]
[winogrande thr=0.20 | sample 284] energy=4971.79 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:38:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:38:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:38:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:38:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:38:40 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 18:38:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.087701 seconds
INFO 05-12 18:38:41 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 18:38:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:38:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:38:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:38:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:38:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:39:04 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:39:04 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.81 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.50 toks/s, output: 101.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.50 toks/s, output: 101.46 toks/s]
[winogrande thr=0.20 | sample 285] energy=4951.64 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:39:06 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:39:06 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:39:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:39:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:39:18 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 18:39:18 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.263374 seconds
INFO 05-12 18:39:19 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:39:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:39:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:39:19 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:39:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:39:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 18:39:42 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:39:42 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.87 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.56 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.56 toks/s, output: 101.71 toks/s]
[winogrande thr=0.20 | sample 286] energy=4975.01 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:39:44 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:39:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:39:46 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:39:46 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:39:56 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 18:39:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.080753 seconds
INFO 05-12 18:39:57 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:39:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:39:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:39:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:39:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:39:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 18:40:20 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:40:20 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.95 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.46 toks/s, output: 101.38 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.46 toks/s, output: 101.38 toks/s]
[winogrande thr=0.20 | sample 287] energy=4967.32 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:40:22 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:40:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:40:24 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:40:24 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 18:40:34 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 18:40:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.113144 seconds
INFO 05-12 18:40:35 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:40:35 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:40:35 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:40:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:40:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:40:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 18:40:58 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:40:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.98 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.49 toks/s, output: 101.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.49 toks/s, output: 101.49 toks/s]
[winogrande thr=0.20 | sample 288] energy=4985.77 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:41:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:41:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:41:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:41:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 18:41:12 [loader.py:458] Loading weights took 9.81 seconds
INFO 05-12 18:41:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.375185 seconds
INFO 05-12 18:41:13 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 18:41:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:41:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:41:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:41:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:41:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 18:41:36 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:41:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.10 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.57 toks/s, output: 101.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.57 toks/s, output: 101.63 toks/s]
[winogrande thr=0.20 | sample 289] energy=4998.87 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:41:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:41:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:41:40 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:41:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:41:50 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 18:41:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.103165 seconds
INFO 05-12 18:41:51 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 18:41:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:41:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:41:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:41:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:41:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 18:42:14 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:42:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.10 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.54 toks/s, output: 101.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.54 toks/s, output: 101.59 toks/s]
[winogrande thr=0.20 | sample 290] energy=4998.92 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:42:16 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:42:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:42:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:42:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 18:42:28 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 18:42:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.195829 seconds
INFO 05-12 18:42:29 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:42:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:42:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:42:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:42:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:42:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 18:42:52 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:42:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.89 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.45 toks/s, output: 101.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.45 toks/s, output: 101.35 toks/s]
[winogrande thr=0.20 | sample 291] energy=4967.98 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:42:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:42:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:42:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:42:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.14s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.32s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.19s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.67s/it]

INFO 05-12 18:43:08 [loader.py:458] Loading weights took 10.82 seconds
INFO 05-12 18:43:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.217036 seconds
INFO 05-12 18:43:09 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 18:43:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:43:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:43:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:43:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:43:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.43it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:43:32 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:43:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.70 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.53 toks/s, output: 101.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.53 toks/s, output: 101.60 toks/s]
[winogrande thr=0.20 | sample 292] energy=5176.18 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:43:34 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:43:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:43:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:43:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.10s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.46s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.81s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.83s/it]

INFO 05-12 18:43:47 [loader.py:458] Loading weights took 11.53 seconds
INFO 05-12 18:43:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.863267 seconds
INFO 05-12 18:43:48 [worker.py:267] Memory profiling takes 0.43 seconds
INFO 05-12 18:43:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:43:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:43:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:43:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:43:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:44:11 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:44:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.89 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.45 toks/s, output: 101.51 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.45 toks/s, output: 101.51 toks/s]
[winogrande thr=0.20 | sample 293] energy=5148.88 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:44:14 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:44:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:44:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:44:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.57s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.91s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.64s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.19s/it]

INFO 05-12 18:44:29 [loader.py:458] Loading weights took 12.91 seconds
INFO 05-12 18:44:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.243529 seconds
INFO 05-12 18:44:30 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:44:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:44:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:44:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:44:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:44:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 18:44:53 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:44:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.84 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.50 toks/s, output: 101.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.50 toks/s, output: 101.65 toks/s]
[winogrande thr=0.20 | sample 294] energy=5384.00 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:44:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:44:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:44:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:44:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.38s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.41s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.24s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]

INFO 05-12 18:45:07 [loader.py:458] Loading weights took 10.67 seconds
INFO 05-12 18:45:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.005569 seconds
INFO 05-12 18:45:08 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:45:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:45:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:45:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:45:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:45:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 18:45:31 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:45:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.86 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.44 toks/s, output: 101.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.44 toks/s, output: 101.46 toks/s]
[winogrande thr=0.20 | sample 295] energy=5064.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:45:34 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:45:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 18:45:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 18:45:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 18:45:45 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 18:45:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.173237 seconds
INFO 05-12 18:45:46 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 18:45:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 18:45:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 18:45:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 18:45:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 18:45:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 18:46:10 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 18:46:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.48 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.51 toks/s, output: 101.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it, est. speed input: 35.51 toks/s, output: 101.46 toks/s]
[winogrande thr=0.20 | sample 296] energy=5215.41 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 18:46:14 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 18:46:15 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:00:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:00:51 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.45s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.46s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.47s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.64s/it]

INFO 05-12 19:01:02 [loader.py:458] Loading weights took 10.77 seconds
INFO 05-12 19:01:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.105359 seconds
INFO 05-12 19:01:03 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:01:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:01:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:01:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:01:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:01:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.45it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 19:01:26 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:01:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.11 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.61 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.61 toks/s, output: 101.61 toks/s]
[winogrande thr=0.20 | sample 297] energy=107263.50 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:01:28 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:01:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:01:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:01:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:01:39 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 19:01:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.090965 seconds
INFO 05-12 19:01:41 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:01:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:01:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:01:41 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:01:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:01:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:02:03 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:02:03 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.61 toks/s, output: 101.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.61 toks/s, output: 101.59 toks/s]
[winogrande thr=0.20 | sample 298] energy=4922.95 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:02:06 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:02:06 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:02:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:02:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:02:18 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 19:02:18 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.083973 seconds
INFO 05-12 19:02:19 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:02:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:02:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:02:19 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:02:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:02:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:02:42 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:02:42 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.58 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.62 toks/s, output: 101.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.62 toks/s, output: 101.63 toks/s]
[winogrande thr=0.20 | sample 299] energy=5035.66 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:02:45 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:02:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:02:46 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:02:46 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:02:56 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 19:02:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.069949 seconds
INFO 05-12 19:02:57 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:02:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:02:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:02:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:02:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:02:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:03:21 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:03:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.86 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]
[winogrande thr=0.20 | sample 300] energy=5063.66 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:03:23 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:03:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:03:25 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:03:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:03:35 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 19:03:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.104701 seconds
INFO 05-12 19:03:36 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:03:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:03:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:03:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:03:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:03:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:03:59 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:03:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.60 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.62 toks/s]
[winogrande thr=0.20 | sample 301] energy=5010.87 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:04:01 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:04:01 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:04:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:04:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:04:12 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 19:04:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.072026 seconds
INFO 05-12 19:04:13 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:04:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:04:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:04:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:04:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:04:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:04:36 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:04:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.54 toks/s, output: 101.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.54 toks/s, output: 101.65 toks/s]
[winogrande thr=0.20 | sample 302] energy=4894.13 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:04:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:04:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:04:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:04:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:04:50 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 19:04:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.070735 seconds
INFO 05-12 19:04:51 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:04:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:04:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:04:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:04:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:04:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:05:13 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:05:13 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.56 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 303] energy=4922.09 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:05:15 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:05:15 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:05:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:05:17 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.02s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.07s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 19:05:27 [loader.py:458] Loading weights took 9.99 seconds
INFO 05-12 19:05:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.337745 seconds
INFO 05-12 19:05:28 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:05:28 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:05:28 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:05:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:05:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:05:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:05:51 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:05:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.55 toks/s, output: 101.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.55 toks/s, output: 101.77 toks/s]
[winogrande thr=0.20 | sample 304] energy=4942.40 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:05:53 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:05:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:05:55 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:05:55 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:06:05 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 19:06:05 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.424523 seconds
INFO 05-12 19:06:06 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:06:06 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:06:06 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:06:06 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:06:06 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:06:06 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.49it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:06:29 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:06:29 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.51 toks/s, output: 101.77 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.51 toks/s, output: 101.77 toks/s]
[winogrande thr=0.20 | sample 305] energy=5002.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:06:31 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:06:31 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:06:33 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:06:33 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 19:06:43 [loader.py:458] Loading weights took 9.83 seconds
INFO 05-12 19:06:43 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.159076 seconds
INFO 05-12 19:06:44 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:06:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:06:44 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:06:44 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:06:44 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:06:44 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:07:07 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:07:07 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.89 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 45.31 toks/s, output: 101.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 45.31 toks/s, output: 101.34 toks/s]
[winogrande thr=0.20 | sample 306] energy=4940.19 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:07:09 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:07:09 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:07:11 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:07:11 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.97s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.15s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]

INFO 05-12 19:07:21 [loader.py:458] Loading weights took 10.26 seconds
INFO 05-12 19:07:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.772639 seconds
INFO 05-12 19:07:23 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 19:07:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:07:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:07:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:07:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:07:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:07:46 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:07:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.97 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.47 toks/s, output: 101.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.47 toks/s, output: 101.46 toks/s]
[winogrande thr=0.20 | sample 307] energy=5063.35 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:07:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:07:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:07:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:07:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:07:59 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 19:08:00 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.090214 seconds
INFO 05-12 19:08:00 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:08:00 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:08:00 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:08:01 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:08:01 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:08:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.49it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:08:24 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:08:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.98 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 308] energy=4977.04 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:08:26 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:08:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:08:27 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:08:27 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:08:37 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 19:08:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.102977 seconds
INFO 05-12 19:08:38 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 19:08:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:08:38 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:08:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:08:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:08:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.49it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:09:02 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:09:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.97 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 59.86 toks/s, output: 100.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 59.86 toks/s, output: 100.81 toks/s]
[winogrande thr=0.20 | sample 309] energy=4836.17 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:09:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:09:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:09:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:09:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:09:15 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 19:09:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.093496 seconds
INFO 05-12 19:09:16 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:09:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:09:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:09:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:09:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:09:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.49it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 19:09:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:09:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.04 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.60 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.60 toks/s, output: 101.71 toks/s]
[winogrande thr=0.20 | sample 310] energy=4969.73 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:09:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:09:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:09:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:09:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:09:53 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 19:09:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.167785 seconds
INFO 05-12 19:09:54 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:09:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:09:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:09:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:09:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:09:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:10:17 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:10:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.99 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.53 toks/s, output: 101.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.53 toks/s, output: 101.66 toks/s]
[winogrande thr=0.20 | sample 311] energy=4963.56 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:10:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:10:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:10:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:10:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:10:31 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 19:10:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.120201 seconds
INFO 05-12 19:10:32 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 19:10:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:10:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:10:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:10:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:10:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.48it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.49it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.49it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 19:10:56 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:10:56 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.19 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.50 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.50 toks/s, output: 101.61 toks/s]
[winogrande thr=0.20 | sample 312] energy=5031.67 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:10:58 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:10:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:10:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:10:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:11:09 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 19:11:10 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.461420 seconds
INFO 05-12 19:11:11 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:11:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:11:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:11:11 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:11:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:11:11 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.49it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.49it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.49it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.48it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 19:11:34 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:11:34 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.20 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.43 toks/s, output: 101.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.43 toks/s, output: 101.20 toks/s]
[winogrande thr=0.20 | sample 313] energy=5045.69 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:11:36 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:11:36 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:11:38 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:11:38 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.47s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.48s/it]

INFO 05-12 19:11:48 [loader.py:458] Loading weights took 10.14 seconds
INFO 05-12 19:11:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.488434 seconds
INFO 05-12 19:11:49 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:11:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:11:49 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:11:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:11:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:11:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:12:12 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:12:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.72 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.61 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.61 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 314] energy=5003.08 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:12:14 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:12:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:12:16 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:12:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.48s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.82s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.56s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.13s/it]

INFO 05-12 19:12:29 [loader.py:458] Loading weights took 12.65 seconds
INFO 05-12 19:12:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.987770 seconds
INFO 05-12 19:12:30 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:12:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:12:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:12:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:12:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:12:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:12:53 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:12:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.69 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.54 toks/s, output: 101.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.54 toks/s, output: 101.55 toks/s]
[winogrande thr=0.20 | sample 315] energy=5270.71 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:12:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:12:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:12:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:12:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.15s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.19s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]

INFO 05-12 19:13:07 [loader.py:458] Loading weights took 10.24 seconds
INFO 05-12 19:13:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.600266 seconds
INFO 05-12 19:13:08 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:13:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:13:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:13:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:13:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:13:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:05<01:50,  3.33s/it]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:08<01:30,  2.83s/it]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [02:11<26:07, 50.57s/it]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [04:00<35:48, 71.60s/it]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [07:23<56:09, 116.19s/it]Capturing CUDA graph shapes:  20%|██        | 7/35 [09:32<56:06, 120.23s/it]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [10:20<43:50, 97.44s/it] Capturing CUDA graph shapes:  26%|██▌       | 9/35 [10:28<30:08, 69.54s/it]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [14:10<48:30, 116.42s/it]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [17:05<53:43, 134.33s/it]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [21:44<1:08:27, 178.57s/it]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [24:54<1:06:39, 181.79s/it]Capturing CUDA graph shapes:  40%|████      | 14/35 [26:48<56:31, 161.49s/it]  Capturing CUDA graph shapes:  43%|████▎     | 15/35 [27:39<42:41, 128.05s/it]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [27:45<28:58, 91.51s/it] Capturing CUDA graph shapes:  49%|████▊     | 17/35 [27:46<19:15, 64.20s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [27:47<12:46, 45.12s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [27:47<08:28, 31.77s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [27:48<05:36, 22.43s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [27:49<03:43, 15.93s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [27:50<02:27, 11.35s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [27:50<01:37,  8.14s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [27:51<01:04,  5.90s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [27:52<00:43,  4.33s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [27:52<00:29,  3.22s/it]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [27:53<00:19,  2.45s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [27:54<00:13,  1.91s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [27:54<00:09,  1.54s/it]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [27:55<00:06,  1.27s/it]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [27:56<00:04,  1.09s/it]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [27:56<00:02,  1.04it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [27:57<00:01,  1.14it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [27:58<00:00,  1.24it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [27:58<00:00,  1.31it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [27:58<00:00, 47.96s/it]
INFO 05-12 19:41:07 [model_runner.py:1598] Graph capturing finished in 1679 secs, took 0.06 GiB
INFO 05-12 19:41:07 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 1679.75 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.53 toks/s, output: 101.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.53 toks/s, output: 101.42 toks/s]
[winogrande thr=0.20 | sample 316] energy=198133.11 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:41:09 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:41:09 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:41:10 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:41:10 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:41:20 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 19:41:21 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.125807 seconds
INFO 05-12 19:41:21 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:41:21 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:41:21 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:41:22 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:41:22 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:41:22 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.49it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.49it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.48it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.49it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.49it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]
INFO 05-12 19:41:45 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:41:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.36 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.65 toks/s, output: 101.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 41.65 toks/s, output: 101.58 toks/s]
[winogrande thr=0.20 | sample 317] energy=5030.32 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:41:47 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:41:47 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:41:48 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:41:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:41:59 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 19:41:59 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.135468 seconds
INFO 05-12 19:42:00 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:42:00 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:42:00 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:42:00 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:42:00 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:42:00 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.49it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.49it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 19:42:23 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:42:23 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.15 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.67 toks/s, output: 101.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.67 toks/s, output: 101.62 toks/s]
[winogrande thr=0.20 | sample 318] energy=4973.90 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:42:25 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:42:25 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:42:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:42:27 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:42:37 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 19:42:37 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.150302 seconds
INFO 05-12 19:42:38 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:42:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:42:38 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:42:38 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:42:38 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:42:38 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 19:43:01 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:43:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.11 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.28 toks/s, output: 100.85 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.28 toks/s, output: 100.85 toks/s]
[winogrande thr=0.20 | sample 319] energy=4971.38 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:43:03 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:43:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:43:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:43:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:43:15 [loader.py:458] Loading weights took 9.79 seconds
INFO 05-12 19:43:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.131966 seconds
INFO 05-12 19:43:16 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:43:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:43:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:43:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:43:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:43:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.46it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 19:43:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:43:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.04 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.35it/s, est. speed input: 143.65 toks/s, output: 95.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s, est. speed input: 143.65 toks/s, output: 95.76 toks/s]
[winogrande thr=0.20 | sample 320] energy=4707.69 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:43:40 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:43:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:43:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:43:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:43:52 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 19:43:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.100948 seconds
INFO 05-12 19:43:53 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:43:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:43:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:43:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:43:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:43:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:09<00:32,  1.35s/it]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:19<01:34,  4.10s/it]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [02:47<17:23, 47.43s/it]Capturing CUDA graph shapes:  40%|████      | 14/35 [03:34<16:38, 47.56s/it]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [03:43<11:53, 35.69s/it]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [03:50<08:36, 27.18s/it]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [03:51<05:46, 19.22s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [03:51<03:51, 13.64s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [03:52<02:35,  9.75s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [03:53<01:45,  7.02s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [03:53<01:11,  5.11s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [03:54<00:49,  3.78s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [03:55<00:34,  2.84s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [03:55<00:24,  2.18s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [03:56<00:17,  1.72s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [03:57<00:12,  1.40s/it]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [03:57<00:09,  1.18s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [03:58<00:07,  1.03s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [03:59<00:05,  1.09it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [03:59<00:04,  1.20it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [04:00<00:03,  1.28it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [04:01<00:02,  1.34it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [04:01<00:01,  1.40it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [04:02<00:00,  1.43it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [04:03<00:00,  1.46it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [04:03<00:00,  6.94s/it]
INFO 05-12 19:47:56 [model_runner.py:1598] Graph capturing finished in 243 secs, took 0.06 GiB
INFO 05-12 19:47:56 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 244.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 40.56 toks/s, output: 101.40 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 40.56 toks/s, output: 101.40 toks/s]
[winogrande thr=0.20 | sample 321] energy=30637.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:47:59 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:47:59 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:48:00 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:48:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 19:48:10 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 19:48:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.254636 seconds
INFO 05-12 19:48:11 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 19:48:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:48:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:48:12 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:48:12 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:48:12 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.48it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.49it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 19:48:35 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:48:35 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.15 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 39.53 toks/s, output: 101.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 39.53 toks/s, output: 101.36 toks/s]
[winogrande thr=0.20 | sample 322] energy=5030.85 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:48:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:48:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:48:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:48:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.20s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.32s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.20s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.57s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.65s/it]

INFO 05-12 19:48:50 [loader.py:458] Loading weights took 10.75 seconds
INFO 05-12 19:48:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.153239 seconds
INFO 05-12 19:48:51 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:48:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:48:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:48:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:48:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:48:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:49:15 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:49:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.97 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.44 toks/s, output: 101.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.44 toks/s, output: 101.43 toks/s]
[winogrande thr=0.20 | sample 323] energy=5190.95 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:49:17 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:49:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:49:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:49:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:49:28 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 19:49:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.097650 seconds
INFO 05-12 19:49:29 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:49:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:49:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:49:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:49:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:49:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.46it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:49:52 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:49:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.95 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.51 toks/s, output: 101.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.51 toks/s, output: 101.43 toks/s]
[winogrande thr=0.20 | sample 324] energy=4951.48 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:49:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:49:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:49:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:49:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:50:06 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 19:50:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.087321 seconds
INFO 05-12 19:50:07 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:50:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:50:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:50:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:50:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:50:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:50:30 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:50:30 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.98 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.54 toks/s, output: 101.51 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.54 toks/s, output: 101.51 toks/s]
[winogrande thr=0.20 | sample 325] energy=4991.01 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:50:32 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:50:32 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:50:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:50:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:50:44 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 19:50:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.488635 seconds
INFO 05-12 19:50:45 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:50:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:50:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:50:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:50:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:50:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 19:51:09 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:51:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.04 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.53 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.53 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 326] energy=5013.30 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:51:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:51:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:51:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:51:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 19:51:22 [loader.py:458] Loading weights took 9.98 seconds
INFO 05-12 19:51:23 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.348252 seconds
INFO 05-12 19:51:23 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:51:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:51:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:51:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:51:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:51:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:51:47 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:51:47 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.98 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.45 toks/s, output: 101.44 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.45 toks/s, output: 101.44 toks/s]
[winogrande thr=0.20 | sample 327] energy=4998.59 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:51:49 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:51:49 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:51:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:51:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.40s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.76s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.48s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.06s/it]

INFO 05-12 19:52:03 [loader.py:458] Loading weights took 12.38 seconds
INFO 05-12 19:52:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.717948 seconds
INFO 05-12 19:52:04 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:52:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:52:04 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:52:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:52:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:52:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 19:52:27 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:52:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.99 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.49 toks/s, output: 101.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.49 toks/s, output: 101.43 toks/s]
[winogrande thr=0.20 | sample 328] energy=5282.27 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:52:31 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:52:31 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:52:32 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:52:32 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.90s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.05s/it]

INFO 05-12 19:52:45 [loader.py:458] Loading weights took 12.36 seconds
INFO 05-12 19:52:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.809164 seconds
INFO 05-12 19:52:46 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 19:52:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:52:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:52:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:52:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:52:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.49it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.49it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.48it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 19:53:09 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:53:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.28 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.52 toks/s, output: 101.52 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.52 toks/s, output: 101.52 toks/s]
[winogrande thr=0.20 | sample 329] energy=5483.85 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:53:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:53:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:53:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:53:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.36s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.45s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]

INFO 05-12 19:53:24 [loader.py:458] Loading weights took 10.59 seconds
INFO 05-12 19:53:24 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.962764 seconds
INFO 05-12 19:53:25 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:53:25 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:53:25 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:53:25 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:53:25 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:53:25 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.44it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 19:53:48 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:53:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.25 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.55 toks/s, output: 101.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.55 toks/s, output: 101.62 toks/s]
[winogrande thr=0.20 | sample 330] energy=5097.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:53:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:53:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:53:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:53:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:54:02 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 19:54:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.117832 seconds
INFO 05-12 19:54:03 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:54:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:54:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:54:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:54:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:54:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.49it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 19:54:27 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:54:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.59 toks/s, output: 101.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.59 toks/s, output: 101.59 toks/s]
[winogrande thr=0.20 | sample 331] energy=5033.62 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:54:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:54:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:54:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:54:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 19:54:40 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 19:54:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.114240 seconds
INFO 05-12 19:54:41 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 19:54:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:54:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:54:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:54:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:54:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.49it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 19:55:05 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:55:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.19 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.60 toks/s, output: 101.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.60 toks/s, output: 101.63 toks/s]
[winogrande thr=0.20 | sample 332] energy=4999.74 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:55:07 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:55:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:55:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:55:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:55:19 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 19:55:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.088221 seconds
INFO 05-12 19:55:20 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 19:55:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:55:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:55:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:55:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:55:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.48it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.49it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.49it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 19:55:43 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:55:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.23 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.52 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.52 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 333] energy=4983.81 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:55:45 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:55:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:55:46 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:55:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:55:57 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 19:55:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.078342 seconds
INFO 05-12 19:55:58 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 19:55:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:55:58 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:55:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:55:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:55:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.49it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.48it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.49it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.48it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.49it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 19:56:21 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:56:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.22 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.53 toks/s, output: 101.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.53 toks/s, output: 101.55 toks/s]
[winogrande thr=0.20 | sample 334] energy=5011.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:56:23 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:56:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:56:25 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:56:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:56:35 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 19:56:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.090552 seconds
INFO 05-12 19:56:36 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:56:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:56:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:56:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:56:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:56:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 19:56:59 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:56:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.61 toks/s]
[winogrande thr=0.20 | sample 335] energy=4962.18 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:57:01 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:57:01 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:57:03 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:57:03 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:57:13 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 19:57:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.055716 seconds
INFO 05-12 19:57:14 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:57:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:57:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:57:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:57:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:57:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 19:57:37 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:57:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.58 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.58 toks/s, output: 101.61 toks/s]
[winogrande thr=0.20 | sample 336] energy=4993.05 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:57:39 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 19:57:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:57:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:57:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 19:57:51 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 19:57:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.055689 seconds
INFO 05-12 19:57:52 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:57:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:57:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:57:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:57:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:57:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.45it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:48<01:18,  9.77s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [01:07<01:27, 12.57s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [04:17<06:35, 66.00s/it]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [17:05<23:02, 276.54s/it]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [19:18<15:33, 233.44s/it]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [19:21<08:12, 164.19s/it]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [19:22<03:50, 115.44s/it]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [19:23<01:21, 81.02s/it] Capturing CUDA graph shapes: 100%|██████████| 35/35 [19:24<00:00, 56.91s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [19:24<00:00, 33.26s/it]
INFO 05-12 20:17:16 [model_runner.py:1598] Graph capturing finished in 1164 secs, took 0.06 GiB
INFO 05-12 20:17:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 1165.16 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 27.74 toks/s, output: 95.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 27.74 toks/s, output: 95.64 toks/s]
[winogrande thr=0.20 | sample 337] energy=138182.31 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:17:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:17:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:17:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:17:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.95s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.16s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.53s/it]

INFO 05-12 20:17:31 [loader.py:458] Loading weights took 10.33 seconds
INFO 05-12 20:17:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.668055 seconds
INFO 05-12 20:17:32 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:17:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:17:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:17:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:17:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:17:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 20:17:55 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:17:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.75 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.63 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.63 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 338] energy=4967.22 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:17:57 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:17:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:17:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:17:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 20:18:08 [loader.py:458] Loading weights took 9.86 seconds
INFO 05-12 20:18:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.195949 seconds
INFO 05-12 20:18:09 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:18:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:18:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:18:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:18:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:18:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.49it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 20:18:33 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:18:33 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.04 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.61 toks/s, output: 101.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.61 toks/s, output: 101.65 toks/s]
[winogrande thr=0.20 | sample 339] energy=4990.00 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:18:35 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:18:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:18:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:18:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:18:46 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 20:18:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.084520 seconds
INFO 05-12 20:18:47 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:18:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:18:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:18:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:18:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:18:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.44it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.46it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.48it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.48it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.48it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 20:19:11 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:19:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.18 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.50 toks/s, output: 101.40 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.50 toks/s, output: 101.40 toks/s]
[winogrande thr=0.20 | sample 340] energy=4980.89 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:19:13 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:19:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:19:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:19:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:19:24 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 20:19:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.240956 seconds
INFO 05-12 20:19:26 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:19:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:19:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:19:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:19:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:19:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.49it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.48it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.49it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 20:19:49 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:19:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.23 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.48 toks/s, output: 101.38 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.48 toks/s, output: 101.38 toks/s]
[winogrande thr=0.20 | sample 341] energy=5025.86 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:19:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:19:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:19:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:19:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 20:20:03 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 20:20:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.095475 seconds
INFO 05-12 20:20:04 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:20:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:20:04 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:20:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:20:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:20:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:20:27 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:20:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.92 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.49 toks/s, output: 101.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.49 toks/s, output: 101.59 toks/s]
[winogrande thr=0.20 | sample 342] energy=4983.54 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:20:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:20:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:20:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:20:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 20:20:41 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 20:20:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.106644 seconds
INFO 05-12 20:20:42 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:20:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:20:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:20:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:20:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:20:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 20:21:05 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:21:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.63 toks/s, output: 101.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.63 toks/s, output: 101.75 toks/s]
[winogrande thr=0.20 | sample 343] energy=4951.96 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:21:07 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:21:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:21:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:21:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.15s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]

INFO 05-12 20:21:19 [loader.py:458] Loading weights took 10.65 seconds
INFO 05-12 20:21:20 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.970706 seconds
INFO 05-12 20:21:20 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:21:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:21:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:21:21 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:21:21 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:21:21 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:21:44 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:21:44 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.98 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.46 toks/s, output: 101.28 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 36.46 toks/s, output: 101.28 toks/s]
[winogrande thr=0.20 | sample 344] energy=5075.43 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:21:46 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:21:46 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:21:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:21:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.97s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.63s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.15s/it]

INFO 05-12 20:22:04 [loader.py:458] Loading weights took 16.05 seconds
INFO 05-12 20:22:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 16.399658 seconds
INFO 05-12 20:22:05 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:22:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:22:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:22:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:22:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:22:11 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:23,  1.43it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.49it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.49it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 20:22:35 [model_runner.py:1598] Graph capturing finished in 24 secs, took 0.06 GiB
INFO 05-12 20:22:35 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 30.30 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.51 toks/s, output: 101.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.51 toks/s, output: 101.54 toks/s]
[winogrande thr=0.20 | sample 345] energy=6565.90 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:22:37 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:22:37 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:22:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:22:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 20:22:49 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 20:22:49 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.113934 seconds
INFO 05-12 20:22:50 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:22:50 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:22:50 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:22:50 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:22:50 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:22:50 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.49it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.48it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 20:23:13 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:23:13 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.27 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.48 toks/s, output: 101.44 toks/s]Processed prompts: 100%|██████████| 1/1 [00:32<00:00, 32.99s/it, est. speed input: 33.48 toks/s, output: 101.44 toks/s]
[winogrande thr=0.20 | sample 346] energy=13445.38 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:26:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:26:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:26:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:26:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.06s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.11s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.47s/it]

INFO 05-12 20:27:08 [loader.py:458] Loading weights took 10.09 seconds
INFO 05-12 20:27:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.431098 seconds
INFO 05-12 20:27:09 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:27:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:27:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:27:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:27:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:27:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 20:27:32 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:27:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.80 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.58 toks/s, output: 101.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.58 toks/s, output: 101.65 toks/s]
[winogrande thr=0.20 | sample 347] energy=22246.01 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:27:34 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:27:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:27:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:27:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 20:27:45 [loader.py:458] Loading weights took 9.78 seconds
INFO 05-12 20:27:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.153427 seconds
INFO 05-12 20:27:46 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:27:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:27:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:27:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:27:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:27:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.49it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.48it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.49it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 20:28:10 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:28:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.28 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.52 toks/s, output: 101.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.52 toks/s, output: 101.54 toks/s]
[winogrande thr=0.20 | sample 348] energy=4990.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:28:13 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:28:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:28:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:28:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:28:25 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 20:28:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.230299 seconds
INFO 05-12 20:28:26 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:28:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:28:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:28:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:28:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:28:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.48it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 20:28:49 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:28:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.31 toks/s, output: 100.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.31 toks/s, output: 100.96 toks/s]
[winogrande thr=0.20 | sample 349] energy=5104.86 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:28:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:28:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:28:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:28:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 20:29:03 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 20:29:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.086876 seconds
INFO 05-12 20:29:04 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:29:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:29:04 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:29:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:29:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:29:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:29:27 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:29:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.90 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.51 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.51 toks/s, output: 101.61 toks/s]
[winogrande thr=0.20 | sample 350] energy=4974.90 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:29:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:29:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:29:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:29:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:29:40 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 20:29:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.068754 seconds
INFO 05-12 20:29:41 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:29:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:29:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:29:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:29:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:29:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:30:05 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:30:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.91 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.63 toks/s]
[winogrande thr=0.20 | sample 351] energy=4966.08 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:30:07 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:30:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:30:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:30:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 20:30:18 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 20:30:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.131905 seconds
INFO 05-12 20:30:19 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:30:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:30:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:30:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:30:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:30:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:04,  1.49it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 20:30:43 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:30:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 352] energy=4976.46 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:30:45 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:30:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:30:46 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:30:46 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:30:56 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 20:30:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.076420 seconds
INFO 05-12 20:30:57 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:30:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:30:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:30:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:30:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:30:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:31:20 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:31:20 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.86 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.57it/s, est. speed input: 287.20 toks/s, output: 86.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s, est. speed input: 287.20 toks/s, output: 86.15 toks/s]
[winogrande thr=0.20 | sample 353] energy=4631.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:31:22 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:31:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:31:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:31:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 20:31:33 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 20:31:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.103829 seconds
INFO 05-12 20:31:34 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:31:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:31:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:31:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:31:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:31:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.49it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 20:31:58 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:31:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.25 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.52 toks/s, output: 101.48 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.52 toks/s, output: 101.48 toks/s]
[winogrande thr=0.20 | sample 354] energy=5047.13 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:32:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:32:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:32:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:32:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.12s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.26s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.53s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]

INFO 05-12 20:32:13 [loader.py:458] Loading weights took 10.56 seconds
INFO 05-12 20:32:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.899685 seconds
INFO 05-12 20:32:14 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:32:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:32:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:32:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:32:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:32:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.49it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:32:37 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:32:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.02 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.50 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.50 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 355] energy=5068.45 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:32:40 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:32:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:32:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:32:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.90s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.47s/it]

INFO 05-12 20:32:53 [loader.py:458] Loading weights took 10.08 seconds
INFO 05-12 20:32:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.419445 seconds
INFO 05-12 20:32:54 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:32:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:32:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:32:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:32:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:32:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:33:17 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:33:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.00 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.56 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.56 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 356] energy=5274.29 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:33:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:33:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:33:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:33:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:33:31 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 20:33:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.078348 seconds
INFO 05-12 20:33:32 [worker.py:267] Memory profiling takes 0.44 seconds
INFO 05-12 20:33:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:33:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:33:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:33:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:33:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.49it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 20:33:55 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:33:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.15 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.50 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.50 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 357] energy=4982.75 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:33:57 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:33:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:33:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:33:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:34:09 [loader.py:458] Loading weights took 9.88 seconds
INFO 05-12 20:34:10 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.298103 seconds
INFO 05-12 20:34:10 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:34:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:34:10 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:34:11 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:34:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:34:11 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:14,  1.16it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:13<00:15,  1.01it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [03:42<15:48, 63.25s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [06:59<24:07, 103.40s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [13:06<39:32, 182.46s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [14:16<29:44, 148.72s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [14:16<19:07, 104.36s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [16:37<19:12, 115.22s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [16:41<12:15, 81.72s/it] Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [16:41<07:39, 57.41s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [16:42<04:42, 40.40s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [16:43<02:50, 28.49s/it]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [16:43<01:40, 20.15s/it]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [16:44<00:57, 14.30s/it]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [16:45<00:30, 10.21s/it]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [16:45<00:14,  7.35s/it]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [16:46<00:05,  5.34s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [16:47<00:00,  3.93s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [16:47<00:00, 28.78s/it]
INFO 05-12 20:50:58 [model_runner.py:1598] Graph capturing finished in 1007 secs, took 0.06 GiB
INFO 05-12 20:50:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 1008.23 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.43 toks/s, output: 101.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.43 toks/s, output: 101.43 toks/s]
[winogrande thr=0.20 | sample 358] energy=119896.46 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:51:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:51:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:51:01 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:51:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.58s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.93s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.64s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.20s/it]

INFO 05-12 20:51:15 [loader.py:458] Loading weights took 12.95 seconds
INFO 05-12 20:51:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.287505 seconds
INFO 05-12 20:51:16 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:51:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:51:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:51:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:51:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:51:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:51:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:51:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.91 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.48 toks/s, output: 101.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.48 toks/s, output: 101.55 toks/s]
[winogrande thr=0.20 | sample 359] energy=5305.16 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:51:41 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:51:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:51:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:51:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.41s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.46s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.65s/it]

INFO 05-12 20:51:53 [loader.py:458] Loading weights took 10.80 seconds
INFO 05-12 20:51:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.135970 seconds
INFO 05-12 20:51:54 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:51:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:51:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:51:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:51:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:51:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:52:18 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:52:18 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.87 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.54 toks/s, output: 101.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.54 toks/s, output: 101.47 toks/s]
[winogrande thr=0.20 | sample 360] energy=5069.71 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:52:20 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:52:20 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:52:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:52:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:52:31 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 20:52:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.082445 seconds
INFO 05-12 20:52:32 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:52:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:52:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:52:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:52:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:52:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.42it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:52:55 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:52:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.94 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.52 toks/s, output: 101.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.52 toks/s, output: 101.59 toks/s]
[winogrande thr=0.20 | sample 361] energy=4958.92 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:52:57 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:52:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:52:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:52:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:53:09 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 20:53:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.073920 seconds
INFO 05-12 20:53:10 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:53:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:53:10 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:53:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:53:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:53:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.49it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:53:33 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:53:33 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.00 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 362] energy=4975.01 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:53:35 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:53:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:53:37 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:53:37 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:53:47 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 20:53:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.081005 seconds
INFO 05-12 20:53:48 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:53:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:53:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:53:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:53:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:53:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.49it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 20:54:11 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:54:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.03 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.55 toks/s, output: 101.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.55 toks/s, output: 101.49 toks/s]
[winogrande thr=0.20 | sample 363] energy=4972.04 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:54:13 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:54:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:54:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:54:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:54:25 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 20:54:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.066785 seconds
INFO 05-12 20:54:26 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:54:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:54:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:54:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:54:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:54:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.49it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:54:49 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:54:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.00 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.58 toks/s, output: 101.56 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.58 toks/s, output: 101.56 toks/s]
[winogrande thr=0.20 | sample 364] energy=4964.65 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:54:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:54:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:54:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:54:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 20:55:03 [loader.py:458] Loading weights took 9.82 seconds
INFO 05-12 20:55:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.187673 seconds
INFO 05-12 20:55:04 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:55:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:55:04 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:55:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:55:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:55:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:55:27 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:55:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.94 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.39 toks/s, output: 101.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.39 toks/s, output: 101.21 toks/s]
[winogrande thr=0.20 | sample 365] energy=4973.14 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:55:30 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:55:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:55:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:55:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.15s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.29s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.18s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.55s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.63s/it]

INFO 05-12 20:55:42 [loader.py:458] Loading weights took 10.65 seconds
INFO 05-12 20:55:43 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.051877 seconds
INFO 05-12 20:55:43 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:55:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:55:43 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:55:44 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:55:44 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:55:44 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:56:07 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:56:07 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.94 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.49 toks/s, output: 101.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.49 toks/s, output: 101.57 toks/s]
[winogrande thr=0.20 | sample 366] energy=5191.40 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:56:09 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:56:09 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:56:10 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:56:10 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:56:20 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 20:56:20 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.081023 seconds
INFO 05-12 20:56:21 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 20:56:21 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:56:21 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:56:21 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:56:21 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:56:22 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:56:44 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:56:44 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.95 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.53 toks/s, output: 101.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.53 toks/s, output: 101.57 toks/s]
[winogrande thr=0.20 | sample 367] energy=4930.14 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:56:46 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:56:46 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:56:48 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:56:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:56:58 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 20:56:59 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.387112 seconds
INFO 05-12 20:56:59 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:56:59 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:56:59 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:57:00 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:57:00 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:57:00 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:57:23 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:57:23 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.96 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.42 toks/s, output: 101.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.42 toks/s, output: 101.22 toks/s]
[winogrande thr=0.20 | sample 368] energy=5028.50 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:57:25 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:57:25 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:57:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:57:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 20:57:36 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 20:57:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.078288 seconds
INFO 05-12 20:57:37 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:57:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:57:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:57:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:57:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:57:38 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:58:00 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:58:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.93 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.50 toks/s, output: 101.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.50 toks/s, output: 101.57 toks/s]
[winogrande thr=0.20 | sample 369] energy=4962.36 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:58:02 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:58:02 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:58:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:58:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 20:58:14 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 20:58:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.135330 seconds
INFO 05-12 20:58:15 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:58:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:58:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:58:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:58:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:58:15 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:58:38 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:58:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.88 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.70 toks/s, output: 101.83 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.70 toks/s, output: 101.83 toks/s]
[winogrande thr=0.20 | sample 370] energy=4950.95 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:58:40 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 20:58:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:58:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:58:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.45s/it]

INFO 05-12 20:58:52 [loader.py:458] Loading weights took 10.00 seconds
INFO 05-12 20:58:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.328258 seconds
INFO 05-12 20:58:53 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:58:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:58:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:58:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:58:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:58:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:09<00:21,  1.04it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:16<01:00,  2.88s/it]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:50<04:00, 12.02s/it]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [01:21<05:38, 17.82s/it]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [01:23<03:57, 13.19s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [01:24<02:41,  9.48s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [02:37<07:37, 28.59s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [05:44<19:02, 76.19s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [07:06<18:09, 77.86s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [07:38<13:52, 64.05s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [07:53<09:50, 49.25s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [07:57<06:33, 35.80s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [07:59<04:15, 25.60s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [10:17<08:54, 59.34s/it]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [10:50<06:51, 51.40s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [10:51<04:13, 36.21s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [10:51<02:33, 25.54s/it]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [10:52<01:30, 18.08s/it]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [10:53<00:51, 12.85s/it]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [10:53<00:27,  9.19s/it]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [10:54<00:13,  6.63s/it]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [10:55<00:04,  4.84s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [10:55<00:00,  3.58s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [10:55<00:00, 18.73s/it]
INFO 05-12 21:09:49 [model_runner.py:1598] Graph capturing finished in 656 secs, took 0.06 GiB
INFO 05-12 21:09:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 656.71 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.49 toks/s, output: 101.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.49 toks/s, output: 101.53 toks/s]
[winogrande thr=0.20 | sample 371] energy=78900.41 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:09:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:09:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:09:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:09:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 21:10:03 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 21:10:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.115516 seconds
INFO 05-12 21:10:04 [worker.py:267] Memory profiling takes 0.44 seconds
INFO 05-12 21:10:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:10:04 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:10:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:10:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:10:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.49it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.49it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:10:27 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:10:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.31 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.46 toks/s, output: 101.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.46 toks/s, output: 101.47 toks/s]
[winogrande thr=0.20 | sample 372] energy=5005.76 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:10:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:10:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:10:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:10:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 21:10:41 [loader.py:458] Loading weights took 9.86 seconds
INFO 05-12 21:10:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.205064 seconds
INFO 05-12 21:10:42 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:10:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:10:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:10:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:10:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:10:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.48it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.49it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:11:06 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:11:06 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.30 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.46 toks/s, output: 101.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.46 toks/s, output: 101.49 toks/s]
[winogrande thr=0.20 | sample 373] energy=5007.31 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:11:08 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:11:08 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:11:10 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:11:10 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.15s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.60s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.60s/it]

INFO 05-12 21:11:21 [loader.py:458] Loading weights took 10.63 seconds
INFO 05-12 21:11:21 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.962835 seconds
INFO 05-12 21:11:22 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 21:11:22 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:11:22 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:11:22 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:11:22 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:11:22 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.48it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.48it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.49it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.49it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.48it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.49it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.49it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:16<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]
INFO 05-12 21:11:45 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:11:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.38 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.44 toks/s, output: 101.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.44 toks/s, output: 101.47 toks/s]
[winogrande thr=0.20 | sample 374] energy=5212.97 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:11:47 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:11:47 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:11:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:11:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.36s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.70s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.02s/it]

INFO 05-12 21:12:01 [loader.py:458] Loading weights took 12.21 seconds
INFO 05-12 21:12:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.541495 seconds
INFO 05-12 21:12:03 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:12:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:12:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:12:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:12:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:12:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.48it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.48it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.49it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:12:26 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:12:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.27 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 29.41 toks/s, output: 101.40 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 29.41 toks/s, output: 101.40 toks/s]
[winogrande thr=0.20 | sample 375] energy=5294.51 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:12:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:12:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:12:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:12:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.93s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.86s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.02s/it]

INFO 05-12 21:12:43 [loader.py:458] Loading weights took 12.27 seconds
INFO 05-12 21:12:43 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.627957 seconds
INFO 05-12 21:12:44 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:12:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:12:44 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:12:44 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:12:44 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:12:44 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:24,  1.41it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.47it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.48it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.49it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.48it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:13:08 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:13:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.48 toks/s, output: 101.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.48 toks/s, output: 101.50 toks/s]
[winogrande thr=0.20 | sample 376] energy=5378.42 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:13:10 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:13:10 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:13:11 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:13:11 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.94s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 21:13:21 [loader.py:458] Loading weights took 9.96 seconds
INFO 05-12 21:13:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.364544 seconds
INFO 05-12 21:13:22 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:13:22 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:13:22 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:13:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:13:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:13:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.49it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.49it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.48it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:13:46 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:13:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.25 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.49 toks/s, output: 101.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.49 toks/s, output: 101.54 toks/s]
[winogrande thr=0.20 | sample 377] energy=5045.85 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:13:48 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:13:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:13:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:13:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:14:00 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 21:14:00 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.077941 seconds
INFO 05-12 21:14:01 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:14:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:14:01 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:14:01 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:14:01 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:14:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:14:24 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:14:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.59 toks/s, output: 101.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.59 toks/s, output: 101.63 toks/s]
[winogrande thr=0.20 | sample 378] energy=4992.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:14:26 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:14:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:14:28 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:14:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 21:14:38 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 21:14:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.102360 seconds
INFO 05-12 21:14:39 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:14:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:14:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:14:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:14:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:14:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 21:15:02 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:15:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.05 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.53 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.53 toks/s, output: 101.61 toks/s]
[winogrande thr=0.20 | sample 379] energy=4976.51 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:15:05 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:15:05 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:15:06 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:15:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.92s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.12s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.50s/it]

INFO 05-12 21:15:17 [loader.py:458] Loading weights took 10.21 seconds
INFO 05-12 21:15:17 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.543582 seconds
INFO 05-12 21:15:18 [worker.py:267] Memory profiling takes 0.43 seconds
INFO 05-12 21:15:18 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:15:18 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:15:18 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:15:18 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:15:18 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.49it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.49it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:15:41 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:15:41 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 39.57 toks/s, output: 101.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 39.57 toks/s, output: 101.47 toks/s]
[winogrande thr=0.20 | sample 380] energy=5151.39 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:15:43 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:15:43 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:15:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:15:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.97s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.16s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.45s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]

INFO 05-12 21:15:56 [loader.py:458] Loading weights took 10.26 seconds
INFO 05-12 21:15:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.945999 seconds
INFO 05-12 21:15:57 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:15:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:15:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:15:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:15:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:15:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 21:16:20 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:16:20 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.75 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.50 toks/s, output: 101.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.50 toks/s, output: 101.47 toks/s]
[winogrande thr=0.20 | sample 381] energy=5049.91 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:16:22 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:16:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:16:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:16:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.72s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 21:16:33 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 21:16:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.032047 seconds
INFO 05-12 21:16:34 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:16:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:16:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:16:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:16:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:16:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.46it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.49it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.49it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:16:58 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:16:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.09 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.54 toks/s, output: 101.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.54 toks/s, output: 101.60 toks/s]
[winogrande thr=0.20 | sample 382] energy=4962.75 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:17:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:17:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:17:01 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:17:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 21:17:11 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 21:17:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.044104 seconds
INFO 05-12 21:17:12 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:17:12 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:17:12 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:17:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:17:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:17:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.49it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.49it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.48it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:17:36 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:17:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.17 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.67 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.67 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 383] energy=4979.89 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:17:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:17:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:17:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:17:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:17:50 [loader.py:458] Loading weights took 9.73 seconds
INFO 05-12 21:17:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.261593 seconds
INFO 05-12 21:17:51 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:17:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:17:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:17:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:17:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:17:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.48it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.48it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.49it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:11,  1.48it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.49it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:18:14 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:18:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.27 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.76 toks/s, output: 101.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.76 toks/s, output: 101.68 toks/s]
[winogrande thr=0.20 | sample 384] energy=5027.36 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:18:16 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:18:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:18:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:18:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:18:28 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 21:18:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.074003 seconds
INFO 05-12 21:18:29 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:18:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:18:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:18:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:18:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:18:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.48it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.48it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:18:53 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:18:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.26 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.49 toks/s, output: 101.52 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.49 toks/s, output: 101.52 toks/s]
[winogrande thr=0.20 | sample 385] energy=5014.18 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:18:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:18:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:18:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:18:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 21:19:06 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 21:19:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.033731 seconds
INFO 05-12 21:19:07 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:19:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:19:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:19:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:19:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:19:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 21:19:30 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:19:30 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.70 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.52 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 386] energy=4939.65 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:19:32 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:19:32 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:19:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:19:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.94s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.99s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.83s/it]

INFO 05-12 21:19:48 [loader.py:458] Loading weights took 13.65 seconds
INFO 05-12 21:22:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 203.359709 seconds
INFO 05-12 21:24:53 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:24:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:24:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:26:06 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:26:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:26:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:25,  1.31it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:23,  1.39it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:22,  1.45it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.48it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:26:40 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:26:40 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 108.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.49 toks/s, output: 101.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.49 toks/s, output: 101.59 toks/s]
[winogrande thr=0.20 | sample 387] energy=50710.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:26:42 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:26:42 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:26:44 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:26:44 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:26:54 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 21:26:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.057716 seconds
INFO 05-12 21:26:55 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:26:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:26:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:26:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:26:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:26:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:27:18 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:27:18 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.62 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.63 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.63 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 388] energy=4907.96 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:27:20 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:27:20 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:27:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:27:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:27:31 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 21:27:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.075122 seconds
INFO 05-12 21:27:32 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:27:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:27:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:27:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:27:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:27:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 21:27:55 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:27:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.80 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 40.55 toks/s, output: 101.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 40.55 toks/s, output: 101.36 toks/s]
[winogrande thr=0.20 | sample 389] energy=4945.26 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:27:57 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:27:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:27:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:27:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:28:09 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 21:28:10 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.690195 seconds
INFO 05-12 21:28:10 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 21:28:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:28:10 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:28:11 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:28:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:28:11 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 21:28:34 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:28:34 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.90 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.49 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.49 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 390] energy=5029.16 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:28:36 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:28:36 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:28:37 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:28:37 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:28:47 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 21:28:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.079602 seconds
INFO 05-12 21:28:48 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:28:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:28:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:28:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:28:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:28:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.49it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:29:12 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:29:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.47 toks/s, output: 101.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 30.47 toks/s, output: 101.55 toks/s]
[winogrande thr=0.20 | sample 391] energy=4981.09 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:29:14 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:29:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:29:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:29:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.47s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 21:29:26 [loader.py:458] Loading weights took 10.18 seconds
INFO 05-12 21:29:26 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.545327 seconds
INFO 05-12 21:29:27 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:29:27 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:29:27 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:29:27 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:29:27 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:29:27 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.47it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.49it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.48it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.49it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.48it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:16<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:18<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]
INFO 05-12 21:29:51 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:29:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.39 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.45 toks/s, output: 101.45 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.45 toks/s, output: 101.45 toks/s]
[winogrande thr=0.20 | sample 392] energy=5111.10 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:29:53 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:29:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:29:54 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:29:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.45s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.80s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.10s/it]

INFO 05-12 21:30:07 [loader.py:458] Loading weights took 12.52 seconds
INFO 05-12 21:30:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.850411 seconds
INFO 05-12 21:30:08 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:30:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:30:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:30:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:30:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:30:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.46it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.48it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:28<01:02,  4.84s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:53<02:10, 10.87s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [04:00<11:40, 63.71s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [05:14<11:06, 66.70s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [09:33<18:40, 124.45s/it]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [11:26<16:09, 121.15s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [18:14<24:10, 207.27s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [19:49<17:20, 173.43s/it]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [19:51<10:10, 122.13s/it]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [23:33<10:08, 152.11s/it]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [24:17<05:58, 119.59s/it]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [24:21<02:49, 84.75s/it] Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [24:21<00:59, 59.52s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [24:22<00:00, 41.87s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [24:22<00:00, 41.78s/it]
INFO 05-12 21:54:31 [model_runner.py:1598] Graph capturing finished in 1462 secs, took 0.06 GiB
INFO 05-12 21:54:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 1463.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.44 toks/s, output: 101.30 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 34.44 toks/s, output: 101.30 toks/s]
[winogrande thr=0.20 | sample 393] energy=173248.45 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:54:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:54:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:54:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:54:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.65s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.76s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.77s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.90s/it]

INFO 05-12 21:54:46 [loader.py:458] Loading weights took 11.82 seconds
INFO 05-12 21:54:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.151937 seconds
INFO 05-12 21:54:47 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:54:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:54:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:54:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:54:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:54:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.49it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.49it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:55:11 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:55:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.18 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.70 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.53 toks/s, output: 101.70 toks/s]
[winogrande thr=0.20 | sample 394] energy=5236.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:55:13 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:55:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:55:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:55:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 21:55:24 [loader.py:458] Loading weights took 9.81 seconds
INFO 05-12 21:55:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.146987 seconds
INFO 05-12 21:55:25 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:55:25 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:55:25 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:55:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:55:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:55:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.49it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.48it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.48it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:55:49 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:55:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.18 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.47 toks/s, output: 101.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.47 toks/s, output: 101.47 toks/s]
[winogrande thr=0.20 | sample 395] energy=4997.71 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:55:51 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:55:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:55:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:55:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:56:02 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 21:56:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.044128 seconds
INFO 05-12 21:56:03 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:56:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:56:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:56:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:56:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:56:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.46it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 21:56:27 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:56:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.96 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.53 toks/s, output: 101.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.53 toks/s, output: 101.66 toks/s]
[winogrande thr=0.20 | sample 396] energy=4942.55 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:56:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:56:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:56:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:56:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 21:56:40 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 21:56:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.090454 seconds
INFO 05-12 21:56:41 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:56:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:56:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:56:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:56:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:56:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.49it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:57:05 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:57:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.14 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.48 toks/s, output: 101.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 31.48 toks/s, output: 101.54 toks/s]
[winogrande thr=0.20 | sample 397] energy=5000.16 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:57:07 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:57:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:57:08 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:57:08 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:57:18 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 21:57:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.053117 seconds
INFO 05-12 21:57:20 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:57:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:57:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:57:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:57:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:57:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:57:43 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:57:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.08 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.72 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.55 toks/s, output: 101.72 toks/s]
[winogrande thr=0.20 | sample 398] energy=4982.35 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:57:45 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:57:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:57:46 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:57:46 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:57:56 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 21:57:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.116043 seconds
INFO 05-12 21:57:57 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:57:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:57:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:57:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:57:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:57:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.49it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 21:58:21 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:58:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.02 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.52 toks/s, output: 101.68 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.52 toks/s, output: 101.68 toks/s]
[winogrande thr=0.20 | sample 399] energy=4970.12 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:58:23 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:58:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:58:24 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:58:24 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:58:34 [loader.py:458] Loading weights took 9.74 seconds
INFO 05-12 21:58:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.071324 seconds
INFO 05-12 21:58:35 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:58:35 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:58:35 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:58:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:58:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:58:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.45it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.49it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.49it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:58:59 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:58:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.08 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.47 toks/s, output: 101.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.47 toks/s, output: 101.46 toks/s]
[winogrande thr=0.20 | sample 400] energy=4966.72 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:59:01 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:59:01 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:59:03 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:59:03 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 21:59:13 [loader.py:458] Loading weights took 9.75 seconds
INFO 05-12 21:59:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.077560 seconds
INFO 05-12 21:59:14 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:59:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:59:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:59:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:59:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:59:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.49it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.49it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:59:37 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:59:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.12 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.58 toks/s, output: 101.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.58 toks/s, output: 101.57 toks/s]
[winogrande thr=0.20 | sample 401] energy=4996.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:59:39 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 21:59:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:59:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:59:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.90s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.27s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.17s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.69s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.70s/it]

INFO 05-12 21:59:52 [loader.py:458] Loading weights took 11.00 seconds
INFO 05-12 21:59:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.326754 seconds
INFO 05-12 21:59:53 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:59:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:59:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:59:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:59:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:59:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:24,  1.39it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.47it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 22:00:17 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:00:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.58 toks/s, output: 101.56 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 37.58 toks/s, output: 101.56 toks/s]
[winogrande thr=0.20 | sample 402] energy=5133.31 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:00:19 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:00:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:00:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:00:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.87s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.15s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.63s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.18s/it]

INFO 05-12 22:00:33 [loader.py:458] Loading weights took 12.89 seconds
INFO 05-12 22:00:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.319045 seconds
INFO 05-12 22:00:35 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:00:35 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:00:35 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:00:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:00:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:00:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:00:58 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:00:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.12 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 48.68 toks/s, output: 101.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 48.68 toks/s, output: 101.42 toks/s]
[winogrande thr=0.20 | sample 403] energy=5369.00 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:01:00 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:01:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:01:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:01:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.91s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:01:12 [loader.py:458] Loading weights took 9.88 seconds
INFO 05-12 22:01:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.225020 seconds
INFO 05-12 22:01:13 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:01:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:01:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:01:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:01:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:01:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.49it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 22:01:36 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:01:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.04 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 46.70 toks/s, output: 101.51 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 46.70 toks/s, output: 101.51 toks/s]
[winogrande thr=0.20 | sample 404] energy=4996.66 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:01:38 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:01:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:01:40 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:01:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 22:01:50 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 22:01:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.048822 seconds
INFO 05-12 22:01:51 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:01:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:01:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:01:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:01:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:01:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.49it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:02:14 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:02:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.14 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.57 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.57 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 405] energy=4996.13 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:02:16 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:02:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:02:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:02:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 22:02:28 [loader.py:458] Loading weights took 9.70 seconds
INFO 05-12 22:02:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.073421 seconds
INFO 05-12 22:02:29 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:02:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:02:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:02:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:02:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:02:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.48it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.49it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.47it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.49it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.47it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.47it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.49it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.49it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.49it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 22:02:52 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:02:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.29 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.51 toks/s, output: 101.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.51 toks/s, output: 101.69 toks/s]
[winogrande thr=0.20 | sample 406] energy=4999.41 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:02:55 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:02:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:02:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:02:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 22:03:06 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 22:03:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.104344 seconds
INFO 05-12 22:03:07 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 22:03:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:03:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:03:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:03:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:03:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.47it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.48it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.49it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.49it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:04,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 22:03:31 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:03:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.32 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 38.49 toks/s, output: 101.47 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 38.49 toks/s, output: 101.47 toks/s]
[winogrande thr=0.20 | sample 407] energy=4997.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:03:33 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:03:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:03:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:03:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 22:03:44 [loader.py:458] Loading weights took 9.69 seconds
INFO 05-12 22:03:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.026603 seconds
INFO 05-12 22:03:45 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:03:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:03:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:03:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:03:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:03:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:04:08 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:04:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.79 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.79 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.61 toks/s, output: 101.79 toks/s]
[winogrande thr=0.20 | sample 408] energy=4940.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:04:11 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:04:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:04:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:04:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [13:07<15:25, 462.93s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [13:15<04:14, 254.86s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [13:17<00:00, 155.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [13:17<00:00, 199.46s/it]

INFO 05-12 22:17:30 [loader.py:458] Loading weights took 798.07 seconds
INFO 05-12 22:17:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 798.577264 seconds
INFO 05-12 22:17:32 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:17:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:17:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:17:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:17:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:17:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:04,  1.48it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.49it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.47it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.49it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.49it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:17:55 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:17:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.11 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.40 toks/s, output: 101.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 35.40 toks/s, output: 101.14 toks/s]
[winogrande thr=0.20 | sample 409] energy=96963.17 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:17:57 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:17:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:17:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:17:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 22:18:09 [loader.py:458] Loading weights took 9.76 seconds
INFO 05-12 22:18:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.091943 seconds
INFO 05-12 22:18:10 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:18:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:18:10 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:18:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:18:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:18:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.47it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [01:59<19:16, 39.88s/it]Capturing CUDA graph shapes:  20%|██        | 7/35 [03:03<22:22, 47.95s/it]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [03:39<19:47, 43.99s/it]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [10:04<1:05:19, 150.77s/it]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [15:16<1:23:31, 200.45s/it]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [16:06<1:01:45, 154.41s/it]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [16:07<41:17, 107.70s/it]  Capturing CUDA graph shapes:  37%|███▋      | 13/35 [16:29<30:01, 81.87s/it] Capturing CUDA graph shapes:  40%|████      | 14/35 [16:34<20:30, 58.61s/it]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [16:35<13:43, 41.17s/it]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [16:36<09:10, 28.98s/it]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [16:36<06:08, 20.47s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [16:37<04:06, 14.52s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [16:38<02:45, 10.36s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [16:38<01:51,  7.46s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [16:39<01:15,  5.43s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [16:40<00:51,  4.00s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [16:40<00:35,  3.00s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [16:41<00:25,  2.30s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [16:42<00:18,  1.81s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [16:42<00:13,  1.46s/it]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [16:43<00:09,  1.22s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [16:44<00:07,  1.05s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [16:44<00:05,  1.06it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [16:45<00:04,  1.17it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [16:46<00:03,  1.25it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [16:46<00:02,  1.32it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [16:47<00:01,  1.37it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [16:48<00:00,  1.41it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [16:48<00:00,  1.44it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [16:48<00:00, 28.83s/it]
INFO 05-12 22:34:59 [model_runner.py:1598] Graph capturing finished in 1009 secs, took 0.06 GiB
INFO 05-12 22:34:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 1010.01 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.59 toks/s, output: 101.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.59 toks/s, output: 101.65 toks/s]
[winogrande thr=0.20 | sample 410] energy=120036.45 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:35:01 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:35:01 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:35:03 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:35:03 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.97s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 22:35:13 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 22:35:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.137291 seconds
INFO 05-12 22:35:14 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 22:35:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:35:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:35:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:35:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:35:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 22:35:38 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:35:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.29 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.48 toks/s, output: 101.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.48 toks/s, output: 101.59 toks/s]
[winogrande thr=0.20 | sample 411] energy=5033.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:35:40 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:35:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:35:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:35:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]

INFO 05-12 22:35:51 [loader.py:458] Loading weights took 9.77 seconds
INFO 05-12 22:35:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.340966 seconds
INFO 05-12 22:35:52 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:35:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:35:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:35:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:35:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:35:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.46it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:36:16 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:36:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.11 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.64 toks/s, output: 101.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.64 toks/s, output: 101.59 toks/s]
[winogrande thr=0.20 | sample 412] energy=5006.01 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:36:18 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:36:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:36:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:36:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 22:36:29 [loader.py:458] Loading weights took 9.80 seconds
INFO 05-12 22:36:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.134178 seconds
INFO 05-12 22:36:30 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:36:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:36:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:36:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:36:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:36:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.49it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.48it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:36:54 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:36:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.14 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 40.60 toks/s, output: 101.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 40.60 toks/s, output: 101.50 toks/s]
[winogrande thr=0.20 | sample 413] energy=4996.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:36:56 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:36:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:36:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:36:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]

INFO 05-12 22:37:07 [loader.py:458] Loading weights took 9.81 seconds
INFO 05-12 22:37:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.140021 seconds
INFO 05-12 22:37:09 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:37:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:37:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:37:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:37:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:37:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.49it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:37:32 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:37:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.58 toks/s, output: 101.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.58 toks/s, output: 101.65 toks/s]
[winogrande thr=0.20 | sample 414] energy=4995.86 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:37:35 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:37:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:37:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:37:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.10s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.24s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]

INFO 05-12 22:37:47 [loader.py:458] Loading weights took 10.51 seconds
INFO 05-12 22:37:47 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.852718 seconds
INFO 05-12 22:37:48 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 22:37:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:37:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:37:48 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:37:48 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:37:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.47it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.49it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.48it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 22:38:12 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:38:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.21 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 46.76 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 46.76 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 415] energy=5157.75 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:38:14 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:38:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:38:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:38:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 22:38:25 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 22:38:26 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.059633 seconds
INFO 05-12 22:38:26 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:38:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:38:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:38:27 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:38:27 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:38:27 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.45it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:38:49 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:38:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.85 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.72 toks/s, output: 101.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.72 toks/s, output: 101.63 toks/s]
[winogrande thr=0.20 | sample 416] energy=4968.72 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:38:52 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:38:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:38:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:38:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 22:39:03 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 22:39:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.061170 seconds
INFO 05-12 22:39:04 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:39:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:39:04 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:39:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:39:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:39:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:39:27 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:39:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.81 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.62 toks/s, output: 101.66 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.62 toks/s, output: 101.66 toks/s]
[winogrande thr=0.20 | sample 417] energy=4951.82 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:39:29 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:39:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:39:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:39:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.02s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.40s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.24s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.76s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.78s/it]

INFO 05-12 22:39:42 [loader.py:458] Loading weights took 11.33 seconds
INFO 05-12 22:39:43 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.684075 seconds
INFO 05-12 22:39:43 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:39:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:39:43 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:39:44 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:39:44 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:39:44 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:40:06 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:40:06 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.78 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.62 toks/s, output: 101.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.62 toks/s, output: 101.62 toks/s]
[winogrande thr=0.20 | sample 418] energy=5116.80 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:40:09 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:40:09 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:40:10 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:40:10 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.87s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.89s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.81s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.97s/it]

INFO 05-12 22:40:23 [loader.py:458] Loading weights took 12.07 seconds
INFO 05-12 22:40:23 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.469032 seconds
INFO 05-12 22:40:24 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 22:40:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:40:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:40:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:40:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:40:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.49it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:40:47 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:40:47 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.92 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.50 toks/s, output: 101.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.50 toks/s, output: 101.53 toks/s]
[winogrande thr=0.20 | sample 419] energy=5299.19 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:40:49 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:40:49 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:40:51 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:40:51 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 22:41:01 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 22:41:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.055575 seconds
INFO 05-12 22:41:02 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:41:02 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:41:02 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:41:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:41:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:41:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:41:25 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:41:25 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.95 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.55 toks/s, output: 101.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.55 toks/s, output: 101.67 toks/s]
[winogrande thr=0.20 | sample 420] energy=4996.17 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:41:27 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:41:27 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:41:28 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:41:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.74s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]

INFO 05-12 22:41:39 [loader.py:458] Loading weights took 9.72 seconds
INFO 05-12 22:41:39 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.052359 seconds
INFO 05-12 22:41:40 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:41:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:41:40 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:41:40 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:41:40 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:41:40 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:42:03 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:42:03 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.88 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.45 toks/s, output: 101.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 32.45 toks/s, output: 101.42 toks/s]
[winogrande thr=0.20 | sample 421] energy=4946.31 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:42:05 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:42:05 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:42:06 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:42:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.96s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 22:42:16 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 22:42:17 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.046559 seconds
INFO 05-12 22:42:17 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:42:17 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:42:17 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:42:18 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:42:18 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:42:18 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:42:40 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:42:41 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.85 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.57 toks/s, output: 101.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.57 toks/s, output: 101.64 toks/s]
[winogrande thr=0.20 | sample 422] energy=4947.18 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:42:43 [config.py:689] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 05-12 22:42:43 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:42:44 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:42:44 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.95s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  2.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]

INFO 05-12 22:42:54 [loader.py:458] Loading weights took 9.71 seconds
INFO 05-12 22:42:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.040696 seconds
INFO 05-12 22:42:55 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:42:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:42:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:42:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:42:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:42:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]slurmstepd: error: *** JOB 5142011 ON lrz-hgx-h100-005 CANCELLED AT 2025-05-12T22:43:04 ***
slurmstepd: error: *** JOB 5142011 STEPD TERMINATED ON lrz-hgx-h100-005 AT 2025-05-12T22:44:04 DUE TO JOB NOT ENDING WITH SIGNALS ***
slurmstepd: error: Container 2419133 in cgroup plugin has 1 processes, giving up after 63 sec
