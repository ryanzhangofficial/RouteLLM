Mon May 12 19:06:48 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100                    On  | 00000000:C6:00.0 Off |                    0 |
| N/A   38C    P0             123W / 600W |      0MiB / 95830MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
[INFO] Extracting squashfs filesystem...
Parallel unsquashfs: Using 20 processors
217327 inodes (393318 blocks) to write


created 215418 files
created 30529 directories
created 1903 symlinks
created 0 devices
created 0 fifos
created 0 sockets

=============
== PyTorch ==
=============

NVIDIA Release 25.01 (build 134983853)
PyTorch Version 2.6.0a0+ecf3bae
Container image Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright (c) 2014-2024 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

NOTE: CUDA Forward Compatibility mode ENABLED.
  Using CUDA 12.8 driver version 570.86.10 with kernel driver version 535.230.02.
  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.

NOTE: Mellanox network driver detected, but NVIDIA peer memory driver not
      detected.  Multi-node communication performance may be reduced.

INFO 05-12 19:07:40 [__init__.py:239] Automatically detected platform cuda.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: ryanzhangofficial (ryzhangofficial). Use `wandb login --relogin` to force relogin
  0%|          | 0/1 [00:00<?, ?it/s]Loading bert:   0%|          | 0/1 [00:00<?, ?it/s]Loading bert:   0%|          | 0/1 [00:01<?, ?it/s]
/dss/dssfs04/lwp-dss-0002/pn72yi/pn72yi-dss-0000/ge56heh2/mess-plus/venv/lib/python3.12/site-packages/datasets/load.py:1298: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
wandb: Currently logged in as: ryanzhangofficial (tum-i13). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /workspace/wandb/run-20250512_190745-ywgd7s52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bert-winogrande-thr-0.45
wandb: ⭐️ View project at https://wandb.ai/tum-i13/routellm-sweep
wandb: 🚀 View run at https://wandb.ai/tum-i13/routellm-sweep/runs/ywgd7s52
Loaded winogrande: 1267 samples
[2025-05-12 19:07:46,487] [zeus.device.gpu.nvidia](nvidia.py:47) pynvml is available and initialized.
[2025-05-12 19:07:46,496] [zeus.device.cpu.rapl](rapl.py:136) RAPL is available.
[2025-05-12 19:07:46,496] [zeus.monitor.energy](energy.py:208) Monitoring GPU indices [0].
[2025-05-12 19:07:46,497] [zeus.monitor.energy](energy.py:209) Monitoring CPU indices []
[2025-05-12 19:07:46,497] [zeus.utils.framework](framework.py:25) PyTorch with CUDA support is available.
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:08:00 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:08:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:08:03 [cuda.py:292] Using Flash Attention backend.
INFO 05-12 19:08:03 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-12 19:08:03 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:08:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.45s/it]

INFO 05-12 19:08:14 [loader.py:458] Loading weights took 10.03 seconds
INFO 05-12 19:08:14 [model_runner.py:1146] Model loading took 14.9889 GiB and 10.726442 seconds
INFO 05-12 19:08:15 [worker.py:267] Memory profiling takes 0.66 seconds
INFO 05-12 19:08:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:08:15 [worker.py:267] model weights take 14.99GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 67.36GiB.
INFO 05-12 19:08:16 [executor_base.py:112] # cuda blocks: 34487, # CPU blocks: 2048
INFO 05-12 19:08:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 269.43x
INFO 05-12 19:08:18 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:08:41 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.32 GiB
INFO 05-12 19:08:41 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 26.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 28.77 toks/s, output: 99.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 28.77 toks/s, output: 99.21 toks/s]
[winogrande thr=0.45 | sample 0] energy=7700.80 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:08:48 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:08:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:08:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:08:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.48s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.65s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.47s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.07s/it]

INFO 05-12 19:09:02 [loader.py:458] Loading weights took 12.45 seconds
INFO 05-12 19:09:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.794554 seconds
INFO 05-12 19:09:03 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:09:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:09:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:09:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:09:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:09:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:09:26 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:09:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.45 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.55 toks/s, output: 101.91 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.55 toks/s, output: 101.91 toks/s]
[winogrande thr=0.45 | sample 1] energy=5325.76 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:09:28 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:09:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:09:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:09:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 19:09:39 [loader.py:458] Loading weights took 9.99 seconds
INFO 05-12 19:09:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.326083 seconds
INFO 05-12 19:09:40 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:09:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:09:40 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:09:41 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:09:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:09:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:10:03 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:10:03 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.26 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.52 toks/s, output: 101.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 33.52 toks/s, output: 101.58 toks/s]
[winogrande thr=0.45 | sample 2] energy=4991.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:10:05 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:10:05 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:10:06 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:10:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:10:17 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 19:10:17 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.240434 seconds
INFO 05-12 19:10:18 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:10:18 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:10:18 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:10:18 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:10:18 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:10:18 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:10:40 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:10:40 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.26 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.55 toks/s, output: 101.76 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.55 toks/s, output: 101.76 toks/s]
[winogrande thr=0.45 | sample 3] energy=4980.98 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:10:42 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:10:42 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:10:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:10:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:10:54 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 19:10:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.255343 seconds
INFO 05-12 19:10:55 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:10:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:10:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:10:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:10:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:10:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:11:17 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:11:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.32 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.58 toks/s, output: 101.80 toks/s]
[winogrande thr=0.45 | sample 4] energy=4979.79 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.2-1B-Instruct
INFO 05-12 19:11:20 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:11:20 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:11:20 [cuda.py:292] Using Flash Attention backend.
INFO 05-12 19:11:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...
INFO 05-12 19:11:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 19:11:22 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]

INFO 05-12 19:11:23 [loader.py:458] Loading weights took 1.60 seconds
INFO 05-12 19:11:23 [model_runner.py:1146] Model loading took 2.3185 GiB and 2.287485 seconds
INFO 05-12 19:11:24 [worker.py:267] Memory profiling takes 0.32 seconds
INFO 05-12 19:11:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:11:24 [worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.17GiB; the rest of the memory reserved for KV Cache is 80.22GiB.
INFO 05-12 19:11:24 [executor_base.py:112] # cuda blocks: 164299, # CPU blocks: 8192
INFO 05-12 19:11:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 1283.59x
INFO 05-12 19:11:27 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.64it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.64it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:18,  1.65it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.62it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.63it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:15,  1.64it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.64it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.64it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:13,  1.65it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:07<00:13,  1.66it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:12,  1.62it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.63it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.64it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:10,  1.64it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:10<00:10,  1.65it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.64it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.64it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:08,  1.64it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:07,  1.65it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.64it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.65it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.65it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:15<00:05,  1.64it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.64it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.64it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:17<00:03,  1.62it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.63it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:18<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:19<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:20<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.63it/s]
INFO 05-12 19:11:49 [model_runner.py:1598] Graph capturing finished in 21 secs, took 0.06 GiB
INFO 05-12 19:11:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 25.15 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.15it/s, est. speed input: 116.46 toks/s, output: 314.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.14it/s, est. speed input: 116.46 toks/s, output: 314.73 toks/s]
[winogrande thr=0.45 | sample 5] energy=3954.17 J, correct=False, model=meta-llama/Llama-3.2-1B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:11:52 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:11:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:11:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:11:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.87s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 19:12:04 [loader.py:458] Loading weights took 10.01 seconds
INFO 05-12 19:12:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.336272 seconds
INFO 05-12 19:12:05 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:12:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:12:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:12:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:12:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:12:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.61it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:17,  1.62it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.62it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.62it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.62it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.62it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.62it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:19<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 19:12:27 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:12:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.90 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.78 toks/s, output: 102.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.78 toks/s, output: 102.05 toks/s]
[winogrande thr=0.45 | sample 6] energy=5020.11 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:12:29 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:12:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:12:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:12:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:12:40 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 19:12:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.249129 seconds
INFO 05-12 19:12:42 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:12:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:12:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:12:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:12:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:12:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:13:04 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:13:04 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.38 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.83 toks/s, output: 102.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.83 toks/s, output: 102.14 toks/s]
[winogrande thr=0.45 | sample 7] energy=4977.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:13:06 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:13:06 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:13:07 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:13:07 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.47s/it]

INFO 05-12 19:13:18 [loader.py:458] Loading weights took 10.14 seconds
INFO 05-12 19:13:18 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.474078 seconds
INFO 05-12 19:13:19 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:13:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:13:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:13:19 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:13:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:13:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 19:13:41 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:13:41 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.09 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 21.57it/s, est. speed input: 718.63 toks/s, output: 65.30 toks/s]
[winogrande thr=0.45 | sample 8] energy=4651.32 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:13:42 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:13:42 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:13:44 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:13:44 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.05s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.58s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.87s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.06s/it]

INFO 05-12 19:13:57 [loader.py:458] Loading weights took 12.44 seconds
INFO 05-12 19:13:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.858236 seconds
INFO 05-12 19:13:58 [worker.py:267] Memory profiling takes 0.37 seconds
INFO 05-12 19:13:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:13:58 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:13:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:13:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:13:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 19:14:20 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:14:20 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.26 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.65 toks/s, output: 101.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.65 toks/s, output: 101.98 toks/s]
[winogrande thr=0.45 | sample 9] energy=5278.94 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:14:22 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:14:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:14:24 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:14:24 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 19:14:34 [loader.py:458] Loading weights took 9.98 seconds
INFO 05-12 19:14:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.320989 seconds
INFO 05-12 19:14:35 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:14:35 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:14:35 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:14:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:14:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:14:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:14:57 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:14:57 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.30 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.72 toks/s, output: 102.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.72 toks/s, output: 102.17 toks/s]
[winogrande thr=0.45 | sample 10] energy=5001.94 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:15:00 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:15:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:15:02 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:15:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.40s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.58s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.98s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.99s/it]

INFO 05-12 19:15:14 [loader.py:458] Loading weights took 12.12 seconds
INFO 05-12 19:15:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.495925 seconds
INFO 05-12 19:15:15 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:15:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:15:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:15:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:15:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:15:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:24,  1.40it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:15:38 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 19:15:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.68 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.64 toks/s, output: 102.06 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.64 toks/s, output: 102.06 toks/s]
[winogrande thr=0.45 | sample 11] energy=5399.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:15:40 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:15:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:15:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:15:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:15:52 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 19:15:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.465725 seconds
INFO 05-12 19:15:53 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:15:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:15:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:15:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:15:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:15:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.49it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:16:16 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 19:16:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.70 toks/s, output: 102.24 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.70 toks/s, output: 102.24 toks/s]
[winogrande thr=0.45 | sample 12] energy=5038.33 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:16:18 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:16:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:16:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:16:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 19:16:30 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 19:16:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.317823 seconds
INFO 05-12 19:16:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:16:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:16:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:16:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:16:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:16:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 19:16:53 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:16:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.05 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.94 toks/s, output: 102.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.94 toks/s, output: 102.36 toks/s]
[winogrande thr=0.45 | sample 13] energy=4972.32 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:16:55 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:16:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:16:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:16:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:17:06 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 19:17:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.245899 seconds
INFO 05-12 19:17:08 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:17:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:17:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:17:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:17:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:17:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.62it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.59it/s]
INFO 05-12 19:17:30 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:17:30 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.99 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.94 toks/s, output: 102.29 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.94 toks/s, output: 102.29 toks/s]
[winogrande thr=0.45 | sample 14] energy=4921.40 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:17:32 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:17:32 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:17:33 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:17:33 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.88s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.08s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]

INFO 05-12 19:17:44 [loader.py:458] Loading weights took 10.09 seconds
INFO 05-12 19:17:44 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.494261 seconds
INFO 05-12 19:17:45 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:17:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:17:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:17:45 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:17:45 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:17:45 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 19:18:08 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 19:18:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.72 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.90 toks/s, output: 102.30 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.90 toks/s, output: 102.30 toks/s]
[winogrande thr=0.45 | sample 15] energy=5035.14 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:18:10 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:18:10 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:18:11 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:18:11 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.08s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 19:18:22 [loader.py:458] Loading weights took 10.19 seconds
INFO 05-12 19:18:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.532324 seconds
INFO 05-12 19:18:23 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:18:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:18:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:18:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:18:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:18:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:18:46 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:18:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.52 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.77 toks/s, output: 102.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.77 toks/s, output: 102.39 toks/s]
[winogrande thr=0.45 | sample 16] energy=5091.97 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:18:48 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:18:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:18:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:18:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 19:18:50 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.927928 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.87s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 19:19:00 [loader.py:458] Loading weights took 10.21 seconds
INFO 05-12 19:19:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.328410 seconds
INFO 05-12 19:19:02 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:19:02 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:19:02 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:19:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:19:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:19:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:19:24 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:19:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.79 toks/s, output: 102.48 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.79 toks/s, output: 102.48 toks/s]
[winogrande thr=0.45 | sample 17] energy=5148.42 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:19:26 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:19:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:19:28 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:19:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.88s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.11s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.50s/it]

INFO 05-12 19:19:38 [loader.py:458] Loading weights took 10.23 seconds
INFO 05-12 19:19:39 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.588705 seconds
INFO 05-12 19:19:39 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:19:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:19:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:19:40 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:19:40 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:19:40 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:15<00:19,  1.30s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [02:21<09:04, 38.90s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [02:50<07:48, 36.02s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [05:41<15:16, 76.39s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [06:12<11:31, 62.88s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [06:15<07:28, 44.81s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [06:16<04:44, 31.56s/it]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [06:16<02:58, 22.28s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [06:17<01:50, 15.78s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [06:18<01:07, 11.24s/it]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [06:18<00:40,  8.05s/it]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [06:19<00:23,  5.83s/it]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [06:19<00:12,  4.29s/it]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [06:20<00:06,  3.19s/it]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [06:21<00:02,  2.43s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [06:21<00:00,  1.89s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [06:21<00:00, 10.91s/it]
INFO 05-12 19:26:02 [model_runner.py:1598] Graph capturing finished in 382 secs, took 0.09 GiB
INFO 05-12 19:26:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 382.96 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.80 toks/s, output: 102.28 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.80 toks/s, output: 102.28 toks/s]
[winogrande thr=0.45 | sample 18] energy=47865.86 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:26:04 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:26:04 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:26:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:26:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 19:26:16 [loader.py:458] Loading weights took 9.98 seconds
INFO 05-12 19:26:16 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.358485 seconds
INFO 05-12 19:26:17 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:26:17 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:26:17 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:26:17 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:26:17 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:26:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:26:39 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:26:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.38 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.67 toks/s, output: 102.30 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.67 toks/s, output: 102.30 toks/s]
[winogrande thr=0.45 | sample 19] energy=5033.44 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:26:41 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:26:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:26:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:26:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.34s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.72s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.05s/it]

INFO 05-12 19:26:55 [loader.py:458] Loading weights took 12.36 seconds
INFO 05-12 19:26:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.691224 seconds
INFO 05-12 19:26:56 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:26:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:26:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:26:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:26:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:26:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:17,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.59it/s]
INFO 05-12 19:27:19 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:27:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.01 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 48.10 toks/s, output: 102.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 48.10 toks/s, output: 102.35 toks/s]
[winogrande thr=0.45 | sample 20] energy=5224.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:27:21 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:27:21 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:27:22 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:27:22 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.93s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.06s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]

INFO 05-12 19:27:32 [loader.py:458] Loading weights took 10.08 seconds
INFO 05-12 19:27:33 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.413609 seconds
INFO 05-12 19:27:33 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:27:33 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:27:33 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:27:34 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:27:34 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:27:34 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 19:27:56 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:27:56 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.08 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.77 toks/s, output: 102.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.77 toks/s, output: 102.33 toks/s]
[winogrande thr=0.45 | sample 21] energy=4969.91 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:27:58 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:27:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:27:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:27:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:28:10 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 19:28:10 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.440625 seconds
INFO 05-12 19:28:11 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:28:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:28:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:28:11 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:28:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:28:11 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:28:33 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:28:33 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.36 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.73 toks/s, output: 102.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.85s/it, est. speed input: 31.73 toks/s, output: 102.35 toks/s]
[winogrande thr=0.45 | sample 22] energy=17919.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:34:52 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:34:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:34:54 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:34:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.48s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.62s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.85s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.91s/it]

INFO 05-12 19:35:06 [loader.py:458] Loading weights took 11.84 seconds
INFO 05-12 19:35:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.178053 seconds
INFO 05-12 19:35:07 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:35:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:35:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:35:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:35:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:35:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:14<00:31,  1.75s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:17<00:37,  2.23s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:21<00:41,  2.61s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:23<00:36,  2.45s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:24<00:29,  2.10s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:25<00:22,  1.70s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:26<00:16,  1.38s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:26<00:12,  1.16s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:27<00:10,  1.01s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:28<00:08,  1.11it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:28<00:06,  1.22it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:29<00:05,  1.30it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:30<00:04,  1.37it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:30<00:03,  1.43it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:31<00:02,  1.47it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:32<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:32<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:33<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:33<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:33<00:00,  1.03it/s]
INFO 05-12 19:35:41 [model_runner.py:1598] Graph capturing finished in 34 secs, took 0.09 GiB
INFO 05-12 19:35:41 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 34.94 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.82 toks/s, output: 102.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.82 toks/s, output: 102.16 toks/s]
[winogrande thr=0.45 | sample 23] energy=38565.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:35:43 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:35:43 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:35:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:35:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 19:35:55 [loader.py:458] Loading weights took 9.96 seconds
INFO 05-12 19:35:55 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.293857 seconds
INFO 05-12 19:35:56 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 19:35:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:35:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:36:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:36:22 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:36:22 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:24,  1.40it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:36:45 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.09 GiB
INFO 05-12 19:36:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 50.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.91 toks/s, output: 102.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.91 toks/s, output: 102.39 toks/s]
[winogrande thr=0.45 | sample 24] energy=8203.05 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:36:47 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:36:47 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:36:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:36:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:36:59 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 19:36:59 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.278724 seconds
INFO 05-12 19:37:00 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:37:00 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:37:00 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:37:00 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:37:00 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:37:00 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 19:37:22 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:37:22 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.89 toks/s, output: 102.29 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.89 toks/s, output: 102.29 toks/s]
[winogrande thr=0.45 | sample 25] energy=4968.11 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:37:24 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:37:24 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:37:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:37:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 19:37:36 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 19:37:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.444291 seconds
INFO 05-12 19:37:37 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:37:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:37:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:37:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:37:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:37:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:38:00 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:38:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.36 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.67 toks/s, output: 102.10 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.67 toks/s, output: 102.10 toks/s]
[winogrande thr=0.45 | sample 26] energy=5032.39 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:38:03 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:38:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:38:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:38:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:38:15 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 19:38:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.243270 seconds
INFO 05-12 19:38:16 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:38:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:38:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:38:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:38:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:38:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.61it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 19:38:38 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:38:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.07 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.72 toks/s, output: 102.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.72 toks/s, output: 102.31 toks/s]
[winogrande thr=0.45 | sample 27] energy=5121.00 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:38:40 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:38:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:38:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:38:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:38:52 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 19:38:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.266287 seconds
INFO 05-12 19:38:53 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:38:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:38:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:38:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:38:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:38:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:39:16 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:39:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.53 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.70 toks/s, output: 102.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.70 toks/s, output: 102.34 toks/s]
[winogrande thr=0.45 | sample 28] energy=5032.06 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:39:18 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:39:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:39:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:39:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:39:29 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 19:39:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.246926 seconds
INFO 05-12 19:39:30 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:39:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:39:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:39:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:39:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:39:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:39:53 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:39:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.44 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.73 toks/s, output: 102.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.73 toks/s, output: 102.09 toks/s]
[winogrande thr=0.45 | sample 29] energy=5030.05 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:39:55 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:39:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:39:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:39:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.01s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.35s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.24s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.87s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.84s/it]

INFO 05-12 19:40:08 [loader.py:458] Loading weights took 11.60 seconds
INFO 05-12 19:40:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.936547 seconds
INFO 05-12 19:40:09 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:40:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:40:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:40:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:40:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:40:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:40:32 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:40:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.43 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.73 toks/s, output: 102.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.73 toks/s, output: 102.22 toks/s]
[winogrande thr=0.45 | sample 30] energy=5209.12 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:40:35 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:40:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:40:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:40:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.92s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.20s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.90s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.63s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.60s/it]

INFO 05-12 19:40:51 [loader.py:458] Loading weights took 14.57 seconds
INFO 05-12 19:40:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 14.899048 seconds
INFO 05-12 19:40:52 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:40:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:40:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:40:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:40:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:40:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:41:14 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:41:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.29 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.92 toks/s, output: 102.29 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.92 toks/s, output: 102.29 toks/s]
[winogrande thr=0.45 | sample 31] energy=5604.21 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:41:16 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:41:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:41:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:41:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.13s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.50s/it]

INFO 05-12 19:41:28 [loader.py:458] Loading weights took 10.26 seconds
INFO 05-12 19:41:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.603776 seconds
INFO 05-12 19:41:29 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:41:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:41:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:41:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:41:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:41:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:41:52 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.09 GiB
INFO 05-12 19:41:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.90 toks/s, output: 102.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.90 toks/s, output: 102.26 toks/s]
[winogrande thr=0.45 | sample 32] energy=5050.48 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:41:54 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:41:54 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:41:55 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:41:55 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:42:06 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 19:42:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.269019 seconds
INFO 05-12 19:42:07 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:42:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:42:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:42:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:42:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:42:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:42:29 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:42:29 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.82 toks/s, output: 102.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.82 toks/s, output: 102.21 toks/s]
[winogrande thr=0.45 | sample 33] energy=5001.80 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:42:31 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:42:31 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:42:33 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:42:33 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:42:43 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 19:42:44 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.417072 seconds
INFO 05-12 19:42:44 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:42:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:42:44 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:42:45 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:42:45 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:42:45 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:43:07 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:43:07 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.76 toks/s, output: 102.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.76 toks/s, output: 102.36 toks/s]
[winogrande thr=0.45 | sample 34] energy=5046.19 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:43:09 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:43:09 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:43:10 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:43:10 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:43:21 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 19:43:21 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.315042 seconds
INFO 05-12 19:43:22 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:43:22 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:43:22 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:43:22 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:43:22 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:43:22 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:43:45 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:43:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.83 toks/s, output: 102.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.83 toks/s, output: 102.17 toks/s]
[winogrande thr=0.45 | sample 35] energy=5022.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.2-1B-Instruct
INFO 05-12 19:43:47 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:43:47 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:43:48 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...
INFO 05-12 19:43:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 19:43:48 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]

INFO 05-12 19:43:50 [loader.py:458] Loading weights took 1.60 seconds
INFO 05-12 19:43:50 [model_runner.py:1146] Model loading took 2.3029 GiB and 1.905389 seconds
INFO 05-12 19:43:51 [worker.py:267] Memory profiling takes 0.33 seconds
INFO 05-12 19:43:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:43:51 [worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.17GiB; the rest of the memory reserved for KV Cache is 80.24GiB.
INFO 05-12 19:43:51 [executor_base.py:112] # cuda blocks: 164331, # CPU blocks: 8192
INFO 05-12 19:43:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 1283.84x
INFO 05-12 19:43:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.62it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.62it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.62it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.63it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.63it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:15,  1.64it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.65it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.65it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:13,  1.65it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:07<00:13,  1.65it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:12,  1.65it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.65it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.65it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:10,  1.65it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:10<00:10,  1.65it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.66it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.65it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:08,  1.66it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:07,  1.64it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.64it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.65it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.62it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:15<00:05,  1.63it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.63it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:17<00:03,  1.64it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.65it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:18<00:02,  1.64it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:19<00:01,  1.65it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.66it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:20<00:00,  1.66it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.67it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.64it/s]
INFO 05-12 19:44:12 [model_runner.py:1598] Graph capturing finished in 21 secs, took 0.03 GiB
INFO 05-12 19:44:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.32 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.67it/s, est. speed input: 121.12 toks/s, output: 311.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, est. speed input: 121.12 toks/s, output: 311.96 toks/s]
[winogrande thr=0.45 | sample 36] energy=3532.80 J, correct=False, model=meta-llama/Llama-3.2-1B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:44:16 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:44:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:44:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:44:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.56s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.91s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.58s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.23s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.23s/it]

INFO 05-12 19:44:32 [loader.py:458] Loading weights took 13.17 seconds
INFO 05-12 19:44:32 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.493596 seconds
INFO 05-12 19:44:33 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:44:33 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:44:33 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:44:33 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:44:33 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:44:33 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 19:44:55 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:44:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.61 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.61 toks/s, output: 101.74 toks/s]
[winogrande thr=0.45 | sample 37] energy=5535.78 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:44:58 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:44:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:44:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:44:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.60s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.52s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.32s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.70s/it]

INFO 05-12 19:45:10 [loader.py:458] Loading weights took 11.04 seconds
INFO 05-12 19:45:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.369177 seconds
INFO 05-12 19:45:11 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:45:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:45:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:45:12 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:45:12 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:45:12 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:45:34 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:45:34 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.28 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.69 toks/s, output: 102.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.69 toks/s, output: 102.17 toks/s]
[winogrande thr=0.45 | sample 38] energy=5162.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:45:36 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:45:36 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:45:37 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:45:37 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 19:45:47 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 19:45:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.221514 seconds
INFO 05-12 19:45:48 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:45:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:45:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:45:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:45:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:45:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:46:11 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:46:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.27 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.69 toks/s, output: 102.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.69 toks/s, output: 102.03 toks/s]
[winogrande thr=0.45 | sample 39] energy=4991.11 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:46:13 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:46:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:46:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:46:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:46:25 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 19:46:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.257288 seconds
INFO 05-12 19:46:26 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:46:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:46:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:46:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:46:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:46:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 19:46:48 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:46:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.16 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.12 toks/s]
[winogrande thr=0.45 | sample 40] energy=4981.72 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:46:50 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:46:50 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:46:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:46:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 19:47:02 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 19:47:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.238614 seconds
INFO 05-12 19:47:03 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:47:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:47:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:47:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:47:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:47:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:47:26 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:47:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.38 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.64 toks/s, output: 102.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.64 toks/s, output: 102.14 toks/s]
[winogrande thr=0.45 | sample 41] energy=4993.24 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:47:28 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:47:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:47:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:47:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 19:47:39 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 19:47:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.238925 seconds
INFO 05-12 19:47:40 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 19:47:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:47:40 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:47:41 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:47:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:47:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:48:03 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:48:03 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.91 toks/s, output: 102.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.91 toks/s, output: 102.11 toks/s]
[winogrande thr=0.45 | sample 42] energy=4989.92 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:48:05 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:48:05 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:48:06 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:48:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:48:17 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 19:48:17 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.262902 seconds
INFO 05-12 19:48:18 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:48:18 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:48:18 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:48:18 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:48:18 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:48:18 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:48:40 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:48:40 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.33 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.92 toks/s, output: 102.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.92 toks/s, output: 102.13 toks/s]
[winogrande thr=0.45 | sample 43] energy=4996.76 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:48:42 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:48:42 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:48:44 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:48:44 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:26<00:00,  6.69s/it]

INFO 05-12 19:49:11 [loader.py:458] Loading weights took 27.04 seconds
INFO 05-12 19:49:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 27.382493 seconds
INFO 05-12 19:50:09 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:50:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:50:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:50:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:51:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:51:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.44it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:28,  1.12it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:03<00:27,  1.13it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:04<00:24,  1.23it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:05<00:33,  1.17s/it]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:07<00:34,  1.22s/it]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:07<00:28,  1.06s/it]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:08<00:24,  1.07it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:09<00:21,  1.18it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:09<00:18,  1.26it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:10<00:17,  1.34it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:11<00:15,  1.40it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:11<00:14,  1.44it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:12<00:13,  1.48it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:13<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:13<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:14<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:15<00:12,  1.25it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:16<00:11,  1.32it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:16<00:10,  1.38it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:17<00:09,  1.44it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:18<00:08,  1.48it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:18<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:19<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:20<00:06,  1.36it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:21<00:06,  1.25it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:21<00:05,  1.34it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:22<00:04,  1.39it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:23<00:03,  1.44it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:23<00:02,  1.46it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:24<00:02,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:25<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:25<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:26<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:26<00:00,  1.33it/s]
INFO 05-12 19:51:37 [model_runner.py:1598] Graph capturing finished in 26 secs, took 0.06 GiB
INFO 05-12 19:51:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 88.33 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.72 toks/s, output: 102.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 34.72 toks/s, output: 102.11 toks/s]
[winogrande thr=0.45 | sample 44] energy=21725.12 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:51:40 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:51:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:51:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:51:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 19:51:43 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.919452 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.12s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.08s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.60s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.89s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.09s/it]

INFO 05-12 19:51:56 [loader.py:458] Loading weights took 12.57 seconds
INFO 05-12 19:51:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 14.368031 seconds
INFO 05-12 19:51:57 [worker.py:267] Memory profiling takes 0.37 seconds
INFO 05-12 19:51:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:51:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:51:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:51:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:51:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:52:20 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:52:20 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.39 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.89 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.72 toks/s, output: 101.89 toks/s]
[winogrande thr=0.45 | sample 45] energy=5528.55 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:52:22 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:52:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:52:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:52:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:52:33 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 19:52:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.276235 seconds
INFO 05-12 19:52:34 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:52:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:52:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:52:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:52:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:52:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:52:57 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:52:57 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.31 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.75 toks/s, output: 101.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.75 toks/s, output: 101.96 toks/s]
[winogrande thr=0.45 | sample 46] energy=5008.87 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:52:59 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:52:59 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:53:01 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:53:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.93s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.19s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.55s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]

INFO 05-12 19:53:11 [loader.py:458] Loading weights took 10.61 seconds
INFO 05-12 19:53:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.947850 seconds
INFO 05-12 19:53:12 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:53:12 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:53:12 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:53:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:53:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:53:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.43it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 19:53:35 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:53:35 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.49 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.69 toks/s, output: 101.81 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.69 toks/s, output: 101.81 toks/s]
[winogrande thr=0.45 | sample 47] energy=5128.34 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:53:37 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:53:37 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:53:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:53:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.15s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.37s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.74s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.76s/it]

INFO 05-12 19:53:50 [loader.py:458] Loading weights took 11.24 seconds
INFO 05-12 19:53:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.571175 seconds
INFO 05-12 19:53:51 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:53:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:53:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:53:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:53:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:53:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:54:14 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:54:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.63 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.75 toks/s, output: 102.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.75 toks/s, output: 102.07 toks/s]
[winogrande thr=0.45 | sample 48] energy=5195.29 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:54:16 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:54:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:54:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:54:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.86s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 19:54:28 [loader.py:458] Loading weights took 10.22 seconds
INFO 05-12 19:54:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.567243 seconds
INFO 05-12 19:54:29 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:54:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:54:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:54:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:54:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:54:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 19:54:52 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:54:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.80 toks/s, output: 102.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.80 toks/s, output: 102.15 toks/s]
[winogrande thr=0.45 | sample 49] energy=5058.09 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:54:54 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:54:54 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:54:55 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:54:55 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:55:05 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 19:55:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.277615 seconds
INFO 05-12 19:55:06 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:55:06 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:55:06 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:55:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:55:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:55:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 19:55:29 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:55:29 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.18 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.75 toks/s, output: 102.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.75 toks/s, output: 102.09 toks/s]
[winogrande thr=0.45 | sample 50] energy=4983.11 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:55:31 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:55:31 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:55:32 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:55:32 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:55:43 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 19:55:43 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.307852 seconds
INFO 05-12 19:55:44 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 19:55:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:55:44 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:55:44 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:55:44 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:55:44 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 19:56:06 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:56:06 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.17 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.76 toks/s, output: 102.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.76 toks/s, output: 102.12 toks/s]
[winogrande thr=0.45 | sample 51] energy=4978.38 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:56:08 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:56:08 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:56:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:56:10 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:56:20 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 19:56:20 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.272882 seconds
INFO 05-12 19:56:21 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 19:56:21 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:56:21 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:56:21 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:56:21 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:56:21 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.61it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 19:56:43 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 19:56:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.19 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.94 toks/s, output: 102.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.94 toks/s, output: 102.09 toks/s]
[winogrande thr=0.45 | sample 52] energy=4977.91 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:56:45 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:56:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:56:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:56:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:56:57 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 19:56:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.251491 seconds
INFO 05-12 19:56:58 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:56:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:56:58 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:56:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:56:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:56:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 19:57:21 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:57:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.83 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.79 toks/s, output: 102.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.79 toks/s, output: 102.12 toks/s]
[winogrande thr=0.45 | sample 53] energy=5050.07 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:57:23 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:57:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:57:25 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:57:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 19:57:35 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 19:57:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.325395 seconds
INFO 05-12 19:57:36 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 19:57:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:57:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:57:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:57:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:57:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 19:57:59 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:57:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.85 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.79 toks/s, output: 102.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.79 toks/s, output: 102.12 toks/s]
[winogrande thr=0.45 | sample 54] energy=5107.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:58:01 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:58:01 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:58:03 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:58:03 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.01s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.44s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.28s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.85s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.85s/it]

INFO 05-12 19:58:15 [loader.py:458] Loading weights took 11.63 seconds
INFO 05-12 19:58:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.967606 seconds
INFO 05-12 19:58:16 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 19:58:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:58:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:58:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:58:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:58:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 19:58:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:58:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.74 toks/s, output: 102.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.74 toks/s, output: 102.11 toks/s]
[winogrande thr=0.45 | sample 55] energy=5245.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:58:41 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:58:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:58:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:58:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.05s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.57s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.87s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.06s/it]

INFO 05-12 19:58:56 [loader.py:458] Loading weights took 12.43 seconds
INFO 05-12 19:58:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.844539 seconds
INFO 05-12 19:58:57 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 19:58:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:58:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:58:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:58:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:58:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 19:59:20 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:59:20 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.75 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.77 toks/s, output: 102.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.77 toks/s, output: 102.20 toks/s]
[winogrande thr=0.45 | sample 56] energy=5429.25 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:59:22 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:59:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 19:59:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 19:59:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 19:59:33 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 19:59:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.267998 seconds
INFO 05-12 19:59:34 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 19:59:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 19:59:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 19:59:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 19:59:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 19:59:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 19:59:57 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 19:59:57 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.75 toks/s, output: 102.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.75 toks/s, output: 102.15 toks/s]
[winogrande thr=0.45 | sample 57] energy=5047.94 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 19:59:59 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 19:59:59 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:00:01 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:00:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.82s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 20:00:11 [loader.py:458] Loading weights took 10.00 seconds
INFO 05-12 20:00:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.333634 seconds
INFO 05-12 20:00:12 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:00:12 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:00:12 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:00:12 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:00:12 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:00:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.49it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:00:35 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:00:35 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.89 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.70 toks/s, output: 101.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.70 toks/s, output: 101.96 toks/s]
[winogrande thr=0.45 | sample 58] energy=5090.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:00:38 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:00:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:00:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:00:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 20:00:49 [loader.py:458] Loading weights took 9.96 seconds
INFO 05-12 20:00:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.298406 seconds
INFO 05-12 20:00:50 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:00:50 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:00:50 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:00:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:00:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:00:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:11<03:31,  6.41s/it]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:20<04:02,  7.58s/it]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [02:45<32:05, 62.10s/it]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [05:32<49:59, 99.98s/it]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [05:33<32:03, 66.33s/it]Capturing CUDA graph shapes:  20%|██        | 7/35 [05:34<20:56, 44.87s/it]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [05:35<13:51, 30.78s/it]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [05:35<09:15, 21.37s/it]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [05:36<06:14, 14.96s/it]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [05:37<04:14, 10.59s/it]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [05:37<02:53,  7.56s/it]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [05:38<02:00,  5.46s/it]Capturing CUDA graph shapes:  40%|████      | 14/35 [05:39<01:23,  4.00s/it]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [05:39<00:59,  2.98s/it]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [05:40<00:43,  2.27s/it]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [05:40<00:31,  1.78s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [05:41<00:24,  1.43s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [05:42<00:19,  1.19s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [05:42<00:15,  1.02s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [05:43<00:12,  1.11it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [05:44<00:10,  1.22it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [05:44<00:09,  1.32it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [05:45<00:07,  1.39it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [05:45<00:06,  1.45it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [05:46<00:06,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [05:47<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [05:47<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [05:48<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [05:49<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [05:49<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [05:50<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [05:50<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [05:51<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [05:52<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [05:52<00:00, 10.06s/it]
INFO 05-12 20:06:43 [model_runner.py:1598] Graph capturing finished in 352 secs, took 0.06 GiB
INFO 05-12 20:06:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 353.60 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.73 toks/s, output: 101.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.73 toks/s, output: 101.96 toks/s]
[winogrande thr=0.45 | sample 59] energy=44357.63 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:06:45 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:06:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:06:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:06:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.06s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.29s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.18s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.64s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.68s/it]

INFO 05-12 20:06:58 [loader.py:458] Loading weights took 10.91 seconds
INFO 05-12 20:06:58 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.248686 seconds
INFO 05-12 20:06:59 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:06:59 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:06:59 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:06:59 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:06:59 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:06:59 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 20:07:21 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:07:21 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.25 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.66 toks/s, output: 102.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.66 toks/s, output: 102.21 toks/s]
[winogrande thr=0.45 | sample 60] energy=5119.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:07:23 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:07:23 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:07:25 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:07:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:07:35 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:07:35 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.264500 seconds
INFO 05-12 20:07:36 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:07:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:07:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:07:36 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:07:36 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:07:36 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:07:59 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:07:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.28 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.63it/s, est. speed input: 356.39 toks/s, output: 86.67 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.55it/s, est. speed input: 356.39 toks/s, output: 86.67 toks/s]
[winogrande thr=0.45 | sample 61] energy=4681.76 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:08:00 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:08:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:08:01 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:08:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:08:11 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 20:08:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.282409 seconds
INFO 05-12 20:08:12 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:08:12 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:08:12 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:08:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:08:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:08:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 20:08:36 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:08:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.81 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.66 toks/s, output: 102.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.66 toks/s, output: 102.07 toks/s]
[winogrande thr=0.45 | sample 62] energy=5057.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:08:38 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:08:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:08:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:08:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.87s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.10s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 20:08:50 [loader.py:458] Loading weights took 10.22 seconds
INFO 05-12 20:08:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.546567 seconds
INFO 05-12 20:08:51 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:08:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:08:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:08:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:08:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:08:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 20:09:14 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:09:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.71 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.64 toks/s, output: 102.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.64 toks/s, output: 102.01 toks/s]
[winogrande thr=0.45 | sample 63] energy=5110.85 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:09:16 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:09:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:09:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:09:17 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:09:27 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 20:09:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.246304 seconds
INFO 05-12 20:09:28 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:09:28 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:09:28 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:09:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:09:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:09:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:09:51 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:09:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.34 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.74 toks/s, output: 101.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.74 toks/s, output: 101.71 toks/s]
[winogrande thr=0.45 | sample 64] energy=5009.04 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:09:53 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:09:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:09:54 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:09:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.07s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.48s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.94s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.92s/it]

INFO 05-12 20:10:07 [loader.py:458] Loading weights took 11.91 seconds
INFO 05-12 20:10:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.243249 seconds
INFO 05-12 20:10:08 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:10:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:10:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:10:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:10:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:10:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.46it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 20:10:31 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:10:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.58 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.87 toks/s, output: 102.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.87 toks/s, output: 102.17 toks/s]
[winogrande thr=0.45 | sample 65] energy=5276.23 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:10:33 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:10:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:10:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:10:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.87s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.86s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.74s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.93s/it]

INFO 05-12 20:10:47 [loader.py:458] Loading weights took 11.94 seconds
INFO 05-12 20:10:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.271578 seconds
INFO 05-12 20:10:48 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:10:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:10:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:10:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:10:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:10:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:11:11 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:11:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.42 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.62 toks/s, output: 102.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.62 toks/s, output: 102.01 toks/s]
[winogrande thr=0.45 | sample 66] energy=5365.15 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:11:13 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:11:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:11:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:11:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:11:25 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:11:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.259065 seconds
INFO 05-12 20:11:26 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:11:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:11:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:11:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:11:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:11:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:11:48 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:11:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.67 toks/s, output: 102.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.67 toks/s, output: 102.16 toks/s]
[winogrande thr=0.45 | sample 67] energy=5007.38 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:11:50 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:11:50 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:11:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:11:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:12:02 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 20:12:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.256102 seconds
INFO 05-12 20:12:03 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:12:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:12:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:12:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:12:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:12:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:12:26 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:12:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.43 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.77 toks/s, output: 102.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.77 toks/s, output: 102.02 toks/s]
[winogrande thr=0.45 | sample 68] energy=5008.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:12:28 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:12:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:12:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:12:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 20:12:39 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 20:12:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.249700 seconds
INFO 05-12 20:12:40 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:12:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:12:40 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:12:41 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:12:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:12:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:13:03 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:13:03 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.33 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.55 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.55 toks/s, output: 101.88 toks/s]
[winogrande thr=0.45 | sample 69] energy=5004.32 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:13:05 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:13:05 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:13:06 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:13:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:13:17 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:13:17 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.277838 seconds
INFO 05-12 20:13:18 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:13:18 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:13:18 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:13:18 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:13:18 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:13:18 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:13:40 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:13:40 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.73 toks/s, output: 102.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.73 toks/s, output: 102.02 toks/s]
[winogrande thr=0.45 | sample 70] energy=5001.23 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:13:43 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:13:43 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:13:44 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:13:44 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:13:54 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 20:13:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.248513 seconds
INFO 05-12 20:13:55 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:13:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:13:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:13:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:13:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:13:56 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:14:18 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:14:18 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.36 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.78 toks/s, output: 102.10 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.78 toks/s, output: 102.10 toks/s]
[winogrande thr=0.45 | sample 71] energy=5010.37 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:14:20 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:14:20 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:14:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:14:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 20:14:32 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 20:14:32 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.477847 seconds
INFO 05-12 20:14:33 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 20:14:33 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:14:33 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:14:33 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:14:33 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:14:33 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:14:55 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:14:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.37 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.82 toks/s, output: 101.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.82 toks/s, output: 101.99 toks/s]
[winogrande thr=0.45 | sample 72] energy=5016.25 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:14:57 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:14:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:14:59 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:14:59 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.89s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.48s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.54s/it]

INFO 05-12 20:15:09 [loader.py:458] Loading weights took 10.39 seconds
INFO 05-12 20:15:10 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.732533 seconds
INFO 05-12 20:15:11 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:15:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:15:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:15:11 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:15:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:15:11 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:15:33 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:15:33 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.31 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.74 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.59 toks/s, output: 101.74 toks/s]
[winogrande thr=0.45 | sample 73] energy=5061.72 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:15:35 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:15:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:15:37 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:15:37 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.12s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.33s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.20s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.68s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.72s/it]

INFO 05-12 20:15:48 [loader.py:458] Loading weights took 11.06 seconds
INFO 05-12 20:15:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.392637 seconds
INFO 05-12 20:15:49 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:15:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:15:49 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:15:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:15:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:15:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.46it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:16:12 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:16:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.30 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.73 toks/s, output: 102.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.73 toks/s, output: 102.15 toks/s]
[winogrande thr=0.45 | sample 74] energy=5125.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:16:14 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:16:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:16:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:16:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.20s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.67s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.67s/it]

INFO 05-12 20:16:26 [loader.py:458] Loading weights took 10.90 seconds
INFO 05-12 20:16:27 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.246843 seconds
INFO 05-12 20:16:27 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:16:27 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:16:27 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:16:27 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:16:27 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:16:28 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:16:50 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:16:50 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.48 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.82 toks/s, output: 102.17 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.82 toks/s, output: 102.17 toks/s]
[winogrande thr=0.45 | sample 75] energy=5125.36 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:16:53 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:16:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:16:54 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:16:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.07s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.07s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.59s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.88s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.07s/it]

INFO 05-12 20:17:07 [loader.py:458] Loading weights took 12.50 seconds
INFO 05-12 20:17:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.839787 seconds
INFO 05-12 20:17:08 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:17:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:17:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:17:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:17:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:17:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:19,  1.45it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.47it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.49it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.49it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.50it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:17:31 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:17:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.86 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.63 toks/s, output: 101.97 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.63 toks/s, output: 101.97 toks/s]
[winogrande thr=0.45 | sample 76] energy=5475.29 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.2-1B-Instruct
INFO 05-12 20:17:33 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:17:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:17:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...
INFO 05-12 20:17:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 20:17:35 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]

INFO 05-12 20:17:37 [loader.py:458] Loading weights took 1.60 seconds
INFO 05-12 20:17:37 [model_runner.py:1146] Model loading took 2.3029 GiB and 1.893590 seconds
INFO 05-12 20:17:38 [worker.py:267] Memory profiling takes 0.35 seconds
INFO 05-12 20:17:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:17:38 [worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.17GiB; the rest of the memory reserved for KV Cache is 80.24GiB.
INFO 05-12 20:17:38 [executor_base.py:112] # cuda blocks: 164331, # CPU blocks: 8192
INFO 05-12 20:17:38 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 1283.84x
INFO 05-12 20:17:38 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.63it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.64it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:17,  1.62it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.63it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.64it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.63it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.62it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.63it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.63it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:12,  1.63it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.63it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.63it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.64it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.64it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.64it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.64it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:08,  1.64it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:07,  1.64it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.64it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.64it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:15<00:05,  1.63it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.62it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.63it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:17<00:03,  1.64it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.63it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:19<00:01,  1.63it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.64it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:20<00:00,  1.65it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.66it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.63it/s]
INFO 05-12 20:17:59 [model_runner.py:1598] Graph capturing finished in 21 secs, took 0.03 GiB
INFO 05-12 20:17:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.49 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s, est. speed input: 98.00 toks/s, output: 316.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.15it/s, est. speed input: 98.00 toks/s, output: 316.11 toks/s]
[winogrande thr=0.45 | sample 77] energy=3599.05 J, correct=False, model=meta-llama/Llama-3.2-1B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:18:03 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:18:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:18:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:18:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 20:18:15 [loader.py:458] Loading weights took 9.98 seconds
INFO 05-12 20:18:16 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.310501 seconds
INFO 05-12 20:18:16 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:18:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:18:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:18:17 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:18:17 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:18:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:18:39 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:18:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.42 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.66 toks/s, output: 101.94 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.66 toks/s, output: 101.94 toks/s]
[winogrande thr=0.45 | sample 78] energy=5136.14 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:18:41 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:18:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:18:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:18:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:18:53 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 20:18:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.256702 seconds
INFO 05-12 20:18:54 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 20:18:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:18:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:18:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:18:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:18:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:19:17 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:19:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.31 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.66 toks/s, output: 101.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.66 toks/s, output: 101.93 toks/s]
[winogrande thr=0.45 | sample 79] energy=5048.59 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:19:19 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:19:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:19:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:19:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 20:19:30 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 20:19:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.301947 seconds
INFO 05-12 20:19:31 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:19:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:19:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:19:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:19:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:19:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:19:54 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:19:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.53 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.75 toks/s, output: 102.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.75 toks/s, output: 102.21 toks/s]
[winogrande thr=0.45 | sample 80] energy=5023.29 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:19:56 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:19:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:19:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:19:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]

INFO 05-12 20:20:08 [loader.py:458] Loading weights took 10.32 seconds
INFO 05-12 20:20:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.665398 seconds
INFO 05-12 20:20:09 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:20:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:20:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:20:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:20:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:20:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:20:32 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:20:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.35 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.65 toks/s, output: 102.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.65 toks/s, output: 102.15 toks/s]
[winogrande thr=0.45 | sample 81] energy=5060.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:20:34 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:20:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:20:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:20:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.03s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.22s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.74s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.41s/it]

INFO 05-12 20:20:50 [loader.py:458] Loading weights took 13.83 seconds
INFO 05-12 20:20:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 14.165132 seconds
INFO 05-12 20:20:51 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:20:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:20:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:20:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:20:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:20:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 20:21:14 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:21:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.20 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s, est. speed input: 223.16 toks/s, output: 91.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s, est. speed input: 223.16 toks/s, output: 91.88 toks/s]
[winogrande thr=0.45 | sample 82] energy=5211.50 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:21:15 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:21:15 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:21:16 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:21:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.87s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 20:21:27 [loader.py:458] Loading weights took 10.19 seconds
INFO 05-12 20:21:27 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.650786 seconds
INFO 05-12 20:21:28 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:21:28 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:21:28 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:21:28 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:21:28 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:21:28 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:21:50 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:21:50 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.38 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.86 toks/s, output: 102.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.86 toks/s, output: 102.26 toks/s]
[winogrande thr=0.45 | sample 83] energy=5045.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:21:52 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:21:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:21:54 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:21:54 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:22:04 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:22:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.259377 seconds
INFO 05-12 20:22:05 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:22:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:22:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:22:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:22:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:22:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:22:28 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:22:28 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.45 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.84 toks/s, output: 102.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.84 toks/s, output: 102.26 toks/s]
[winogrande thr=0.45 | sample 84] energy=4998.79 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:22:30 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:22:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:22:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:22:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:22:42 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:22:42 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.486943 seconds
INFO 05-12 20:22:43 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 20:22:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:22:43 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:22:43 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:22:43 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:22:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 20:23:05 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:23:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.31 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.73 toks/s, output: 102.27 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.73 toks/s, output: 102.27 toks/s]
[winogrande thr=0.45 | sample 85] energy=5016.59 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:23:07 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:23:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:23:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:23:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:23:19 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:23:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.266287 seconds
INFO 05-12 20:23:20 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:23:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:23:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:23:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:23:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:23:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:23:43 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:23:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.30 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.84 toks/s, output: 102.27 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.84 toks/s, output: 102.27 toks/s]
[winogrande thr=0.45 | sample 86] energy=5002.32 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:23:45 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:23:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:23:46 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:23:46 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 20:23:56 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 20:23:57 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.241486 seconds
INFO 05-12 20:23:57 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 20:23:57 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:23:57 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:23:58 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:23:58 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:23:58 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 20:24:20 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:24:20 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.23 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.88 toks/s, output: 102.25 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.88 toks/s, output: 102.25 toks/s]
[winogrande thr=0.45 | sample 87] energy=4990.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:24:22 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:24:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:24:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:24:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 20:24:33 [loader.py:458] Loading weights took 9.87 seconds
INFO 05-12 20:24:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.199734 seconds
INFO 05-12 20:24:34 [worker.py:267] Memory profiling takes 0.43 seconds
INFO 05-12 20:24:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:24:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:24:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:24:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:24:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 20:24:57 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:24:57 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.11 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.74 toks/s, output: 102.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.74 toks/s, output: 102.18 toks/s]
[winogrande thr=0.45 | sample 88] energy=4975.54 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:24:59 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:24:59 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:25:00 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:25:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.98s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.39s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.26s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.82s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.81s/it]

INFO 05-12 20:25:12 [loader.py:458] Loading weights took 11.49 seconds
INFO 05-12 20:25:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.814959 seconds
INFO 05-12 20:25:13 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:25:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:25:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:25:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:25:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:25:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 20:25:36 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:25:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.78 toks/s, output: 102.29 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.78 toks/s, output: 102.29 toks/s]
[winogrande thr=0.45 | sample 89] energy=5171.52 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:25:38 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:25:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:25:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:25:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.45s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.48s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.29s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.50s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.66s/it]

INFO 05-12 20:25:50 [loader.py:458] Loading weights took 10.90 seconds
INFO 05-12 20:25:50 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.227032 seconds
INFO 05-12 20:25:51 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:25:51 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:25:51 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:25:51 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:25:51 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:25:51 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 20:26:14 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:26:14 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.19 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.85 toks/s, output: 102.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.85 toks/s, output: 102.18 toks/s]
[winogrande thr=0.45 | sample 90] energy=5095.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:26:16 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:26:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:26:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:26:17 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 20:31:35 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 278.897183 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.89s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.06s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.47s/it]

INFO 05-12 20:31:45 [loader.py:458] Loading weights took 10.12 seconds
INFO 05-12 20:31:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 327.847331 seconds
INFO 05-12 20:31:46 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:31:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:31:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:31:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:31:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:31:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.48it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.49it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.49it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 20:32:09 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:32:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.62 toks/s, output: 102.01 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.62 toks/s, output: 102.01 toks/s]
[winogrande thr=0.45 | sample 91] energy=42883.23 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:32:12 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:32:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:32:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:32:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.46s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.65s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.06s/it]

INFO 05-12 20:32:26 [loader.py:458] Loading weights took 12.41 seconds
INFO 05-12 20:32:26 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.784943 seconds
INFO 05-12 20:32:27 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:32:27 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:32:27 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:32:27 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:32:27 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:32:27 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:32:50 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:32:50 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.53 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.72 toks/s, output: 101.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.72 toks/s, output: 101.95 toks/s]
[winogrande thr=0.45 | sample 92] energy=5382.36 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:32:52 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:32:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:32:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:32:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:33:04 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 20:33:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.350926 seconds
INFO 05-12 20:33:05 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:33:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:33:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:33:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:33:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:33:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:33:28 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:33:28 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.50 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.64 toks/s, output: 101.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.64 toks/s, output: 101.73 toks/s]
[winogrande thr=0.45 | sample 93] energy=5042.66 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:33:30 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:33:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:33:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:33:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 20:33:42 [loader.py:458] Loading weights took 9.97 seconds
INFO 05-12 20:33:42 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.603835 seconds
INFO 05-12 20:33:43 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:33:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:33:43 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:33:43 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:33:43 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:33:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 20:34:06 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:34:06 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.63 toks/s, output: 102.04 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.63 toks/s, output: 102.04 toks/s]
[winogrande thr=0.45 | sample 94] energy=5096.04 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:34:08 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:34:08 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:34:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:34:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:34:19 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:34:20 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.282225 seconds
INFO 05-12 20:34:20 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:34:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:34:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:34:21 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:34:21 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:34:21 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:34:43 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:34:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.57 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.59 toks/s, output: 102.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.59 toks/s, output: 102.03 toks/s]
[winogrande thr=0.45 | sample 95] energy=5040.13 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:34:45 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:34:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:34:48 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:34:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:34:58 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 20:34:59 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.269054 seconds
INFO 05-12 20:34:59 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:34:59 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:34:59 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:35:00 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:35:00 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:35:00 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:35:22 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:35:22 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.68 toks/s, output: 101.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.68 toks/s, output: 101.98 toks/s]
[winogrande thr=0.45 | sample 96] energy=5197.69 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:35:24 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:35:24 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:35:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:35:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:35:36 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:35:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.273565 seconds
INFO 05-12 20:35:37 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:35:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:35:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:35:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:35:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:35:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.46it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.47it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.49it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 20:36:00 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:36:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.04 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.66 toks/s, output: 102.06 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.66 toks/s, output: 102.06 toks/s]
[winogrande thr=0.45 | sample 97] energy=5104.36 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:36:02 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:36:02 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:36:04 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:36:04 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:36:14 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 20:36:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.230795 seconds
INFO 05-12 20:36:15 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:36:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:36:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:36:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:36:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:36:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:36:38 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:36:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.51 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.13 toks/s]
[winogrande thr=0.45 | sample 98] energy=5050.17 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:36:40 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:36:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:36:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:36:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 20:36:52 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 20:36:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.227097 seconds
INFO 05-12 20:36:53 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:36:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:36:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:36:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:36:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:36:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:37:16 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:37:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.55 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.85 toks/s, output: 102.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.85 toks/s, output: 102.23 toks/s]
[winogrande thr=0.45 | sample 99] energy=5084.75 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:37:18 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:37:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:37:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:37:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]

INFO 05-12 20:37:30 [loader.py:458] Loading weights took 10.59 seconds
INFO 05-12 20:37:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.926484 seconds
INFO 05-12 20:37:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:37:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:37:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:37:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:37:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:37:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:37:54 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:37:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.38 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.58 toks/s, output: 101.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.58 toks/s, output: 101.99 toks/s]
[winogrande thr=0.45 | sample 100] energy=5081.08 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:37:57 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:37:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:37:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:37:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.05s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.58s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.86s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.05s/it]

INFO 05-12 20:38:11 [loader.py:458] Loading weights took 12.42 seconds
INFO 05-12 20:38:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.806356 seconds
INFO 05-12 20:38:12 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:38:12 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:38:12 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:38:12 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:38:12 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:38:12 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 20:38:34 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:38:34 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.36 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.66 toks/s, output: 102.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.66 toks/s, output: 102.20 toks/s]
[winogrande thr=0.45 | sample 101] energy=5379.76 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:38:37 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:38:37 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:38:38 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:38:38 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:38:48 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 20:38:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.240757 seconds
INFO 05-12 20:38:49 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 20:38:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:38:49 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:38:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:38:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:38:50 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:39:12 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:39:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.49 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.96 toks/s, output: 102.24 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.96 toks/s, output: 102.24 toks/s]
[winogrande thr=0.45 | sample 102] energy=5037.46 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:39:14 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:39:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:39:15 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:39:15 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:39:26 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 20:39:26 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.247019 seconds
INFO 05-12 20:39:27 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:39:27 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:39:27 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:39:27 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:39:27 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:39:27 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:39:49 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:39:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.43 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.70 toks/s, output: 102.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.70 toks/s, output: 102.12 toks/s]
[winogrande thr=0.45 | sample 103] energy=4998.85 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:39:51 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:39:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:39:53 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:39:53 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 20:40:03 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 20:40:03 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.226739 seconds
INFO 05-12 20:40:04 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:40:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:40:04 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:40:04 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:40:04 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:40:04 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:40:27 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:40:27 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.51 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.80 toks/s, output: 102.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.80 toks/s, output: 102.21 toks/s]
[winogrande thr=0.45 | sample 104] energy=5033.52 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:40:29 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:40:29 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:40:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:40:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:40:40 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 20:40:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.251082 seconds
INFO 05-12 20:40:41 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:40:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:40:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:40:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:40:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:40:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:41:04 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:41:04 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.53 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.63 toks/s, output: 101.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.63 toks/s, output: 101.90 toks/s]
[winogrande thr=0.45 | sample 105] energy=5019.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:41:07 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:41:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:41:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:41:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.45s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.64s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.45s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.05s/it]

INFO 05-12 20:41:21 [loader.py:458] Loading weights took 12.34 seconds
INFO 05-12 20:41:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.775961 seconds
INFO 05-12 20:41:23 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 20:41:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:41:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:41:23 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:41:23 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:41:23 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:24,  1.39it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 20:41:46 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:41:46 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.97 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.67 toks/s, output: 101.90 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.67 toks/s, output: 101.90 toks/s]
[winogrande thr=0.45 | sample 106] energy=5504.12 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:41:48 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:41:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:41:49 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:41:49 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]

INFO 05-12 20:42:00 [loader.py:458] Loading weights took 10.07 seconds
INFO 05-12 20:42:00 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.403076 seconds
INFO 05-12 20:42:01 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 20:42:01 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:42:01 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:42:01 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:42:01 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:42:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.49it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.48it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.49it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 20:42:24 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:42:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.77 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.85 toks/s, output: 102.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.85 toks/s, output: 102.07 toks/s]
[winogrande thr=0.45 | sample 107] energy=5089.82 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:42:26 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:42:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:42:27 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:42:27 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:42:37 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 20:42:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.236688 seconds
INFO 05-12 20:42:39 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:42:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:42:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:42:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:42:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:42:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:43:01 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:43:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.55 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.96 toks/s, output: 102.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 43.96 toks/s, output: 102.23 toks/s]
[winogrande thr=0.45 | sample 108] energy=5026.18 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:43:03 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:43:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:43:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:43:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:43:15 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 20:43:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.270661 seconds
INFO 05-12 20:43:16 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 20:43:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:43:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:43:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:43:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:43:16 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 20:43:39 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:43:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.23 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.79 toks/s, output: 102.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.79 toks/s, output: 102.33 toks/s]
[winogrande thr=0.45 | sample 109] energy=5004.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:43:41 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:43:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:43:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:43:42 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:43:52 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 20:43:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.235405 seconds
INFO 05-12 20:43:53 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:43:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:43:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:43:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:43:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:43:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 20:44:16 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:44:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.12 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.71 toks/s, output: 102.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.71 toks/s, output: 102.09 toks/s]
[winogrande thr=0.45 | sample 110] energy=4950.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:44:18 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:44:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:44:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:44:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.88s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.10s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.53s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.56s/it]

INFO 05-12 20:44:30 [loader.py:458] Loading weights took 10.47 seconds
INFO 05-12 20:44:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.806393 seconds
INFO 05-12 20:44:31 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 20:44:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:44:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:44:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:44:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:44:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 20:44:54 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:44:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.70 toks/s, output: 102.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.70 toks/s, output: 102.11 toks/s]
[winogrande thr=0.45 | sample 111] energy=5157.89 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:44:56 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:44:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:44:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:44:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.69s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.70s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.38s/it]

INFO 05-12 20:45:12 [loader.py:458] Loading weights took 13.76 seconds
INFO 05-12 20:45:12 [model_runner.py:1146] Model loading took 14.9576 GiB and 14.094272 seconds
INFO 05-12 20:45:13 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:45:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:45:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:45:13 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:45:13 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:45:13 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 20:45:36 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:45:36 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.76 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.70 toks/s, output: 102.27 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.70 toks/s, output: 102.27 toks/s]
[winogrande thr=0.45 | sample 112] energy=5541.59 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:45:38 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:45:38 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:45:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:45:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.55s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.71s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.79s/it]

INFO 05-12 20:45:51 [loader.py:458] Loading weights took 11.40 seconds
INFO 05-12 20:45:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.736473 seconds
INFO 05-12 20:45:52 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:45:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:45:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:45:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:45:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:45:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 20:46:15 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:46:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.81 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.78 toks/s, output: 102.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.78 toks/s, output: 102.22 toks/s]
[winogrande thr=0.45 | sample 113] energy=5226.75 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:46:17 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:46:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:46:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:46:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 20:46:29 [loader.py:458] Loading weights took 9.97 seconds
INFO 05-12 20:46:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.304126 seconds
INFO 05-12 20:46:30 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:46:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:46:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:46:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:46:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:46:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.13it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:38<00:00,  5.39s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [01:41<00:00,  2.91s/it]
INFO 05-12 20:52:30 [model_runner.py:1598] Graph capturing finished in 359 secs, took 0.06 GiB
INFO 05-12 20:53:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 445.81 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.50 toks/s, output: 101.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.04s/it, est. speed input: 31.50 toks/s, output: 101.61 toks/s]
[winogrande thr=0.45 | sample 114] energy=61758.06 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:54:57 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:54:57 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:54:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:54:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.20s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.38s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.23s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.72s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.76s/it]

INFO 05-12 20:55:10 [loader.py:458] Loading weights took 11.23 seconds
INFO 05-12 20:55:10 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.572351 seconds
INFO 05-12 20:55:11 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:55:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:55:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:55:11 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:55:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:55:11 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 20:55:34 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:55:34 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.63 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.65 toks/s, output: 102.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.65 toks/s, output: 102.09 toks/s]
[winogrande thr=0.45 | sample 115] energy=5796.03 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.2-1B-Instruct
INFO 05-12 20:55:36 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:55:36 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:55:37 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...
INFO 05-12 20:55:37 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 20:55:38 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]

INFO 05-12 20:55:39 [loader.py:458] Loading weights took 1.60 seconds
INFO 05-12 20:55:40 [model_runner.py:1146] Model loading took 2.3029 GiB and 2.056451 seconds
INFO 05-12 20:55:40 [worker.py:267] Memory profiling takes 0.34 seconds
INFO 05-12 20:55:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:55:40 [worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.17GiB; the rest of the memory reserved for KV Cache is 80.24GiB.
INFO 05-12 20:55:41 [executor_base.py:112] # cuda blocks: 164331, # CPU blocks: 8192
INFO 05-12 20:55:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 1283.84x
INFO 05-12 20:55:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.61it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.62it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.63it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:12,  1.63it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.63it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.64it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:10,  1.64it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.64it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.65it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.64it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:08,  1.64it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:07,  1.65it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.64it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.65it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.62it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.62it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.63it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:17<00:03,  1.63it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.62it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:19<00:01,  1.63it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.64it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:20<00:00,  1.65it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.65it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.63it/s]
INFO 05-12 20:56:02 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.03 GiB
INFO 05-12 20:56:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.54 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.15it/s, est. speed input: 116.62 toks/s, output: 315.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.14it/s, est. speed input: 116.62 toks/s, output: 315.18 toks/s]
[winogrande thr=0.45 | sample 116] energy=3625.30 J, correct=False, model=meta-llama/Llama-3.2-1B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:56:06 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:56:06 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:56:07 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:56:07 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.83s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 20:56:18 [loader.py:458] Loading weights took 9.98 seconds
INFO 05-12 20:56:18 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.300991 seconds
INFO 05-12 20:56:19 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:56:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:56:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:56:19 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:56:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:56:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 20:56:41 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 20:56:41 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.44 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.79 toks/s, output: 102.08 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.79 toks/s, output: 102.08 toks/s]
[winogrande thr=0.45 | sample 117] energy=5106.28 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:56:43 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:56:43 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:56:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:56:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:56:55 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:56:55 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.257937 seconds
INFO 05-12 20:56:56 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 20:56:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:56:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:56:56 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:56:56 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:56:56 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 20:57:19 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:57:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.62 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.69 toks/s, output: 101.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.69 toks/s, output: 101.82 toks/s]
[winogrande thr=0.45 | sample 118] energy=5036.09 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:57:21 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:57:21 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:57:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:57:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:57:33 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 20:57:33 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.285443 seconds
INFO 05-12 20:57:34 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 20:57:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:57:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:57:34 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:57:34 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:57:34 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 20:57:57 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:57:57 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.71 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.12 toks/s]
[winogrande thr=0.45 | sample 119] energy=5051.32 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:57:59 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:57:59 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:58:00 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:58:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 20:58:11 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 20:58:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.275792 seconds
INFO 05-12 20:58:12 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 20:58:12 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:58:12 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:58:12 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:58:12 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:58:12 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 20:58:35 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:58:35 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.66 toks/s, output: 102.06 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.66 toks/s, output: 102.06 toks/s]
[winogrande thr=0.45 | sample 120] energy=5050.61 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:58:37 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:58:37 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:58:38 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:58:38 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 20:58:48 [loader.py:458] Loading weights took 9.96 seconds
INFO 05-12 20:58:49 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.478886 seconds
INFO 05-12 20:58:49 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 20:58:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:58:49 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:58:50 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:58:50 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:58:50 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 20:59:12 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:59:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.60 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.91 toks/s, output: 102.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.91 toks/s, output: 102.22 toks/s]
[winogrande thr=0.45 | sample 121] energy=5052.98 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:59:14 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:59:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:59:16 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:59:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.34s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.73s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.10s/it]

INFO 05-12 20:59:29 [loader.py:458] Loading weights took 12.66 seconds
INFO 05-12 20:59:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.994086 seconds
INFO 05-12 20:59:30 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 20:59:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 20:59:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 20:59:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 20:59:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 20:59:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 20:59:53 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 20:59:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.60 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.88 toks/s, output: 102.08 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.88 toks/s, output: 102.08 toks/s]
[winogrande thr=0.45 | sample 122] energy=5361.96 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 20:59:55 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 20:59:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 20:59:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 20:59:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.63s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.65s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.60s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.77s/it]

INFO 05-12 21:00:08 [loader.py:458] Loading weights took 11.33 seconds
INFO 05-12 21:00:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.714507 seconds
INFO 05-12 21:00:09 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:00:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:00:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:00:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:00:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:00:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 21:00:32 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:00:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.52 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.80 toks/s, output: 101.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 41.80 toks/s, output: 101.95 toks/s]
[winogrande thr=0.45 | sample 123] energy=5277.90 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:00:34 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:00:34 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:00:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:00:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:00:46 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 21:00:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.247350 seconds
INFO 05-12 21:00:47 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 21:00:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:00:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:00:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:00:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:00:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 21:01:10 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:01:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.39 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.14 toks/s]
[winogrande thr=0.45 | sample 124] energy=5019.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:01:12 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:01:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:01:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:01:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:01:24 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 21:01:24 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.454312 seconds
INFO 05-12 21:01:25 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:01:25 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:01:25 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:01:25 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:01:25 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:01:25 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 21:01:47 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:01:47 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.14 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.60it/s, est. speed input: 204.70 toks/s, output: 92.43 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s, est. speed input: 204.70 toks/s, output: 92.43 toks/s]
[winogrande thr=0.45 | sample 125] energy=4716.70 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:01:48 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:01:48 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:01:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:01:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.92s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.16s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.57s/it]

INFO 05-12 21:02:01 [loader.py:458] Loading weights took 10.48 seconds
INFO 05-12 21:02:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.810068 seconds
INFO 05-12 21:02:02 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 21:02:02 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:02:02 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:02:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:02:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:02:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.59it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.59it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.59it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 21:02:24 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:02:24 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.15 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.64 toks/s, output: 102.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.64 toks/s, output: 102.07 toks/s]
[winogrande thr=0.45 | sample 126] energy=5019.78 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:02:26 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:02:26 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:02:27 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:02:28 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 21:02:38 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 21:02:38 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.316944 seconds
INFO 05-12 21:02:39 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:02:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:02:39 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:02:39 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:02:39 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:02:39 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 21:03:01 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:03:01 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.32 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.90 toks/s, output: 102.05 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.90 toks/s, output: 102.05 toks/s]
[winogrande thr=0.45 | sample 127] energy=4992.78 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:03:03 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:03:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:03:05 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:03:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:03:15 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 21:03:15 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.277460 seconds
INFO 05-12 21:03:16 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:03:16 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:03:16 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:03:16 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:03:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:03:17 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:03:39 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:03:39 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.92 toks/s, output: 102.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 42.92 toks/s, output: 102.18 toks/s]
[winogrande thr=0.45 | sample 128] energy=5041.68 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:03:41 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:03:41 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:03:42 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:03:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 21:03:53 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 21:03:53 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.403568 seconds
INFO 05-12 21:03:54 [worker.py:267] Memory profiling takes 0.44 seconds
INFO 05-12 21:03:54 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:03:54 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:03:54 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:03:54 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:03:54 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:04:17 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:04:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.67 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.82 toks/s, output: 102.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.82 toks/s, output: 102.21 toks/s]
[winogrande thr=0.45 | sample 129] energy=5064.78 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:04:19 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:04:19 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:04:20 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:04:20 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:04:30 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 21:04:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.252269 seconds
INFO 05-12 21:04:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:04:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:04:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:04:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:04:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:04:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 21:04:54 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:04:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.60 toks/s, output: 101.93 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.60 toks/s, output: 101.93 toks/s]
[winogrande thr=0.45 | sample 130] energy=5022.95 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:04:56 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:04:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:04:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:04:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 21:05:08 [loader.py:458] Loading weights took 9.88 seconds
INFO 05-12 21:05:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.214844 seconds
INFO 05-12 21:05:09 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:05:09 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:05:09 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:05:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:05:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:05:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.59it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.59it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 21:05:31 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:05:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.18 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.76 toks/s, output: 102.24 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.76 toks/s, output: 102.24 toks/s]
[winogrande thr=0.45 | sample 131] energy=4949.57 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:05:33 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:05:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:05:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:05:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.19s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.62s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.38s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.97s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.97s/it]

INFO 05-12 21:05:47 [loader.py:458] Loading weights took 12.13 seconds
INFO 05-12 21:05:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.454117 seconds
INFO 05-12 21:05:48 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:05:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:05:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:05:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:05:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:05:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.59it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.59it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.60it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.59it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 21:06:11 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:06:11 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.08 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.00 toks/s, output: 102.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 44.00 toks/s, output: 102.33 toks/s]
[winogrande thr=0.45 | sample 132] energy=5246.48 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:06:13 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:06:13 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:06:14 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:06:14 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.33s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.21s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.48s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.60s/it]

INFO 05-12 21:06:25 [loader.py:458] Loading weights took 10.66 seconds
INFO 05-12 21:06:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.992674 seconds
INFO 05-12 21:06:26 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 21:06:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:06:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:06:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:06:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:06:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.58it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.60it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.60it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:05,  1.59it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.60it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.59it/s]
INFO 05-12 21:06:48 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:06:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.04 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.86 toks/s, output: 102.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.86 toks/s, output: 102.32 toks/s]
[winogrande thr=0.45 | sample 133] energy=5052.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:06:50 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:06:50 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:06:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:06:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:07:02 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 21:07:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.239756 seconds
INFO 05-12 21:07:03 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:07:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:07:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:07:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:07:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:07:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.59it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.61it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.61it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.60it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.60it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.60it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.59it/s]
INFO 05-12 21:07:25 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:07:25 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.08 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.79 toks/s, output: 102.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.79 toks/s, output: 102.26 toks/s]
[winogrande thr=0.45 | sample 134] energy=4949.01 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:07:27 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:07:27 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:07:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:07:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.98s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:07:39 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 21:07:39 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.260568 seconds
INFO 05-12 21:07:40 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:07:40 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:07:40 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:07:40 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:07:40 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:07:40 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.59it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.59it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 21:08:03 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:08:03 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.63 toks/s, output: 101.86 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.63 toks/s, output: 101.86 toks/s]
[winogrande thr=0.45 | sample 135] energy=4995.80 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:08:05 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:08:05 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:08:06 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:08:06 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 21:08:16 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 21:08:17 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.224954 seconds
INFO 05-12 21:08:17 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:08:17 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:08:17 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:08:18 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:08:18 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:08:18 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.59it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]
INFO 05-12 21:08:40 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:08:40 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.32 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.76 toks/s, output: 102.30 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.76 toks/s, output: 102.30 toks/s]
[winogrande thr=0.45 | sample 136] energy=4982.08 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:08:42 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:08:42 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:08:43 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:08:43 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 21:08:54 [loader.py:458] Loading weights took 9.96 seconds
INFO 05-12 21:08:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.296259 seconds
INFO 05-12 21:08:55 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:08:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:08:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:08:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:08:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:08:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]
INFO 05-12 21:09:17 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:09:17 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.13 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.68 toks/s, output: 102.13 toks/s]
[winogrande thr=0.45 | sample 137] energy=5020.23 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:09:20 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:09:20 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:09:21 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:09:21 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.10s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.09s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.45s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.50s/it]

INFO 05-12 21:09:32 [loader.py:458] Loading weights took 10.25 seconds
INFO 05-12 21:09:32 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.671636 seconds
INFO 05-12 21:09:33 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:09:33 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:09:33 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:09:33 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:09:33 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:09:33 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 21:09:55 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:09:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.47 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.63 toks/s, output: 101.98 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.63 toks/s, output: 101.98 toks/s]
[winogrande thr=0.45 | sample 138] energy=5087.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:09:58 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:09:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:10:00 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:10:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.34s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.59s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.00s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.01s/it]

INFO 05-12 21:10:12 [loader.py:458] Loading weights took 12.28 seconds
INFO 05-12 21:10:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.766463 seconds
INFO 05-12 21:10:14 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:10:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:10:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:10:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:10:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:10:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:10:37 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:10:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.69 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.71 toks/s, output: 102.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.71 toks/s, output: 102.09 toks/s]
[winogrande thr=0.45 | sample 139] energy=5463.52 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:10:39 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:10:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:10:40 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:10:40 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.94s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.19s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.54s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.58s/it]

INFO 05-12 21:10:51 [loader.py:458] Loading weights took 10.55 seconds
INFO 05-12 21:10:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.891025 seconds
INFO 05-12 21:10:52 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:10:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:10:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:10:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:10:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:10:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:11:15 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:11:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.63 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.71 toks/s, output: 102.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.71 toks/s, output: 102.09 toks/s]
[winogrande thr=0.45 | sample 140] energy=5097.05 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:11:17 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:11:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:11:18 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:11:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 21:11:28 [loader.py:458] Loading weights took 9.89 seconds
INFO 05-12 21:11:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.224943 seconds
INFO 05-12 21:11:29 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:11:29 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:11:29 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:11:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:11:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:11:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:11:52 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:11:52 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.65 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.64 toks/s, output: 102.15 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.64 toks/s, output: 102.15 toks/s]
[winogrande thr=0.45 | sample 141] energy=5025.99 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:11:54 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:11:54 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:11:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:11:56 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 21:12:07 [loader.py:458] Loading weights took 9.88 seconds
INFO 05-12 21:12:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.208371 seconds
INFO 05-12 21:12:08 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:12:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:12:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:12:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:12:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:12:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:12:31 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:12:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.69 toks/s, output: 102.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.69 toks/s, output: 102.14 toks/s]
[winogrande thr=0.45 | sample 142] energy=5105.96 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:12:33 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:12:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:12:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:12:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.80s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.02s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.45s/it]

INFO 05-12 21:12:44 [loader.py:458] Loading weights took 10.03 seconds
INFO 05-12 21:12:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.368700 seconds
INFO 05-12 21:12:45 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:12:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:12:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:12:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:12:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:12:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 21:13:08 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:13:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.52 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.63 toks/s, output: 102.08 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.63 toks/s, output: 102.08 toks/s]
[winogrande thr=0.45 | sample 143] energy=5045.03 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:13:10 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:13:10 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:13:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:13:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.11s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.60s/it]

INFO 05-12 21:13:23 [loader.py:458] Loading weights took 10.66 seconds
INFO 05-12 21:13:23 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.059746 seconds
INFO 05-12 21:13:24 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:13:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:13:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:13:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:13:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:13:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.46it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.48it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 21:13:47 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:13:47 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.87 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.65 toks/s, output: 101.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.65 toks/s, output: 101.96 toks/s]
[winogrande thr=0.45 | sample 144] energy=5177.18 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:13:49 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:13:49 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:13:51 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:13:51 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.03s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.12s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.62s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.95s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.13s/it]

INFO 05-12 21:14:04 [loader.py:458] Loading weights took 12.70 seconds
INFO 05-12 21:14:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 13.042069 seconds
INFO 05-12 21:14:05 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:14:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:14:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:14:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:14:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:14:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 21:14:28 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:14:28 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.81 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.64 toks/s, output: 101.99 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.64 toks/s, output: 101.99 toks/s]
[winogrande thr=0.45 | sample 145] energy=5446.58 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.2-1B-Instruct
INFO 05-12 21:14:30 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:14:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:14:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...
INFO 05-12 21:14:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 21:14:32 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.47s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.47s/it]

INFO 05-12 21:14:33 [loader.py:458] Loading weights took 1.61 seconds
INFO 05-12 21:14:33 [model_runner.py:1146] Model loading took 2.3029 GiB and 1.898157 seconds
INFO 05-12 21:14:34 [worker.py:267] Memory profiling takes 0.34 seconds
INFO 05-12 21:14:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:14:34 [worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.17GiB; the rest of the memory reserved for KV Cache is 80.24GiB.
INFO 05-12 21:14:34 [executor_base.py:112] # cuda blocks: 164331, # CPU blocks: 8192
INFO 05-12 21:14:34 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 1283.84x
INFO 05-12 21:14:34 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.62it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.62it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:17,  1.62it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.61it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.61it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.59it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.60it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.60it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.60it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.60it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.61it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.61it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.61it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.61it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.61it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.61it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.61it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.62it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.62it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:19<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-12 21:14:56 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.03 GiB
INFO 05-12 21:14:56 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.85 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s, est. speed input: 107.72 toks/s, output: 316.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s, est. speed input: 107.72 toks/s, output: 316.80 toks/s]
[winogrande thr=0.45 | sample 146] energy=3639.37 J, correct=False, model=meta-llama/Llama-3.2-1B-Instruct
Routed Model: meta-llama/Llama-3.2-1B-Instruct
INFO 05-12 21:15:00 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:15:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:15:01 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...
INFO 05-12 21:15:02 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 21:15:02 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.48s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.48s/it]

INFO 05-12 21:15:04 [loader.py:458] Loading weights took 1.62 seconds
INFO 05-12 21:15:04 [model_runner.py:1146] Model loading took 2.3029 GiB and 2.105889 seconds
INFO 05-12 21:15:05 [worker.py:267] Memory profiling takes 0.33 seconds
INFO 05-12 21:15:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:15:05 [worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.17GiB; the rest of the memory reserved for KV Cache is 80.24GiB.
INFO 05-12 21:15:05 [executor_base.py:112] # cuda blocks: 164331, # CPU blocks: 8192
INFO 05-12 21:15:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 1283.84x
INFO 05-12 21:15:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:20,  1.65it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:19,  1.66it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.66it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:18,  1.66it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.67it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:17,  1.66it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:16,  1.66it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.65it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:15,  1.65it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.65it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.61it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.62it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:07<00:13,  1.63it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:12,  1.64it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.65it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.62it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.63it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:10<00:10,  1.64it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.65it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.65it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:08,  1.66it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:07,  1.66it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:13<00:07,  1.66it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.65it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.65it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:15<00:05,  1.65it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.64it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.61it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:23<00:14,  2.40s/it]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [01:04<01:09, 13.88s/it]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [03:25<03:28, 52.05s/it]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [04:25<02:42, 54.33s/it]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [04:27<01:17, 38.85s/it]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [05:35<00:47, 47.45s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [05:43<00:00, 35.80s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [05:44<00:00,  9.84s/it]
INFO 05-12 21:20:50 [model_runner.py:1598] Graph capturing finished in 345 secs, took 0.03 GiB
INFO 05-12 21:20:50 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 345.80 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s, est. speed input: 107.60 toks/s, output: 316.46 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s, est. speed input: 107.60 toks/s, output: 316.46 toks/s]
[winogrande thr=0.45 | sample 147] energy=42193.68 J, correct=False, model=meta-llama/Llama-3.2-1B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:20:53 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:20:53 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:20:55 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:20:55 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.84s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.03s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 21:21:05 [loader.py:458] Loading weights took 9.99 seconds
INFO 05-12 21:21:05 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.312530 seconds
INFO 05-12 21:21:06 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:21:06 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:21:06 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:21:06 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:21:06 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:21:06 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.57it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:21:29 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:21:29 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.67 toks/s, output: 102.03 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.67 toks/s, output: 102.03 toks/s]
[winogrande thr=0.45 | sample 148] energy=5064.35 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:21:31 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:21:31 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:21:33 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:21:33 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.50s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.68s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.08s/it]

INFO 05-12 21:21:46 [loader.py:458] Loading weights took 12.58 seconds
INFO 05-12 21:21:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.979207 seconds
INFO 05-12 21:21:47 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:21:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:21:47 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:21:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:21:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:21:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 21:22:10 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:22:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.59 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.57 toks/s, output: 101.97 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.57 toks/s, output: 101.97 toks/s]
[winogrande thr=0.45 | sample 149] energy=5441.39 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:22:12 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:22:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:22:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:22:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 21:22:23 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 21:22:24 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.276114 seconds
INFO 05-12 21:22:25 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:22:25 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:22:25 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:22:25 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:22:25 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:22:25 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.57it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:22:48 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:22:48 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.63 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.74 toks/s, output: 102.18 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.74 toks/s, output: 102.18 toks/s]
[winogrande thr=0.45 | sample 150] energy=5051.71 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:22:50 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:22:50 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:22:51 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:22:51 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.55s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.55s/it]

INFO 05-12 21:23:02 [loader.py:458] Loading weights took 10.45 seconds
INFO 05-12 21:23:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.780244 seconds
INFO 05-12 21:23:03 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:23:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:23:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:23:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:23:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:23:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.57it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:23:26 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:23:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.60 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.82 toks/s, output: 102.16 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 38.82 toks/s, output: 102.16 toks/s]
[winogrande thr=0.45 | sample 151] energy=5100.41 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:23:28 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:23:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:23:30 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:23:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.05s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.06s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.59s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.89s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.08s/it]

INFO 05-12 21:23:43 [loader.py:458] Loading weights took 12.51 seconds
INFO 05-12 21:23:43 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.849012 seconds
INFO 05-12 21:23:44 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:23:44 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:23:44 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:23:44 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:23:44 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:23:44 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.49it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.49it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.48it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:24:07 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:24:07 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.07 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.82 toks/s, output: 102.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.82 toks/s, output: 102.34 toks/s]
[winogrande thr=0.45 | sample 152] energy=5491.27 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:24:09 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:24:09 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:24:11 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:24:11 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:24:21 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 21:24:21 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.288311 seconds
INFO 05-12 21:24:22 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:24:22 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:24:22 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:24:22 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:24:22 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:24:22 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:24:45 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:24:45 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.68 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.74 toks/s, output: 102.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.74 toks/s, output: 102.31 toks/s]
[winogrande thr=0.45 | sample 153] energy=5063.55 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:24:47 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:24:47 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:24:48 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:24:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:24:59 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 21:24:59 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.291264 seconds
INFO 05-12 21:25:00 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:25:00 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:25:00 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:25:00 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:25:00 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:25:00 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:25:23 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:25:23 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.66 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.78 toks/s, output: 102.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.78 toks/s, output: 102.09 toks/s]
[winogrande thr=0.45 | sample 154] energy=5053.42 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:25:25 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:25:25 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:25:26 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:25:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:25:36 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 21:25:37 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.249946 seconds
INFO 05-12 21:25:37 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:25:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:25:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:25:38 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:25:38 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:25:38 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:12<01:46,  3.67s/it]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:28<03:36,  7.74s/it]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [01:15<09:04, 20.16s/it]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [02:25<15:24, 35.56s/it]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [03:02<15:02, 36.10s/it]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [03:06<10:30, 26.27s/it]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [03:09<07:21, 19.21s/it]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [03:11<05:07, 13.97s/it]Capturing CUDA graph shapes:  40%|████      | 14/35 [03:12<03:30, 10.01s/it]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [03:48<05:56, 17.82s/it]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [06:48<21:07, 66.71s/it]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [07:48<19:27, 64.87s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [07:53<13:12, 46.61s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [08:57<13:53, 52.09s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [09:10<10:01, 40.09s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [09:12<06:44, 28.86s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [09:29<05:28, 25.29s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [09:32<03:41, 18.42s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [09:33<02:26, 13.30s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [09:34<01:35,  9.51s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [09:34<01:01,  6.85s/it]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [09:35<00:39,  4.99s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [09:36<00:25,  3.69s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [09:36<00:16,  2.77s/it]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [09:37<00:10,  2.14s/it]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [09:38<00:06,  1.69s/it]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [09:38<00:04,  1.39s/it]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [09:39<00:02,  1.17s/it]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [09:40<00:01,  1.02s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [09:40<00:00,  1.10it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [09:40<00:00, 16.59s/it]
INFO 05-12 21:35:18 [model_runner.py:1598] Graph capturing finished in 581 secs, took 0.06 GiB
INFO 05-12 21:35:18 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 581.74 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.87 toks/s, output: 102.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.87 toks/s, output: 102.35 toks/s]
[winogrande thr=0.45 | sample 155] energy=71563.34 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:35:21 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:35:21 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:35:22 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:35:22 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:35:32 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 21:35:33 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.348703 seconds
INFO 05-12 21:35:33 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:35:33 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:35:33 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:35:34 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:35:34 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:35:34 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.44it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.48it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:35:57 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:35:57 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.07 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 46.93 toks/s, output: 102.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 46.93 toks/s, output: 102.02 toks/s]
[winogrande thr=0.45 | sample 156] energy=5092.91 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:35:59 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:35:59 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:36:00 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:36:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 21:36:11 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 21:36:11 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.507634 seconds
INFO 05-12 21:36:12 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:36:12 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:36:12 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:36:12 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:36:12 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:36:12 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.50it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.49it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:36:35 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:36:35 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.09 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.79 toks/s, output: 102.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.79 toks/s, output: 102.32 toks/s]
[winogrande thr=0.45 | sample 157] energy=5146.10 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:36:37 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:36:37 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:36:39 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:36:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:36:49 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 21:36:49 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.252385 seconds
INFO 05-12 21:36:50 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:36:50 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:36:50 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:36:50 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:36:50 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:36:50 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.49it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:37:13 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:37:13 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.14 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.80 toks/s, output: 102.35 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.80 toks/s, output: 102.35 toks/s]
[winogrande thr=0.45 | sample 158] energy=5097.44 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:37:16 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:37:16 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:37:17 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:37:18 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.09s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.35s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.23s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.70s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.74s/it]

INFO 05-12 21:37:29 [loader.py:458] Loading weights took 11.20 seconds
INFO 05-12 21:37:29 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.532397 seconds
INFO 05-12 21:37:30 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:37:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:37:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:37:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:37:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:37:30 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.45it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.49it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:37:53 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:37:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.09 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.72 toks/s, output: 102.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.72 toks/s, output: 102.32 toks/s]
[winogrande thr=0.45 | sample 159] energy=5341.29 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:37:56 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:37:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:37:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:37:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.14s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.36s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.70s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.74s/it]

INFO 05-12 21:38:08 [loader.py:458] Loading weights took 11.15 seconds
INFO 05-12 21:38:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.492148 seconds
INFO 05-12 21:38:10 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:38:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:38:10 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:38:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:38:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:38:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.49it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:38:33 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:38:33 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.13 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.70 toks/s, output: 102.26 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.70 toks/s, output: 102.26 toks/s]
[winogrande thr=0.45 | sample 160] energy=5267.48 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:38:35 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:38:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:38:36 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:38:36 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.06s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.55s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.55s/it]

INFO 05-12 21:38:47 [loader.py:458] Loading weights took 10.46 seconds
INFO 05-12 21:38:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.789378 seconds
INFO 05-12 21:38:48 [worker.py:267] Memory profiling takes 0.43 seconds
INFO 05-12 21:38:48 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:38:48 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:38:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:38:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:38:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.50it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 21:39:12 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:39:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.68 toks/s, output: 102.20 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.68 toks/s, output: 102.20 toks/s]
[winogrande thr=0.45 | sample 161] energy=5156.48 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:39:14 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:39:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:39:16 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:39:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 21:39:17 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.684610 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.54s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.90s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.28s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.27s/it]

INFO 05-12 21:39:30 [loader.py:458] Loading weights took 13.22 seconds
INFO 05-12 21:39:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 14.104806 seconds
INFO 05-12 21:39:31 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:39:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:39:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:39:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:39:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:39:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.51it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.49it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 21:39:54 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:39:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.12 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.83 toks/s, output: 102.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.83 toks/s, output: 102.23 toks/s]
[winogrande thr=0.45 | sample 162] energy=5659.79 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:39:56 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:39:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:39:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:39:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.26s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.25s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.19s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.48s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]

INFO 05-12 21:40:09 [loader.py:458] Loading weights took 10.62 seconds
INFO 05-12 21:40:09 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.972973 seconds
INFO 05-12 21:40:10 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:40:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:40:10 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:40:10 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:40:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:40:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 21:40:33 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:40:33 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.02 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.74 toks/s, output: 102.24 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.74 toks/s, output: 102.24 toks/s]
[winogrande thr=0.45 | sample 163] energy=5170.53 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.2-1B-Instruct
INFO 05-12 21:40:35 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:40:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:40:37 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...
INFO 05-12 21:40:37 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 21:40:37 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.46s/it]

INFO 05-12 21:40:39 [loader.py:458] Loading weights took 1.60 seconds
INFO 05-12 21:40:39 [model_runner.py:1146] Model loading took 2.3029 GiB and 1.892297 seconds
INFO 05-12 21:40:39 [worker.py:267] Memory profiling takes 0.34 seconds
INFO 05-12 21:40:39 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:40:39 [worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.17GiB; the rest of the memory reserved for KV Cache is 80.24GiB.
INFO 05-12 21:40:40 [executor_base.py:112] # cuda blocks: 164331, # CPU blocks: 8192
INFO 05-12 21:40:40 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 1283.84x
INFO 05-12 21:40:40 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.60it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.61it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.60it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.61it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.61it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.60it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.60it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.61it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.60it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:14,  1.60it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.58it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:08,  1.59it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.59it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.60it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.60it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.60it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.60it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.62it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.59it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.60it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.59it/s]
INFO 05-12 21:41:02 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.03 GiB
INFO 05-12 21:41:02 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.01 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s, est. speed input: 101.41 toks/s, output: 316.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s, est. speed input: 101.41 toks/s, output: 316.88 toks/s]
[winogrande thr=0.45 | sample 164] energy=3676.06 J, correct=False, model=meta-llama/Llama-3.2-1B-Instruct
Routed Model: meta-llama/Llama-3.2-1B-Instruct
INFO 05-12 21:41:05 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:41:05 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:41:07 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...
INFO 05-12 21:41:07 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 21:41:07 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.52s/it]

INFO 05-12 21:41:09 [loader.py:458] Loading weights took 1.66 seconds
INFO 05-12 21:41:09 [model_runner.py:1146] Model loading took 2.3029 GiB and 1.955505 seconds
INFO 05-12 21:41:10 [worker.py:267] Memory profiling takes 0.33 seconds
INFO 05-12 21:41:10 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:41:10 [worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.17GiB; the rest of the memory reserved for KV Cache is 80.24GiB.
INFO 05-12 21:41:10 [executor_base.py:112] # cuda blocks: 164331, # CPU blocks: 8192
INFO 05-12 21:41:10 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 1283.84x
INFO 05-12 21:41:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.62it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.63it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:19,  1.62it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.63it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.63it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:17,  1.63it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.63it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:16,  1.63it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.61it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.62it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.59it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.60it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.60it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.61it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:09<00:11,  1.61it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.61it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.62it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:11<00:09,  1.63it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.63it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:12<00:08,  1.63it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:13<00:07,  1.64it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.64it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:14<00:06,  1.64it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.63it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.63it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:16<00:04,  1.63it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.63it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:17<00:03,  1.62it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:18<00:03,  1.61it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.62it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:19<00:01,  1.60it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:20<00:01,  1.61it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.61it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.62it/s]
INFO 05-12 21:41:32 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.03 GiB
INFO 05-12 21:41:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 22.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s, est. speed input: 101.71 toks/s, output: 317.82 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s, est. speed input: 101.71 toks/s, output: 317.82 toks/s]
[winogrande thr=0.45 | sample 165] energy=3673.69 J, correct=False, model=meta-llama/Llama-3.2-1B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:41:36 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:41:36 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:41:38 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:41:38 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.85s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.04s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.44s/it]

INFO 05-12 21:41:48 [loader.py:458] Loading weights took 10.01 seconds
INFO 05-12 21:41:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.343524 seconds
INFO 05-12 21:41:49 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 21:41:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:41:49 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:41:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:41:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:41:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 21:42:12 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:42:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.89 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.72 toks/s, output: 102.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.72 toks/s, output: 102.00 toks/s]
[winogrande thr=0.45 | sample 166] energy=5211.09 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:42:14 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:42:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:42:16 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:42:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.63s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.11s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:02,  2.84s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.44s/it]

INFO 05-12 21:42:30 [loader.py:458] Loading weights took 13.93 seconds
INFO 05-12 21:42:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 14.262578 seconds
INFO 05-12 21:42:31 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:42:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:42:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:42:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:42:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:42:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.58it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.58it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.59it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.59it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 21:42:54 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:42:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.43 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.71 toks/s, output: 101.92 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.71 toks/s, output: 101.92 toks/s]
[winogrande thr=0.45 | sample 167] energy=5505.06 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:42:56 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:42:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:42:58 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:42:58 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.67s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.72s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.70s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.86s/it]

INFO 05-12 21:43:10 [loader.py:458] Loading weights took 11.67 seconds
INFO 05-12 21:43:10 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.019752 seconds
INFO 05-12 21:43:11 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:43:11 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:43:11 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:43:11 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:43:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:43:11 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 21:43:34 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:43:34 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.90 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.59 toks/s, output: 101.91 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.59 toks/s, output: 101.91 toks/s]
[winogrande thr=0.45 | sample 168] energy=5335.08 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:43:37 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:43:37 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:43:38 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:43:38 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.47s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.65s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.47s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.07s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.07s/it]

INFO 05-12 21:43:51 [loader.py:458] Loading weights took 12.45 seconds
INFO 05-12 21:43:51 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.855911 seconds
INFO 05-12 21:43:52 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:43:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:43:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:43:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:43:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:43:52 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 21:44:15 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:44:15 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.89 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.63 toks/s, output: 101.96 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.63 toks/s, output: 101.96 toks/s]
[winogrande thr=0.45 | sample 169] energy=5489.67 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:44:17 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:44:17 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:44:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:44:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.81s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.05s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.46s/it]

INFO 05-12 21:44:29 [loader.py:458] Loading weights took 10.09 seconds
INFO 05-12 21:44:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.421628 seconds
INFO 05-12 21:44:30 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 21:44:30 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:44:30 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:44:30 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:44:30 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:44:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.55it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.58it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.58it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.58it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.58it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 21:44:53 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:44:53 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.41 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.79 toks/s, output: 102.02 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.79 toks/s, output: 102.02 toks/s]
[winogrande thr=0.45 | sample 170] energy=5028.52 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:44:55 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:44:55 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:44:56 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:44:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:45:07 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 21:45:07 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.256736 seconds
INFO 05-12 21:45:08 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:45:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:45:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:45:08 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:45:08 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:45:08 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.57it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.57it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.58it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.58it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.58it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:15,  1.58it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.58it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.58it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.58it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.58it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.58it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.58it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.58it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.58it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 21:45:31 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 21:45:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.42 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.82 toks/s, output: 102.09 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.82 toks/s, output: 102.09 toks/s]
[winogrande thr=0.45 | sample 171] energy=5042.94 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:45:33 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:45:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:45:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:45:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:45:44 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 21:45:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.256594 seconds
INFO 05-12 21:45:45 [worker.py:267] Memory profiling takes 0.43 seconds
INFO 05-12 21:45:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:45:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:45:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:45:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:45:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 21:46:08 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:46:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.79 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 41.61 toks/s, output: 101.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 41.61 toks/s, output: 101.49 toks/s]
[winogrande thr=0.45 | sample 172] energy=5060.09 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:46:10 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:46:10 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:46:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:46:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:46:22 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 21:46:22 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.258615 seconds
INFO 05-12 21:46:23 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:46:23 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:46:23 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:46:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:46:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:46:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.49it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.46it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.48it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:17,  1.46it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.47it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.45it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.47it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.49it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.48it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.50it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.49it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:46:47 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:46:47 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.31 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.77 toks/s, output: 102.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.77 toks/s, output: 102.34 toks/s]
[winogrande thr=0.45 | sample 173] energy=5127.29 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:46:49 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:46:49 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:46:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:46:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:47:00 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 21:47:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.260862 seconds
INFO 05-12 21:47:02 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:47:02 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:47:02 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:47:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:47:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:47:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.49it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.49it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.49it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.49it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:47:25 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:47:25 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.23 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.97 toks/s, output: 102.42 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.97 toks/s, output: 102.42 toks/s]
[winogrande thr=0.45 | sample 174] energy=5116.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:47:27 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:47:27 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:47:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:47:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 05-12 21:47:29 [weight_utils.py:281] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 0.691704 seconds
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:47:40 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 21:47:40 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.915054 seconds
INFO 05-12 21:47:41 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:47:41 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:47:41 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:47:41 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:47:41 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:47:41 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.47it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.48it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:10,  1.49it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.49it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.46it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.49it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]
INFO 05-12 21:48:04 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:48:04 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.03 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.67 toks/s, output: 101.97 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.67 toks/s, output: 101.97 toks/s]
[winogrande thr=0.45 | sample 175] energy=5192.00 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:48:06 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:48:06 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:48:07 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:48:07 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.37s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 21:48:18 [loader.py:458] Loading weights took 9.97 seconds
INFO 05-12 21:48:18 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.302426 seconds
INFO 05-12 21:48:19 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:48:19 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:48:19 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:48:19 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:48:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:48:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.49it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.58it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.58it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.58it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.58it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 21:48:42 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:48:42 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.69 toks/s, output: 102.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.69 toks/s, output: 102.22 toks/s]
[winogrande thr=0.45 | sample 176] energy=5045.84 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:48:44 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:48:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:48:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:48:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 21:48:55 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 21:48:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.230939 seconds
INFO 05-12 21:48:56 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:48:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:48:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:48:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:48:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:48:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 21:49:19 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:49:19 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.76 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 32.81 toks/s, output: 102.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.81 toks/s, output: 102.54 toks/s]
[winogrande thr=0.45 | sample 177] energy=5049.93 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:49:22 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:49:22 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:49:23 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:49:23 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 21:49:33 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 21:49:34 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.274935 seconds
INFO 05-12 21:49:34 [worker.py:267] Memory profiling takes 0.43 seconds
INFO 05-12 21:49:34 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:49:34 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:49:35 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:49:35 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:49:35 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.49it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 21:49:58 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:49:58 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.00 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.73 toks/s, output: 102.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.73 toks/s, output: 102.21 toks/s]
[winogrande thr=0.45 | sample 178] energy=5087.28 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:50:00 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:50:00 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:50:01 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:50:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.15s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.67s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.65s/it]

INFO 05-12 21:50:12 [loader.py:458] Loading weights took 10.85 seconds
INFO 05-12 21:50:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.183896 seconds
INFO 05-12 21:50:13 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 21:50:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:50:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:50:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:50:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:50:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.51it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.48it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.46it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.47it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.46it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.48it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 21:50:37 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 21:50:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.24 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 33.08 toks/s, output: 100.24 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 33.08 toks/s, output: 100.24 toks/s]
[winogrande thr=0.45 | sample 179] energy=5214.65 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 21:50:40 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 21:50:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 21:50:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 21:50:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.01s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.60s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.92s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.10s/it]

INFO 05-12 21:50:54 [loader.py:458] Loading weights took 12.59 seconds
INFO 05-12 21:50:54 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.935509 seconds
INFO 05-12 21:50:55 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 21:50:55 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 21:50:55 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 21:50:55 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 21:50:55 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 21:50:55 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.48it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:18,  1.30it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [01:13<07:50, 20.45s/it]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [03:12<18:26, 50.31s/it]Capturing CUDA graph shapes:  40%|████      | 14/35 [06:05<30:38, 87.53s/it]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [07:06<26:31, 79.58s/it]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [07:37<20:31, 64.83s/it]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [07:42<14:03, 46.86s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [07:45<09:34, 33.78s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [07:47<06:26, 24.13s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [08:40<08:10, 32.69s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [09:30<08:52, 38.06s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [09:43<06:38, 30.62s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [09:45<04:24, 22.04s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [09:46<02:52, 15.70s/it]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [09:47<01:51, 11.18s/it]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [09:48<01:12,  8.02s/it]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [09:48<00:46,  5.81s/it]Capturing CUDA graph shapes:  80%|████████  | 28/35 [09:49<00:29,  4.25s/it]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [09:50<00:19,  3.24s/it]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [09:50<00:12,  2.49s/it]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [09:51<00:07,  1.94s/it]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [09:52<00:04,  1.55s/it]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [09:52<00:02,  1.28s/it]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [09:53<00:01,  1.09s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [09:54<00:00,  1.03s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [09:54<00:00, 16.98s/it]
INFO 05-12 22:00:50 [model_runner.py:1598] Graph capturing finished in 594 secs, took 0.06 GiB
INFO 05-12 22:00:50 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 595.54 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.02 toks/s, output: 102.31 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 45.02 toks/s, output: 102.31 toks/s]
[winogrande thr=0.45 | sample 180] energy=73464.16 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:00:52 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:00:52 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:00:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:00:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:01:07 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 22:01:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.297058 seconds
INFO 05-12 22:01:08 [worker.py:267] Memory profiling takes 0.42 seconds
INFO 05-12 22:01:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:01:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:01:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:01:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:01:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:20,  1.58it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.57it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.58it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:18,  1.58it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.57it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:15<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.57it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 22:01:31 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 22:01:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.52 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.94 toks/s, output: 102.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 40.94 toks/s, output: 102.34 toks/s]
[winogrande thr=0.45 | sample 181] energy=5470.02 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:01:33 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:01:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:01:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:01:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.87s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.49s/it]

INFO 05-12 22:01:45 [loader.py:458] Loading weights took 10.21 seconds
INFO 05-12 22:01:46 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.572562 seconds
INFO 05-12 22:01:46 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 22:01:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:01:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:01:47 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:01:47 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:01:47 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.55it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:02:10 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:02:10 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.97 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.74 toks/s, output: 102.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.74 toks/s, output: 102.23 toks/s]
[winogrande thr=0.45 | sample 182] energy=5110.53 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:02:12 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:02:12 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:02:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:02:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:02:23 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 22:02:24 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.241353 seconds
INFO 05-12 22:02:24 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:02:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:02:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:02:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:02:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:02:25 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.55it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 22:02:47 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:02:47 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.63 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.88 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.64 toks/s, output: 101.88 toks/s]
[winogrande thr=0.45 | sample 183] energy=5046.14 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:02:49 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:02:49 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:02:51 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:02:51 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.95s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.19s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.55s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]

INFO 05-12 22:03:01 [loader.py:458] Loading weights took 10.62 seconds
INFO 05-12 22:03:02 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.956076 seconds
INFO 05-12 22:03:03 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:03:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:03:03 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:03:03 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:03:03 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:03:03 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.49it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.50it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.49it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:03:26 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:03:26 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.00 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.69 toks/s, output: 102.14 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.69 toks/s, output: 102.14 toks/s]
[winogrande thr=0.45 | sample 184] energy=5177.91 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:03:28 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:03:28 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:03:29 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:03:30 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.12s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.33s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.20s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.69s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.72s/it]

INFO 05-12 22:03:41 [loader.py:458] Loading weights took 11.09 seconds
INFO 05-12 22:03:41 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.419251 seconds
INFO 05-12 22:03:42 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:03:42 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:03:42 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:03:42 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:03:42 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:03:42 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.50it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.51it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:04:05 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:04:05 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.01 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.62 toks/s, output: 102.06 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.62 toks/s, output: 102.06 toks/s]
[winogrande thr=0.45 | sample 185] energy=5253.66 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:04:07 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:04:07 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:04:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:04:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:04:19 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 22:04:19 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.264558 seconds
INFO 05-12 22:04:20 [worker.py:267] Memory profiling takes 0.45 seconds
INFO 05-12 22:04:20 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:04:20 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:04:20 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:04:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:04:20 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:04:43 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:04:43 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.88 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.78 toks/s, output: 102.23 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.78 toks/s, output: 102.23 toks/s]
[winogrande thr=0.45 | sample 186] energy=5071.75 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:04:45 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:04:45 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:04:47 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:04:47 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:04:58 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 22:04:58 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.329370 seconds
INFO 05-12 22:04:59 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 22:04:59 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:04:59 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:04:59 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:04:59 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:04:59 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.52it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:05:22 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:05:22 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.90 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.79 toks/s, output: 102.39 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 33.79 toks/s, output: 102.39 toks/s]
[winogrande thr=0.45 | sample 187] energy=5158.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:05:24 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:05:24 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:05:25 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:05:26 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:05:36 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 22:05:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.263471 seconds
INFO 05-12 22:05:37 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:05:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:05:37 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:05:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:05:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:05:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:06:00 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:06:00 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.83 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.69 toks/s, output: 102.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.69 toks/s, output: 102.22 toks/s]
[winogrande thr=0.45 | sample 188] energy=5080.49 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:06:02 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:06:02 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:06:03 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:06:03 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:06:14 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 22:06:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.298778 seconds
INFO 05-12 22:06:15 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:06:15 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:06:15 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:06:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:06:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:06:15 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.55it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:06:38 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:06:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.84 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.70 toks/s, output: 102.33 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.70 toks/s, output: 102.33 toks/s]
[winogrande thr=0.45 | sample 189] energy=5072.72 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:06:40 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:06:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:06:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:06:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]

INFO 05-12 22:06:52 [loader.py:458] Loading weights took 9.95 seconds
INFO 05-12 22:06:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.284005 seconds
INFO 05-12 22:06:53 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:06:53 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:06:53 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:06:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:06:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:06:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.56it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.56it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 22:07:16 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:07:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.63 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.70 toks/s, output: 101.80 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 39.70 toks/s, output: 101.80 toks/s]
[winogrande thr=0.45 | sample 190] energy=5029.10 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:07:18 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:07:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:07:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:07:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.08s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.47s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.89s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.88s/it]

INFO 05-12 22:07:31 [loader.py:458] Loading weights took 11.76 seconds
INFO 05-12 22:07:31 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.083214 seconds
INFO 05-12 22:07:32 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 22:07:32 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:07:32 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:07:32 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:07:32 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:07:32 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.42it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:07:55 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:07:55 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.80 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.80 toks/s, output: 102.29 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.80 toks/s, output: 102.29 toks/s]
[winogrande thr=0.45 | sample 191] energy=5283.76 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:07:58 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:07:58 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:08:00 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:08:00 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  4.00s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.56s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.84s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.02s/it]

INFO 05-12 22:08:12 [loader.py:458] Loading weights took 12.32 seconds
INFO 05-12 22:08:13 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.717072 seconds
INFO 05-12 22:08:13 [worker.py:267] Memory profiling takes 0.40 seconds
INFO 05-12 22:08:13 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:08:13 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:08:14 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:08:14 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:08:14 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.50it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.49it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.48it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.50it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.50it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.49it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.50it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.49it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.50it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.48it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.49it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.50it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]
INFO 05-12 22:08:37 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:08:37 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.20 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.75 toks/s, output: 102.40 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.75 toks/s, output: 102.40 toks/s]
[winogrande thr=0.45 | sample 192] energy=5515.90 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:08:39 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:08:39 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:08:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:08:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.88s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.10s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.11s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.43s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.50s/it]

INFO 05-12 22:08:51 [loader.py:458] Loading weights took 10.24 seconds
INFO 05-12 22:08:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.569736 seconds
INFO 05-12 22:08:52 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:08:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:08:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:08:52 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:08:52 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:08:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.50it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.49it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:09:16 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:09:16 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.73 toks/s, output: 102.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.73 toks/s, output: 102.36 toks/s]
[winogrande thr=0.45 | sample 193] energy=5154.21 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:09:18 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:09:18 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:09:19 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:09:19 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:09:30 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 22:09:30 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.558391 seconds
INFO 05-12 22:09:31 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:09:31 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:09:31 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:09:31 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:09:31 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:09:31 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.55it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.53it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 22:09:54 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:09:54 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.69 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.81 toks/s, output: 102.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 35.81 toks/s, output: 102.32 toks/s]
[winogrande thr=0.45 | sample 194] energy=5106.07 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:09:56 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:09:56 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:09:57 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:09:57 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:10:07 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 22:10:08 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.250920 seconds
INFO 05-12 22:10:08 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:10:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:10:08 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:10:09 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:10:09 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:10:09 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.47it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.53it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 22:10:31 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:10:31 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.61 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 29.73 toks/s, output: 102.52 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 29.73 toks/s, output: 102.52 toks/s]
[winogrande thr=0.45 | sample 195] energy=5045.25 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:10:33 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:10:33 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:10:35 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:10:35 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:10:45 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 22:10:45 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.247879 seconds
INFO 05-12 22:10:46 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:10:46 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:10:46 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:10:46 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:10:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:10:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.55it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.56it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.57it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 22:11:09 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:11:09 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.64 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.69 toks/s, output: 102.28 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.69 toks/s, output: 102.28 toks/s]
[winogrande thr=0.45 | sample 196] energy=5031.81 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:11:11 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:11:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:11:13 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:11:13 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.41s/it]

INFO 05-12 22:11:23 [loader.py:458] Loading weights took 9.88 seconds
INFO 05-12 22:11:23 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.220446 seconds
INFO 05-12 22:11:24 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 22:11:24 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:11:24 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:11:24 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:11:24 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:11:24 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.57it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.57it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.57it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.57it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:15,  1.57it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.57it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.57it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.57it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:08<00:13,  1.57it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.57it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.57it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:17<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.57it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.57it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.57it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]
INFO 05-12 22:11:47 [model_runner.py:1598] Graph capturing finished in 22 secs, took 0.06 GiB
INFO 05-12 22:11:47 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.49 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 36.90 toks/s, output: 102.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 36.90 toks/s, output: 102.50 toks/s]
[winogrande thr=0.45 | sample 197] energy=5075.67 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:11:49 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:11:49 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:11:50 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:11:50 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:12:01 [loader.py:458] Loading weights took 9.90 seconds
INFO 05-12 22:12:01 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.234050 seconds
INFO 05-12 22:12:02 [worker.py:267] Memory profiling takes 0.38 seconds
INFO 05-12 22:12:02 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:12:02 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:12:02 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:12:02 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:12:02 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.54it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:20<04:55,  9.24s/it]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [01:18<14:36, 28.26s/it]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [02:38<23:32, 47.09s/it]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [03:44<25:47, 53.35s/it]Capturing CUDA graph shapes:  20%|██        | 7/35 [04:37<24:50, 53.25s/it]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [05:12<21:24, 47.59s/it]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [05:35<17:17, 39.91s/it]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [06:09<15:51, 38.06s/it]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [06:14<11:08, 27.85s/it]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [06:16<07:39, 20.00s/it]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [06:17<05:15, 14.32s/it]Capturing CUDA graph shapes:  40%|████      | 14/35 [06:18<03:34, 10.20s/it]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [06:18<02:26,  7.33s/it]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [06:19<01:41,  5.32s/it]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [06:20<01:10,  3.93s/it]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [06:20<00:50,  2.95s/it]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [06:21<00:36,  2.26s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [06:22<00:26,  1.78s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [06:22<00:20,  1.44s/it]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [06:23<00:15,  1.20s/it]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [06:24<00:12,  1.04s/it]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [06:24<00:10,  1.08it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [06:25<00:08,  1.19it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [06:26<00:07,  1.27it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [06:26<00:05,  1.34it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [06:27<00:04,  1.40it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [06:28<00:04,  1.44it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [06:28<00:03,  1.47it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [06:29<00:02,  1.49it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [06:30<00:02,  1.50it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [06:30<00:01,  1.51it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [06:31<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [06:32<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [06:32<00:00, 11.20s/it]
INFO 05-12 22:18:34 [model_runner.py:1598] Graph capturing finished in 392 secs, took 0.06 GiB
INFO 05-12 22:18:34 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 393.10 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.71 toks/s, output: 102.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.71 toks/s, output: 102.36 toks/s]
[winogrande thr=0.45 | sample 198] energy=49057.50 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:18:36 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:18:36 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:18:38 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:18:38 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:18:48 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 22:18:48 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.263596 seconds
INFO 05-12 22:18:49 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:18:49 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:18:49 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:18:49 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:18:49 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:18:49 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:23,  1.46it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.50it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.50it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.54it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:19:12 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:19:12 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.98 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.73 toks/s, output: 102.34 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.73 toks/s, output: 102.34 toks/s]
[winogrande thr=0.45 | sample 199] energy=5103.37 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:19:14 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:19:14 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:19:16 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:19:16 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.10s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.35s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.23s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.71s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.74s/it]

INFO 05-12 22:19:27 [loader.py:458] Loading weights took 11.20 seconds
INFO 05-12 22:19:28 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.568266 seconds
INFO 05-12 22:19:28 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 22:19:28 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:19:28 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:19:29 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:19:29 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:19:29 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.49it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.55it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.55it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.55it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:19:51 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:19:51 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.87 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.71 toks/s, output: 102.22 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.71 toks/s, output: 102.22 toks/s]
[winogrande thr=0.45 | sample 200] energy=5238.91 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:19:54 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:19:54 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:19:55 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:19:55 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.95s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.20s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.14s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.54s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.59s/it]

INFO 05-12 22:20:06 [loader.py:458] Loading weights took 10.57 seconds
INFO 05-12 22:20:06 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.909314 seconds
INFO 05-12 22:20:07 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:20:07 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:20:07 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:20:07 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:20:07 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:20:07 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.52it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.47it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.48it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.49it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.48it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.53it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.55it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.55it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:20:30 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:20:30 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.94 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.73 toks/s, output: 102.28 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.73 toks/s, output: 102.28 toks/s]
[winogrande thr=0.45 | sample 201] energy=5180.56 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:20:32 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:20:32 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:20:34 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:20:34 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:20:44 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 22:20:44 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.275931 seconds
INFO 05-12 22:20:45 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:20:45 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:20:45 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:20:45 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:20:45 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:20:46 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.55it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.56it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.56it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.56it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.55it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.55it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.56it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]
INFO 05-12 22:21:08 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:21:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.78 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.64 toks/s, output: 102.12 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.64 toks/s, output: 102.12 toks/s]
[winogrande thr=0.45 | sample 202] energy=5090.81 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:21:11 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:21:11 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:21:12 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:21:12 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:09,  3.30s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.68s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.41s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.03s/it]

INFO 05-12 22:21:25 [loader.py:458] Loading weights took 12.38 seconds
INFO 05-12 22:21:25 [model_runner.py:1146] Model loading took 14.9576 GiB and 12.724023 seconds
INFO 05-12 22:21:26 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:21:26 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:21:26 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:21:26 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:21:26 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:21:26 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.55it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.56it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.53it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.51it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.55it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.55it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.56it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.56it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:10,  1.56it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.57it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.57it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:08,  1.57it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.57it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.57it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.57it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.57it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.55it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.54it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.52it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.53it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.56it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 22:21:49 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:21:49 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.66 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.78 toks/s, output: 102.30 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.78 toks/s, output: 102.30 toks/s]
[winogrande thr=0.45 | sample 203] energy=5393.83 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:21:51 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:21:51 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:21:52 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:21:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:03<00:10,  3.44s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:06<00:06,  3.46s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:07<00:02,  2.29s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:10<00:00,  2.66s/it]

INFO 05-12 22:22:04 [loader.py:458] Loading weights took 10.89 seconds
INFO 05-12 22:22:04 [model_runner.py:1146] Model loading took 14.9576 GiB and 11.224459 seconds
INFO 05-12 22:22:05 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:22:05 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:22:05 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:22:05 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:22:05 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:22:05 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.55it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.55it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.55it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.55it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.56it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.55it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.56it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:10<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:12<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.55it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:14<00:07,  1.56it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.56it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.56it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:16<00:05,  1.56it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.56it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.56it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.56it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.57it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:19<00:02,  1.57it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.56it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.56it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:21<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]
INFO 05-12 22:22:28 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:22:28 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.66 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.76 toks/s, output: 102.24 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 34.76 toks/s, output: 102.24 toks/s]
[winogrande thr=0.45 | sample 204] energy=5189.13 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:22:30 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:22:30 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:22:31 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:22:31 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:22:42 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 22:22:42 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.295079 seconds
INFO 05-12 22:22:43 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 22:22:43 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:22:43 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:22:43 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:22:43 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:22:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.55it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.50it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.55it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.56it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.55it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.52it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:06,  1.49it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:23:06 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:23:06 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.99 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.72 toks/s, output: 102.41 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.72 toks/s, output: 102.41 toks/s]
[winogrande thr=0.45 | sample 205] energy=5117.19 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:23:08 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:23:08 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:23:09 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:23:09 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:23:20 [loader.py:458] Loading weights took 9.94 seconds
INFO 05-12 22:23:20 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.263591 seconds
INFO 05-12 22:23:21 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:23:21 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:23:21 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:23:21 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:23:21 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:23:21 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.51it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.51it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.51it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:18,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.49it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.48it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:16,  1.49it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.49it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.48it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:14,  1.50it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.48it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.49it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.48it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.50it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.49it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.50it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:38<00:00,  1.09s/it]
INFO 05-12 22:28:54 [model_runner.py:1598] Graph capturing finished in 333 secs, took 0.06 GiB
INFO 05-12 22:32:23 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 542.67 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 30.63 toks/s, output: 102.11 toks/s]Processed prompts: 100%|██████████| 1/1 [00:37<00:00, 37.77s/it, est. speed input: 30.63 toks/s, output: 102.11 toks/s]
[winogrande thr=0.45 | sample 206] energy=102849.82 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:40:05 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:40:05 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:40:07 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:40:07 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:40:17 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 22:40:18 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.298014 seconds
INFO 05-12 22:40:18 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:40:18 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:40:18 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:40:19 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:40:19 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:40:19 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.54it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.54it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.53it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.53it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.53it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.52it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.51it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.52it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.52it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.52it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.52it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.52it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.53it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.54it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.54it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.52it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.51it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.47it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:02,  1.49it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.50it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.51it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:40:42 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:40:42 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.06 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.82 toks/s, output: 102.21 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 37.82 toks/s, output: 102.21 toks/s]
[winogrande thr=0.45 | sample 207] energy=23877.47 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:40:44 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:40:44 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:40:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:40:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:40:55 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 22:40:56 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.308197 seconds
INFO 05-12 22:40:56 [worker.py:267] Memory profiling takes 0.41 seconds
INFO 05-12 22:40:56 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:40:56 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:40:57 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:40:57 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:40:57 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.52it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.53it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.54it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.54it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.54it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.51it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.51it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.50it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.51it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:12,  1.49it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.51it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.51it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.52it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.53it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.53it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.54it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.54it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.52it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.52it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.52it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.44it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:41:20 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:41:20 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.16 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.68 toks/s, output: 102.19 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 31.68 toks/s, output: 102.19 toks/s]
[winogrande thr=0.45 | sample 208] energy=5104.37 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:41:24 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:41:24 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:41:25 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:41:25 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:41:35 [loader.py:458] Loading weights took 9.91 seconds
INFO 05-12 22:41:36 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.241647 seconds
INFO 05-12 22:41:36 [worker.py:267] Memory profiling takes 0.44 seconds
INFO 05-12 22:41:36 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:41:36 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:41:37 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:41:37 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:41:37 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.51it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:22,  1.48it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:21,  1.51it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.52it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.52it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.53it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.54it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.54it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.54it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.55it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.54it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:14,  1.54it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:12,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.54it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.53it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.52it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.53it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.51it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:07,  1.52it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.51it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.53it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.53it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:18<00:03,  1.53it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.54it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:20<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.55it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:22<00:00,  1.53it/s]
INFO 05-12 22:41:59 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:41:59 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 23.97 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 31.70 toks/s, output: 99.07 toks/s]Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 31.70 toks/s, output: 99.07 toks/s]
[winogrande thr=0.45 | sample 209] energy=5290.96 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:42:02 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:42:02 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:42:03 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:42:03 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.79s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  3.00s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.04s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:42:13 [loader.py:458] Loading weights took 9.92 seconds
INFO 05-12 22:42:14 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.265561 seconds
INFO 05-12 22:42:14 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:42:14 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:42:14 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:42:15 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:42:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:42:15 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:22,  1.48it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.50it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:21,  1.48it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:21,  1.47it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:20,  1.49it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:19,  1.47it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:18,  1.50it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.50it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:17,  1.51it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.53it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.54it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.53it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:11<00:11,  1.54it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:12<00:10,  1.54it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:13<00:09,  1.54it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:13<00:09,  1.52it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:14<00:08,  1.53it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:15<00:08,  1.48it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:15<00:07,  1.50it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:16<00:06,  1.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:17<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:17<00:05,  1.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:18<00:04,  1.51it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:19<00:03,  1.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:19<00:03,  1.53it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:20<00:02,  1.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:21<00:01,  1.54it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:22<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.54it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.52it/s]
INFO 05-12 22:42:38 [model_runner.py:1598] Graph capturing finished in 23 secs, took 0.06 GiB
INFO 05-12 22:42:38 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 24.08 seconds
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.74 toks/s, output: 102.32 toks/s]Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 32.74 toks/s, output: 102.32 toks/s]
[winogrande thr=0.45 | sample 210] energy=5095.11 J, correct=False, model=meta-llama/Llama-3.1-8B-Instruct
Routed Model: meta-llama/Llama-3.1-8B-Instruct
INFO 05-12 22:42:40 [config.py:689] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-12 22:42:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 05-12 22:42:41 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 05-12 22:42:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.78s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.01s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.36s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:09<00:00,  2.42s/it]

INFO 05-12 22:42:51 [loader.py:458] Loading weights took 9.93 seconds
INFO 05-12 22:42:52 [model_runner.py:1146] Model loading took 14.9576 GiB and 10.264033 seconds
INFO 05-12 22:42:52 [worker.py:267] Memory profiling takes 0.39 seconds
INFO 05-12 22:42:52 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.02GiB) x gpu_memory_utilization (0.90) = 83.71GiB
INFO 05-12 22:42:52 [worker.py:267] model weights take 14.96GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 67.58GiB.
INFO 05-12 22:42:53 [executor_base.py:112] # cuda blocks: 34599, # CPU blocks: 2048
INFO 05-12 22:42:53 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 270.30x
INFO 05-12 22:42:53 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:21,  1.56it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:20,  1.56it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:19,  1.56it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:19,  1.56it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:18,  1.57it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:17,  1.56it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:05<00:17,  1.56it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:16,  1.56it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:06<00:16,  1.54it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:07<00:15,  1.55it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:07<00:15,  1.53it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:08<00:14,  1.54it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:09<00:13,  1.53it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:10<00:12,  1.55it/s]slurmstepd: error: *** JOB 5142516 ON lrz-hgx-h100-002 CANCELLED AT 2025-05-12T22:43:04 ***
slurmstepd: error: *** JOB 5142516 STEPD TERMINATED ON lrz-hgx-h100-002 AT 2025-05-12T22:44:04 DUE TO JOB NOT ENDING WITH SIGNALS ***
slurmstepd: error: Container 1422196 in cgroup plugin has 1 processes, giving up after 63 sec
